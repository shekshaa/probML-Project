{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import argparse\n",
    "from torch.backends import cudnn\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def delete_module(name):\n",
    "    import sys\n",
    "    del sys.modules[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pkgs/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(data=Namespace(batch_size=32, cates=['airplane'], data_dir='data/ShapeNetCore.v2.PC15k', dataset_scale=1, dataset_type='shapenet15k', normalize_per_shape=False, normalize_std_per_axis=False, num_workers=4, recenter_per_shape=True, te_max_sample_points=2048, tr_max_sample_points=2048, type='datasets.pointflow_datasets'), inference=Namespace(num_points=2048, num_steps=10, step_size_ratio=1, weight=1), models=Namespace(scorenet=Namespace(dim=3, hidden_size=256, n_blocks=24, out_dim=3, param_likelihood=False, sigma_condition=True, type='models.decoders.resnet_add', xyz_condition=True, z_dim=128)), trainer=Namespace(epochs=2000, opt_criticnet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), opt_scorenet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), seed=100, sigma_begin=1, sigma_end=0.01, sigma_num=10, type='trainers.ae_trainer_3D'), viz=Namespace(log_freq=10, save_freq=100, val_freq=100, viz_freq=5000))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = './test_config.yaml'\n",
    "\n",
    "def dict2namespace(config):\n",
    "    namespace = argparse.Namespace()\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            new_value = dict2namespace(value)\n",
    "        else:\n",
    "            new_value = value\n",
    "        setattr(namespace, key, new_value)\n",
    "    return namespace\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = yaml.load(f)\n",
    "        \n",
    "cfg = dict2namespace(config)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=32, cates=['airplane'], data_dir='data/ShapeNetCore.v2.PC15k', dataset_scale=1, dataset_type='shapenet15k', normalize_per_shape=False, normalize_std_per_axis=False, num_workers=4, recenter_per_shape=True, te_max_sample_points=2048, tr_max_sample_points=2048, type='datasets.pointflow_datasets')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(epochs=2000, opt_criticnet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), opt_scorenet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), seed=100, sigma_begin=1, sigma_end=0.01, sigma_num=10, type='trainers.ae_trainer_3D')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(scorenet=Namespace(dim=3, hidden_size=256, n_blocks=24, out_dim=3, param_likelihood=False, sigma_condition=True, type='models.decoders.resnet_add', xyz_condition=True, z_dim=128))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(num_points=2048, num_steps=10, step_size_ratio=1, weight=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(log_freq=10, save_freq=100, val_freq=100, viz_freq=5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scorenet import Scorenet\n",
    "from critic import Criticnet\n",
    "from utils import get_opt, set_random_seed, exact_jacobian_trace, langevin_dynamics, visualize\n",
    "from data_loader import get_data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data:2832\n",
      "Min number of points: (train)2048 (test)2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0832, -0.0008, -0.0106],\n",
       "         [ 0.2124, -0.1133,  0.3423],\n",
       "         [ 0.3726, -0.1521, -0.1293],\n",
       "         ...,\n",
       "         [-0.0380,  0.0209, -0.6827],\n",
       "         [ 0.0835, -0.0098, -0.6379],\n",
       "         [ 0.6067, -0.1337, -0.0316]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_random_seed(getattr(cfg.trainer, \"seed\", 666))\n",
    "\n",
    "# load data\n",
    "train_data = get_data(cfg.data, 0)\n",
    "tr_pts = train_data['tr_points'].unsqueeze(0)\n",
    "te_pts = train_data['te_points'].unsqueeze(0)\n",
    "tr_pts = tr_pts.to(device)\n",
    "te_pts = te_pts.to(device)\n",
    "tr_pts.requires_grad_()\n",
    "te_pts.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma:,  [1.         0.59948425 0.35938137 0.21544347 0.12915497 0.07742637\n",
      " 0.04641589 0.02782559 0.01668101 0.01      ]\n"
     ]
    }
   ],
   "source": [
    "# sigmas\n",
    "if hasattr(cfg.trainer, \"sigmas\"):\n",
    "    np_sigmas = cfg.trainer.sigmas\n",
    "else:\n",
    "    sigma_begin = float(cfg.trainer.sigma_begin)\n",
    "    sigma_end = float(cfg.trainer.sigma_end)\n",
    "    num_classes = int(cfg.trainer.sigma_num)\n",
    "    np_sigmas = np.exp(np.linspace(np.log(sigma_begin), np.log(sigma_end), num_classes))\n",
    "print(\"Sigma:, \", np_sigmas)\n",
    "sigmas = torch.tensor(np.array(np_sigmas)).float().cuda().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorenet(\n",
      "  (conv_p): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (1): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (2): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (3): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (4): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (bn_out): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_out): Conv1d(256, 3, kernel_size=(1,), stride=(1,))\n",
      "  (actvn_out): ReLU()\n",
      ")\n",
      "Criticnet(\n",
      "  (conv_p): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (1): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (2): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (3): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (4): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (bn_out): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_out): Conv1d(256, 3, kernel_size=(1,), stride=(1,))\n",
      "  (actvn_out): ReLU()\n",
      ")\n",
      "Start epoch: 0 End epoch: 2000\n",
      "Epoch (critic) 0 Loss=-0.04002 t1=-0.01712 t2=-0.02290\n",
      "Epoch (critic) 1 Loss=0.25263 t1=0.13264 t2=0.11999\n",
      "Epoch (critic) 2 Loss=0.43381 t1=0.13282 t2=0.30099\n",
      "Epoch (critic) 3 Loss=0.55929 t1=0.07691 t2=0.48238\n",
      "Epoch (critic) 4 Loss=0.85169 t1=0.03547 t2=0.81622\n",
      "Epoch (critic) 5 Loss=1.52651 t1=0.02320 t2=1.50331\n",
      "Epoch (critic) 6 Loss=2.51135 t1=0.03410 t2=2.47725\n",
      "Epoch (critic) 7 Loss=3.87890 t1=0.05351 t2=3.82540\n",
      "Epoch (critic) 8 Loss=5.23946 t1=0.04774 t2=5.19171\n",
      "Epoch (critic) 9 Loss=5.34794 t1=0.02283 t2=5.32511\n",
      "Epoch (critic) 10 Loss=5.07062 t1=0.05124 t2=5.01938\n",
      "Epoch (critic) 11 Loss=4.79295 t1=0.10548 t2=4.68747\n",
      "Epoch (critic) 12 Loss=7.32517 t1=0.08202 t2=7.24316\n",
      "Epoch (critic) 13 Loss=11.45204 t1=0.11151 t2=11.34053\n",
      "Epoch (critic) 14 Loss=13.05455 t1=0.06700 t2=12.98755\n",
      "Epoch (critic) 15 Loss=7.86419 t1=0.05416 t2=7.81003\n",
      "Epoch (critic) 16 Loss=4.81045 t1=0.02686 t2=4.78359\n",
      "Epoch (critic) 17 Loss=7.70694 t1=0.10043 t2=7.60651\n",
      "Epoch (critic) 18 Loss=8.89789 t1=0.10627 t2=8.79162\n",
      "Epoch (critic) 19 Loss=10.93980 t1=0.04024 t2=10.89956\n",
      "Epoch (score) 20 Loss=10.04238 t1=0.05692 t2=9.98545\n",
      "Epoch (critic) 21 Loss=10.08164 t1=-0.92233 t2=11.00397\n",
      "Epoch (critic) 22 Loss=8.61939 t1=0.32220 t2=8.29719\n",
      "Epoch (critic) 23 Loss=11.71089 t1=1.29739 t2=10.41349\n",
      "Epoch (critic) 24 Loss=13.55338 t1=1.55844 t2=11.99494\n",
      "Epoch (critic) 25 Loss=22.43976 t1=1.32619 t2=21.11357\n",
      "Epoch (critic) 26 Loss=18.90323 t1=1.16318 t2=17.74005\n",
      "Epoch (critic) 27 Loss=20.05975 t1=0.72949 t2=19.33027\n",
      "Epoch (critic) 28 Loss=17.90827 t1=1.75253 t2=16.15574\n",
      "Epoch (critic) 29 Loss=12.62898 t1=2.03556 t2=10.59342\n",
      "Epoch (critic) 30 Loss=12.13240 t1=2.15830 t2=9.97410\n",
      "Epoch (critic) 31 Loss=14.55107 t1=1.82667 t2=12.72439\n",
      "Epoch (critic) 32 Loss=17.05354 t1=1.21396 t2=15.83958\n",
      "Epoch (critic) 33 Loss=10.60163 t1=1.58477 t2=9.01685\n",
      "Epoch (critic) 34 Loss=12.58941 t1=1.89172 t2=10.69768\n",
      "Epoch (critic) 35 Loss=9.14847 t1=1.77040 t2=7.37807\n",
      "Epoch (critic) 36 Loss=11.01633 t1=1.62854 t2=9.38779\n",
      "Epoch (critic) 37 Loss=16.74030 t1=1.46351 t2=15.27679\n",
      "Epoch (critic) 38 Loss=17.62327 t1=1.99152 t2=15.63175\n",
      "Epoch (critic) 39 Loss=13.77535 t1=1.91439 t2=11.86096\n",
      "Epoch (critic) 40 Loss=14.15946 t1=1.66926 t2=12.49020\n",
      "Epoch (score) 41 Loss=18.92341 t1=1.48178 t2=17.44162\n",
      "Epoch (critic) 42 Loss=25.02277 t1=0.31357 t2=24.70920\n",
      "Epoch (critic) 43 Loss=17.77109 t1=0.90688 t2=16.86421\n",
      "Epoch (critic) 44 Loss=17.46018 t1=1.35139 t2=16.10880\n",
      "Epoch (critic) 45 Loss=33.96977 t1=0.79018 t2=33.17959\n",
      "Epoch (critic) 46 Loss=17.21888 t1=0.34684 t2=16.87204\n",
      "Epoch (critic) 47 Loss=14.64894 t1=0.56916 t2=14.07979\n",
      "Epoch (critic) 48 Loss=19.62794 t1=0.63624 t2=18.99171\n",
      "Epoch (critic) 49 Loss=20.08107 t1=0.74099 t2=19.34007\n",
      "Epoch (critic) 50 Loss=8.58238 t1=0.79423 t2=7.78815\n",
      "Epoch (critic) 51 Loss=16.13598 t1=1.16576 t2=14.97023\n",
      "Epoch (critic) 52 Loss=6.93748 t1=1.05011 t2=5.88737\n",
      "Epoch (critic) 53 Loss=8.36155 t1=0.96711 t2=7.39444\n",
      "Epoch (critic) 54 Loss=7.83639 t1=1.10110 t2=6.73529\n",
      "Epoch (critic) 55 Loss=13.97008 t1=0.88802 t2=13.08206\n",
      "Epoch (critic) 56 Loss=11.53094 t1=0.91993 t2=10.61101\n",
      "Epoch (critic) 57 Loss=11.08588 t1=1.06107 t2=10.02481\n",
      "Epoch (critic) 58 Loss=33.93415 t1=0.78727 t2=33.14689\n",
      "Epoch (critic) 59 Loss=8.66631 t1=1.02452 t2=7.64179\n",
      "Epoch (critic) 60 Loss=5.83376 t1=1.11297 t2=4.72080\n",
      "Epoch (critic) 61 Loss=8.20568 t1=0.53985 t2=7.66582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 62 Loss=7.37540 t1=0.93016 t2=6.44524\n",
      "Epoch (critic) 63 Loss=7.00124 t1=0.12129 t2=6.87995\n",
      "Epoch (critic) 64 Loss=7.86937 t1=0.53931 t2=7.33006\n",
      "Epoch (critic) 65 Loss=9.52598 t1=0.92837 t2=8.59761\n",
      "Epoch (critic) 66 Loss=11.68926 t1=0.75633 t2=10.93293\n",
      "Epoch (critic) 67 Loss=7.51500 t1=0.69500 t2=6.82000\n",
      "Epoch (critic) 68 Loss=8.23316 t1=0.82578 t2=7.40738\n",
      "Epoch (critic) 69 Loss=8.02761 t1=0.56316 t2=7.46445\n",
      "Epoch (critic) 70 Loss=10.57810 t1=0.43734 t2=10.14076\n",
      "Epoch (critic) 71 Loss=12.00031 t1=0.24821 t2=11.75211\n",
      "Epoch (critic) 72 Loss=15.92436 t1=0.19901 t2=15.72536\n",
      "Epoch (critic) 73 Loss=7.41022 t1=0.42220 t2=6.98802\n",
      "Epoch (critic) 74 Loss=7.39093 t1=0.20188 t2=7.18905\n",
      "Epoch (critic) 75 Loss=12.41480 t1=0.39094 t2=12.02386\n",
      "Epoch (critic) 76 Loss=8.25627 t1=0.51802 t2=7.73824\n",
      "Epoch (critic) 77 Loss=12.68285 t1=0.36426 t2=12.31858\n",
      "Epoch (critic) 78 Loss=10.35757 t1=0.19979 t2=10.15778\n",
      "Epoch (critic) 79 Loss=12.85553 t1=0.10625 t2=12.74928\n",
      "Epoch (critic) 80 Loss=14.65753 t1=0.56702 t2=14.09051\n",
      "Epoch (critic) 81 Loss=22.09848 t1=0.68553 t2=21.41295\n",
      "Epoch (critic) 82 Loss=21.59179 t1=0.70888 t2=20.88290\n",
      "Epoch (score) 83 Loss=29.73006 t1=0.77513 t2=28.95494\n",
      "Epoch (critic) 84 Loss=28.76951 t1=-0.15837 t2=28.92788\n",
      "Epoch (critic) 85 Loss=9.32116 t1=-0.29013 t2=9.61130\n",
      "Epoch (critic) 86 Loss=10.34683 t1=0.50062 t2=9.84621\n",
      "Epoch (critic) 87 Loss=14.91116 t1=0.70283 t2=14.20833\n",
      "Epoch (critic) 88 Loss=11.60876 t1=0.26847 t2=11.34030\n",
      "Epoch (critic) 89 Loss=14.86418 t1=0.27780 t2=14.58638\n",
      "Epoch (critic) 90 Loss=22.34995 t1=0.23547 t2=22.11448\n",
      "Epoch (critic) 91 Loss=18.10430 t1=0.27422 t2=17.83008\n",
      "Epoch (critic) 92 Loss=24.66511 t1=0.52908 t2=24.13603\n",
      "Epoch (critic) 93 Loss=17.76686 t1=0.41552 t2=17.35134\n",
      "Epoch (critic) 94 Loss=29.11917 t1=0.47992 t2=28.63924\n",
      "Epoch (critic) 95 Loss=16.29745 t1=0.86529 t2=15.43216\n",
      "Epoch (critic) 96 Loss=21.81555 t1=0.51372 t2=21.30184\n",
      "Epoch (critic) 97 Loss=24.03902 t1=0.36724 t2=23.67177\n",
      "Epoch (critic) 98 Loss=26.23005 t1=0.09993 t2=26.13012\n",
      "Epoch (critic) 99 Loss=11.20871 t1=0.24496 t2=10.96375\n",
      "Epoch (critic) 100 Loss=32.88643 t1=0.56653 t2=32.31990\n",
      "Epoch (critic) 101 Loss=53.11806 t1=0.54846 t2=52.56960\n",
      "Epoch (critic) 102 Loss=15.46785 t1=0.45302 t2=15.01483\n",
      "Epoch (critic) 103 Loss=14.90666 t1=0.15831 t2=14.74835\n",
      "Epoch (score) 104 Loss=18.85161 t1=-0.18224 t2=19.03384\n",
      "Epoch (critic) 105 Loss=20.43316 t1=-0.86768 t2=21.30084\n",
      "Epoch (critic) 106 Loss=17.40093 t1=-0.72324 t2=18.12417\n",
      "Epoch (critic) 107 Loss=16.85343 t1=0.19849 t2=16.65495\n",
      "Epoch (critic) 108 Loss=19.56175 t1=0.85639 t2=18.70536\n",
      "Epoch (critic) 109 Loss=27.95022 t1=0.46347 t2=27.48675\n",
      "Epoch (critic) 110 Loss=32.75813 t1=0.76451 t2=31.99362\n",
      "Epoch (critic) 111 Loss=36.25686 t1=0.76682 t2=35.49003\n",
      "Epoch (critic) 112 Loss=30.85903 t1=0.92222 t2=29.93680\n",
      "Epoch (critic) 113 Loss=11.82238 t1=1.46180 t2=10.36058\n",
      "Epoch (critic) 114 Loss=11.70806 t1=1.25764 t2=10.45042\n",
      "Epoch (critic) 115 Loss=11.30651 t1=0.96264 t2=10.34387\n",
      "Epoch (critic) 116 Loss=37.40956 t1=0.22197 t2=37.18760\n",
      "Epoch (critic) 117 Loss=4.81065 t1=0.27565 t2=4.53500\n",
      "Epoch (critic) 118 Loss=6.88070 t1=0.19281 t2=6.68789\n",
      "Epoch (critic) 119 Loss=11.97887 t1=0.37456 t2=11.60432\n",
      "Epoch (critic) 120 Loss=17.93935 t1=0.78724 t2=17.15211\n",
      "Epoch (critic) 121 Loss=16.53282 t1=0.79952 t2=15.73330\n",
      "Epoch (critic) 122 Loss=23.29831 t1=0.91551 t2=22.38280\n",
      "Epoch (critic) 123 Loss=3.85960 t1=1.19614 t2=2.66346\n",
      "Epoch (critic) 124 Loss=5.77208 t1=1.20511 t2=4.56698\n",
      "Epoch (score) 125 Loss=5.05932 t1=1.13313 t2=3.92618\n",
      "Epoch (critic) 126 Loss=5.37453 t1=0.60374 t2=4.77079\n",
      "Epoch (critic) 127 Loss=3.78253 t1=0.54913 t2=3.23340\n",
      "Epoch (critic) 128 Loss=4.26562 t1=0.48042 t2=3.78521\n",
      "Epoch (critic) 129 Loss=6.01913 t1=0.44379 t2=5.57535\n",
      "Epoch (critic) 130 Loss=32.91211 t1=0.61396 t2=32.29815\n",
      "Epoch (critic) 131 Loss=4.87303 t1=0.70724 t2=4.16578\n",
      "Epoch (critic) 132 Loss=46.09803 t1=0.74622 t2=45.35181\n",
      "Epoch (critic) 133 Loss=5.46818 t1=1.21900 t2=4.24918\n",
      "Epoch (critic) 134 Loss=8.37462 t1=1.21354 t2=7.16109\n",
      "Epoch (critic) 135 Loss=4.28414 t1=0.86336 t2=3.42078\n",
      "Epoch (critic) 136 Loss=27.34358 t1=0.96745 t2=26.37613\n",
      "Epoch (critic) 137 Loss=9.73105 t1=1.19834 t2=8.53271\n",
      "Epoch (critic) 138 Loss=24.02100 t1=1.23382 t2=22.78718\n",
      "Epoch (critic) 139 Loss=16.53072 t1=1.02478 t2=15.50594\n",
      "Epoch (critic) 140 Loss=7.02476 t1=1.17462 t2=5.85013\n",
      "Epoch (critic) 141 Loss=5.56405 t1=1.23629 t2=4.32776\n",
      "Epoch (critic) 142 Loss=5.64315 t1=1.26127 t2=4.38188\n",
      "Epoch (critic) 143 Loss=6.09140 t1=1.15288 t2=4.93852\n",
      "Epoch (critic) 144 Loss=7.44865 t1=1.00914 t2=6.43951\n",
      "Epoch (critic) 145 Loss=8.05523 t1=0.88015 t2=7.17508\n",
      "Epoch (score) 146 Loss=9.97917 t1=0.82341 t2=9.15576\n",
      "Epoch (critic) 147 Loss=10.17067 t1=0.63787 t2=9.53280\n",
      "Epoch (critic) 148 Loss=11.91705 t1=0.69837 t2=11.21868\n",
      "Epoch (critic) 149 Loss=6.19494 t1=0.47618 t2=5.71876\n",
      "Epoch (critic) 150 Loss=6.87935 t1=0.51406 t2=6.36529\n",
      "Epoch (critic) 151 Loss=8.33197 t1=0.60639 t2=7.72558\n",
      "Epoch (critic) 152 Loss=9.35790 t1=0.66535 t2=8.69256\n",
      "Epoch (critic) 153 Loss=12.81110 t1=0.74844 t2=12.06266\n",
      "Epoch (critic) 154 Loss=15.43317 t1=0.79375 t2=14.63942\n",
      "Epoch (critic) 155 Loss=23.65361 t1=0.68780 t2=22.96581\n",
      "Epoch (critic) 156 Loss=21.41358 t1=0.54991 t2=20.86367\n",
      "Epoch (critic) 157 Loss=23.41782 t1=0.54029 t2=22.87753\n",
      "Epoch (critic) 158 Loss=18.53078 t1=0.24419 t2=18.28660\n",
      "Epoch (critic) 159 Loss=6.77666 t1=0.38666 t2=6.38999\n",
      "Epoch (critic) 160 Loss=22.71981 t1=0.34206 t2=22.37776\n",
      "Epoch (critic) 161 Loss=7.11001 t1=-0.02127 t2=7.13128\n",
      "Epoch (critic) 162 Loss=8.84956 t1=0.08455 t2=8.76501\n",
      "Epoch (critic) 163 Loss=11.33385 t1=0.30394 t2=11.02991\n",
      "Epoch (critic) 164 Loss=16.94996 t1=0.61445 t2=16.33551\n",
      "Epoch (critic) 165 Loss=14.11340 t1=1.08488 t2=13.02852\n",
      "Epoch (critic) 166 Loss=18.17438 t1=1.09488 t2=17.07950\n",
      "Epoch (score) 167 Loss=23.33703 t1=1.03514 t2=22.30189\n",
      "Epoch (critic) 168 Loss=21.57224 t1=0.62291 t2=20.94933\n",
      "Epoch (critic) 169 Loss=18.13503 t1=1.11535 t2=17.01968\n",
      "Epoch (critic) 170 Loss=25.32370 t1=0.99191 t2=24.33180\n",
      "Epoch (critic) 171 Loss=33.27473 t1=0.88439 t2=32.39034\n",
      "Epoch (critic) 172 Loss=18.18972 t1=0.51864 t2=17.67108\n",
      "Epoch (critic) 173 Loss=31.45189 t1=0.20382 t2=31.24806\n",
      "Epoch (critic) 174 Loss=27.76799 t1=0.22069 t2=27.54730\n",
      "Epoch (critic) 175 Loss=8.40756 t1=0.28042 t2=8.12714\n",
      "Epoch (critic) 176 Loss=5.68856 t1=0.64574 t2=5.04282\n",
      "Epoch (critic) 177 Loss=5.39660 t1=0.81939 t2=4.57720\n",
      "Epoch (critic) 178 Loss=4.96301 t1=0.84886 t2=4.11415\n",
      "Epoch (critic) 179 Loss=4.50857 t1=0.75050 t2=3.75807\n",
      "Epoch (critic) 180 Loss=4.25199 t1=0.57614 t2=3.67585\n",
      "Epoch (critic) 181 Loss=4.23109 t1=0.47755 t2=3.75354\n",
      "Epoch (critic) 182 Loss=4.86374 t1=0.39931 t2=4.46443\n",
      "Epoch (critic) 183 Loss=5.33398 t1=0.36347 t2=4.97051\n",
      "Epoch (critic) 184 Loss=6.73005 t1=0.44544 t2=6.28461\n",
      "Epoch (critic) 185 Loss=7.98594 t1=0.51519 t2=7.47075\n",
      "Epoch (critic) 186 Loss=9.68602 t1=0.60940 t2=9.07662\n",
      "Epoch (critic) 187 Loss=13.86335 t1=0.66061 t2=13.20273\n",
      "Epoch (score) 188 Loss=14.16215 t1=0.64288 t2=13.51927\n",
      "Epoch (critic) 189 Loss=15.87881 t1=0.35671 t2=15.52210\n",
      "Epoch (critic) 190 Loss=15.99351 t1=0.39390 t2=15.59961\n",
      "Epoch (critic) 191 Loss=21.38626 t1=0.43488 t2=20.95137\n",
      "Epoch (critic) 192 Loss=15.78196 t1=0.55326 t2=15.22871\n",
      "Epoch (critic) 193 Loss=21.10818 t1=0.62850 t2=20.47968\n",
      "Epoch (critic) 194 Loss=9.83911 t1=0.24969 t2=9.58942\n",
      "Epoch (critic) 195 Loss=12.58481 t1=0.04708 t2=12.53773\n",
      "Epoch (critic) 196 Loss=8.61798 t1=0.03072 t2=8.58727\n",
      "Epoch (critic) 197 Loss=7.42090 t1=0.07746 t2=7.34344\n",
      "Epoch (critic) 198 Loss=8.11325 t1=0.17947 t2=7.93379\n",
      "Epoch (critic) 199 Loss=9.91281 t1=0.27152 t2=9.64129\n",
      "Epoch (critic) 200 Loss=10.95135 t1=0.37665 t2=10.57471\n",
      "Epoch (critic) 201 Loss=28.00022 t1=0.55569 t2=27.44453\n",
      "Epoch (critic) 202 Loss=30.61022 t1=0.52301 t2=30.08721\n",
      "Epoch (critic) 203 Loss=20.17348 t1=0.38782 t2=19.78567\n",
      "Epoch (critic) 204 Loss=19.60296 t1=0.24310 t2=19.35985\n",
      "Epoch (critic) 205 Loss=24.08258 t1=0.40796 t2=23.67462\n",
      "Epoch (critic) 206 Loss=17.70136 t1=0.28472 t2=17.41664\n",
      "Epoch (critic) 207 Loss=19.63708 t1=0.26755 t2=19.36953\n",
      "Epoch (critic) 208 Loss=60.30883 t1=0.25033 t2=60.05850\n",
      "Epoch (score) 209 Loss=39.29744 t1=0.21053 t2=39.08691\n",
      "Epoch (critic) 210 Loss=9.87751 t1=-0.26319 t2=10.14070\n",
      "Epoch (critic) 211 Loss=10.65286 t1=-0.20911 t2=10.86197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 212 Loss=7.85992 t1=-0.07073 t2=7.93065\n",
      "Epoch (critic) 213 Loss=8.07143 t1=0.04872 t2=8.02271\n",
      "Epoch (critic) 214 Loss=10.78274 t1=0.12619 t2=10.65656\n",
      "Epoch (critic) 215 Loss=13.72184 t1=0.22058 t2=13.50126\n",
      "Epoch (critic) 216 Loss=18.93133 t1=0.44212 t2=18.48922\n",
      "Epoch (critic) 217 Loss=20.77901 t1=0.27883 t2=20.50019\n",
      "Epoch (critic) 218 Loss=30.47858 t1=0.53504 t2=29.94354\n",
      "Epoch (critic) 219 Loss=31.98213 t1=0.50704 t2=31.47509\n",
      "Epoch (critic) 220 Loss=36.20663 t1=0.33235 t2=35.87429\n",
      "Epoch (critic) 221 Loss=33.08251 t1=0.04379 t2=33.03872\n",
      "Epoch (critic) 222 Loss=32.26554 t1=0.07961 t2=32.18593\n",
      "Epoch (critic) 223 Loss=17.94314 t1=-0.37946 t2=18.32259\n",
      "Epoch (critic) 224 Loss=43.43725 t1=0.90952 t2=42.52772\n",
      "Epoch (critic) 225 Loss=17.86088 t1=0.60707 t2=17.25381\n",
      "Epoch (critic) 226 Loss=24.63659 t1=0.30291 t2=24.33368\n",
      "Epoch (critic) 227 Loss=20.37271 t1=0.22886 t2=20.14385\n",
      "Epoch (critic) 228 Loss=21.20286 t1=0.34059 t2=20.86227\n",
      "Epoch (critic) 229 Loss=18.39295 t1=0.33609 t2=18.05685\n",
      "Epoch (score) 230 Loss=23.00624 t1=0.58236 t2=22.42389\n",
      "Epoch (critic) 231 Loss=21.74915 t1=0.31069 t2=21.43846\n",
      "Epoch (critic) 232 Loss=27.94931 t1=0.36617 t2=27.58314\n",
      "Epoch (critic) 233 Loss=35.56817 t1=-0.09001 t2=35.65818\n",
      "Epoch (critic) 234 Loss=25.86641 t1=0.40257 t2=25.46384\n",
      "Epoch (critic) 235 Loss=45.43161 t1=0.03907 t2=45.39253\n",
      "Epoch (critic) 236 Loss=25.41854 t1=0.57140 t2=24.84715\n",
      "Epoch (critic) 237 Loss=32.06925 t1=0.61107 t2=31.45818\n",
      "Epoch (critic) 238 Loss=32.24350 t1=0.53249 t2=31.71101\n",
      "Epoch (critic) 239 Loss=25.64217 t1=0.24473 t2=25.39744\n",
      "Epoch (critic) 240 Loss=21.76231 t1=0.10587 t2=21.65644\n",
      "Epoch (critic) 241 Loss=34.69527 t1=-0.34762 t2=35.04288\n",
      "Epoch (critic) 242 Loss=33.58560 t1=-0.13112 t2=33.71672\n",
      "Epoch (critic) 243 Loss=32.79630 t1=0.06996 t2=32.72633\n",
      "Epoch (critic) 244 Loss=39.17992 t1=0.18204 t2=38.99788\n",
      "Epoch (critic) 245 Loss=42.13232 t1=0.20532 t2=41.92700\n",
      "Epoch (critic) 246 Loss=39.21155 t1=0.00607 t2=39.20548\n",
      "Epoch (critic) 247 Loss=37.83765 t1=0.61167 t2=37.22598\n",
      "Epoch (critic) 248 Loss=42.87313 t1=0.40810 t2=42.46503\n",
      "Epoch (critic) 249 Loss=31.00604 t1=0.45404 t2=30.55199\n",
      "Epoch (critic) 250 Loss=59.01703 t1=-0.33145 t2=59.34848\n",
      "Epoch (score) 251 Loss=70.10896 t1=-0.17696 t2=70.28591\n",
      "Epoch (critic) 252 Loss=83.12621 t1=-0.25436 t2=83.38058\n",
      "Epoch (critic) 253 Loss=96.36675 t1=-0.52922 t2=96.89599\n",
      "Epoch (critic) 254 Loss=54.92244 t1=0.89882 t2=54.02362\n",
      "Epoch (critic) 255 Loss=31.39863 t1=0.27717 t2=31.12145\n",
      "Epoch (critic) 256 Loss=25.88025 t1=0.55021 t2=25.33004\n",
      "Epoch (critic) 257 Loss=12.22107 t1=0.62948 t2=11.59159\n",
      "Epoch (critic) 258 Loss=9.98872 t1=0.50726 t2=9.48146\n",
      "Epoch (critic) 259 Loss=9.86387 t1=0.13406 t2=9.72981\n",
      "Epoch (critic) 260 Loss=12.45802 t1=-0.15360 t2=12.61162\n",
      "Epoch (critic) 261 Loss=12.14206 t1=-0.21844 t2=12.36050\n",
      "Epoch (critic) 262 Loss=9.97177 t1=-0.13288 t2=10.10465\n",
      "Epoch (critic) 263 Loss=23.83733 t1=0.14603 t2=23.69130\n",
      "Epoch (critic) 264 Loss=54.67103 t1=0.31590 t2=54.35513\n",
      "Epoch (critic) 265 Loss=67.58130 t1=-0.08344 t2=67.66473\n",
      "Epoch (critic) 266 Loss=66.95464 t1=0.16026 t2=66.79437\n",
      "Epoch (critic) 267 Loss=19.18823 t1=-0.34966 t2=19.53788\n",
      "Epoch (critic) 268 Loss=26.22517 t1=-0.57350 t2=26.79868\n",
      "Epoch (critic) 269 Loss=29.64614 t1=-0.31079 t2=29.95693\n",
      "Epoch (critic) 270 Loss=34.35434 t1=-0.41547 t2=34.76981\n",
      "Epoch (critic) 271 Loss=16.05769 t1=-0.00640 t2=16.06409\n",
      "Epoch (score) 272 Loss=28.65285 t1=-0.44120 t2=29.09404\n",
      "Epoch (critic) 273 Loss=26.61439 t1=-0.79760 t2=27.41199\n",
      "Epoch (critic) 274 Loss=25.05144 t1=-0.67362 t2=25.72506\n",
      "Epoch (critic) 275 Loss=25.74221 t1=-0.33953 t2=26.08174\n",
      "Epoch (critic) 276 Loss=37.87695 t1=-0.58253 t2=38.45948\n",
      "Epoch (critic) 277 Loss=13.32390 t1=0.05548 t2=13.26843\n",
      "Epoch (critic) 278 Loss=15.70019 t1=0.32518 t2=15.37501\n",
      "Epoch (critic) 279 Loss=17.17406 t1=0.59496 t2=16.57911\n",
      "Epoch (critic) 280 Loss=26.83696 t1=0.48585 t2=26.35110\n",
      "Epoch (critic) 281 Loss=13.40412 t1=0.34621 t2=13.05791\n",
      "Epoch (critic) 282 Loss=30.44540 t1=0.84848 t2=29.59692\n",
      "Epoch (critic) 283 Loss=15.71373 t1=0.11380 t2=15.59993\n",
      "Epoch (critic) 284 Loss=23.63403 t1=-0.16569 t2=23.79971\n",
      "Epoch (critic) 285 Loss=69.98425 t1=0.04573 t2=69.93851\n",
      "Epoch (critic) 286 Loss=13.19424 t1=-0.60281 t2=13.79705\n",
      "Epoch (critic) 287 Loss=10.02669 t1=-0.38283 t2=10.40952\n",
      "Epoch (critic) 288 Loss=14.39477 t1=-0.02009 t2=14.41486\n",
      "Epoch (critic) 289 Loss=19.72522 t1=-0.06449 t2=19.78971\n",
      "Epoch (critic) 290 Loss=29.43180 t1=0.00989 t2=29.42191\n",
      "Epoch (critic) 291 Loss=27.51752 t1=0.10316 t2=27.41436\n",
      "Epoch (critic) 292 Loss=40.08950 t1=0.22980 t2=39.85970\n",
      "Epoch (score) 293 Loss=76.74047 t1=0.07839 t2=76.66207\n",
      "Epoch (critic) 294 Loss=58.99658 t1=0.73234 t2=58.26425\n",
      "Epoch (critic) 295 Loss=85.71008 t1=-0.22263 t2=85.93273\n",
      "Epoch (critic) 296 Loss=56.76640 t1=1.25021 t2=55.51619\n",
      "Epoch (critic) 297 Loss=30.77686 t1=0.65047 t2=30.12639\n",
      "Epoch (critic) 298 Loss=13.50186 t1=1.28075 t2=12.22111\n",
      "Epoch (critic) 299 Loss=53.46331 t1=0.55757 t2=52.90573\n",
      "Epoch (critic) 300 Loss=74.85577 t1=-0.07055 t2=74.92632\n",
      "Epoch (critic) 301 Loss=62.86879 t1=-0.22779 t2=63.09657\n",
      "Epoch (critic) 302 Loss=54.40693 t1=0.83957 t2=53.56736\n",
      "Epoch (critic) 303 Loss=76.62041 t1=0.83945 t2=75.78095\n",
      "Epoch (critic) 304 Loss=68.58654 t1=-0.30051 t2=68.88705\n",
      "Epoch (critic) 305 Loss=78.39065 t1=0.98076 t2=77.40990\n",
      "Epoch (critic) 306 Loss=65.58289 t1=0.54910 t2=65.03378\n",
      "Epoch (critic) 307 Loss=55.83434 t1=-0.57443 t2=56.40877\n",
      "Epoch (critic) 308 Loss=91.15237 t1=-0.10363 t2=91.25600\n",
      "Epoch (critic) 309 Loss=61.84153 t1=-0.64371 t2=62.48525\n",
      "Epoch (critic) 310 Loss=69.64071 t1=-1.01900 t2=70.65972\n",
      "Epoch (critic) 311 Loss=87.08584 t1=0.94697 t2=86.13886\n",
      "Epoch (critic) 312 Loss=74.11346 t1=0.85619 t2=73.25728\n",
      "Epoch (critic) 313 Loss=82.74334 t1=0.80770 t2=81.93563\n",
      "Epoch (score) 314 Loss=107.38680 t1=-0.53592 t2=107.92273\n",
      "Epoch (critic) 315 Loss=99.63165 t1=0.35304 t2=99.27860\n",
      "Epoch (critic) 316 Loss=66.20431 t1=-0.42204 t2=66.62635\n",
      "Epoch (critic) 317 Loss=76.01330 t1=-1.03721 t2=77.05050\n",
      "Epoch (critic) 318 Loss=78.19418 t1=-1.60363 t2=79.79783\n",
      "Epoch (critic) 319 Loss=102.05741 t1=1.24311 t2=100.81431\n",
      "Epoch (critic) 320 Loss=82.82784 t1=1.37028 t2=81.45757\n",
      "Epoch (critic) 321 Loss=97.75230 t1=1.45315 t2=96.29913\n",
      "Epoch (critic) 322 Loss=57.72773 t1=-0.26558 t2=57.99332\n",
      "Epoch (critic) 323 Loss=117.87582 t1=0.00846 t2=117.86736\n",
      "Epoch (critic) 324 Loss=122.41912 t1=0.97457 t2=121.44456\n",
      "Epoch (critic) 325 Loss=137.89897 t1=-0.12701 t2=138.02600\n",
      "Epoch (critic) 326 Loss=69.61089 t1=1.61472 t2=67.99618\n",
      "Epoch (critic) 327 Loss=117.81191 t1=0.69829 t2=117.11362\n",
      "Epoch (critic) 328 Loss=63.93401 t1=-1.23755 t2=65.17155\n",
      "Epoch (critic) 329 Loss=36.85305 t1=-1.02981 t2=37.88285\n",
      "Epoch (critic) 330 Loss=73.78117 t1=-0.23393 t2=74.01509\n",
      "Epoch (critic) 331 Loss=84.94173 t1=-0.67466 t2=85.61640\n",
      "Epoch (critic) 332 Loss=131.14056 t1=0.50281 t2=130.63776\n",
      "Epoch (critic) 333 Loss=118.26084 t1=0.41676 t2=117.84407\n",
      "Epoch (critic) 334 Loss=125.80724 t1=-0.83133 t2=126.63855\n",
      "Epoch (score) 335 Loss=82.90089 t1=1.05558 t2=81.84530\n",
      "Epoch (critic) 336 Loss=30.21304 t1=-0.09139 t2=30.30444\n",
      "Epoch (critic) 337 Loss=43.30962 t1=0.19669 t2=43.11293\n",
      "Epoch (critic) 338 Loss=53.98386 t1=0.27125 t2=53.71262\n",
      "Epoch (critic) 339 Loss=92.39013 t1=2.78728 t2=89.60285\n",
      "Epoch (critic) 340 Loss=81.25941 t1=2.77330 t2=78.48611\n",
      "Epoch (critic) 341 Loss=100.53307 t1=1.27620 t2=99.25687\n",
      "Epoch (critic) 342 Loss=79.32991 t1=-0.29981 t2=79.62972\n",
      "Epoch (critic) 343 Loss=104.16064 t1=1.44916 t2=102.71148\n",
      "Epoch (critic) 344 Loss=124.68359 t1=-0.77557 t2=125.45917\n",
      "Epoch (critic) 345 Loss=79.40192 t1=3.22946 t2=76.17246\n",
      "Epoch (critic) 346 Loss=72.89491 t1=2.86900 t2=70.02590\n",
      "Epoch (critic) 347 Loss=32.27711 t1=-1.97068 t2=34.24778\n",
      "Epoch (critic) 348 Loss=69.06525 t1=0.27096 t2=68.79428\n",
      "Epoch (critic) 349 Loss=66.73676 t1=1.95807 t2=64.77869\n",
      "Epoch (critic) 350 Loss=44.78691 t1=-0.01920 t2=44.80611\n",
      "Epoch (critic) 351 Loss=44.91029 t1=0.21399 t2=44.69630\n",
      "Epoch (critic) 352 Loss=94.00034 t1=0.05404 t2=93.94629\n",
      "Epoch (critic) 353 Loss=99.70034 t1=-0.03695 t2=99.73728\n",
      "Epoch (critic) 354 Loss=17.24606 t1=0.33734 t2=16.90872\n",
      "Epoch (critic) 355 Loss=28.91312 t1=-0.32373 t2=29.23685\n",
      "Epoch (score) 356 Loss=85.76408 t1=1.42812 t2=84.33595\n",
      "Epoch (critic) 357 Loss=67.99326 t1=-0.96974 t2=68.96299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 358 Loss=36.97232 t1=2.38044 t2=34.59188\n",
      "Epoch (critic) 359 Loss=38.47107 t1=1.86733 t2=36.60374\n",
      "Epoch (critic) 360 Loss=19.05404 t1=2.40391 t2=16.65013\n",
      "Epoch (critic) 361 Loss=34.92704 t1=1.62911 t2=33.29794\n",
      "Epoch (critic) 362 Loss=33.99706 t1=1.86616 t2=32.13090\n",
      "Epoch (critic) 363 Loss=31.72571 t1=1.07697 t2=30.64875\n",
      "Epoch (critic) 364 Loss=45.60816 t1=0.95799 t2=44.65017\n",
      "Epoch (critic) 365 Loss=31.27006 t1=1.33652 t2=29.93354\n",
      "Epoch (critic) 366 Loss=26.19077 t1=1.09304 t2=25.09773\n",
      "Epoch (critic) 367 Loss=38.61103 t1=0.52298 t2=38.08805\n",
      "Epoch (critic) 368 Loss=45.52943 t1=-0.03197 t2=45.56140\n",
      "Epoch (critic) 369 Loss=38.23691 t1=-0.13422 t2=38.37114\n",
      "Epoch (critic) 370 Loss=34.25155 t1=-0.39364 t2=34.64520\n",
      "Epoch (critic) 371 Loss=45.33377 t1=-1.43925 t2=46.77303\n",
      "Epoch (critic) 372 Loss=49.90918 t1=-1.18700 t2=51.09618\n",
      "Epoch (critic) 373 Loss=55.25904 t1=-1.55266 t2=56.81170\n",
      "Epoch (critic) 374 Loss=56.29768 t1=0.06124 t2=56.23644\n",
      "Epoch (critic) 375 Loss=67.44949 t1=0.43236 t2=67.01714\n",
      "Epoch (critic) 376 Loss=38.10068 t1=1.14602 t2=36.95466\n",
      "Epoch (score) 377 Loss=39.98415 t1=1.03392 t2=38.95023\n",
      "Epoch (critic) 378 Loss=38.71586 t1=0.67673 t2=38.03913\n",
      "Epoch (critic) 379 Loss=47.09492 t1=0.55915 t2=46.53577\n",
      "Epoch (critic) 380 Loss=97.29236 t1=-0.66164 t2=97.95400\n",
      "Epoch (critic) 381 Loss=82.39400 t1=-0.11841 t2=82.51241\n",
      "Epoch (critic) 382 Loss=98.29806 t1=-0.18518 t2=98.48323\n",
      "Epoch (critic) 383 Loss=70.55611 t1=-1.32673 t2=71.88284\n",
      "Epoch (critic) 384 Loss=53.81763 t1=0.98965 t2=52.82798\n",
      "Epoch (critic) 385 Loss=94.69259 t1=-0.53591 t2=95.22851\n",
      "Epoch (critic) 386 Loss=48.40626 t1=1.83871 t2=46.56755\n",
      "Epoch (critic) 387 Loss=105.48367 t1=1.21593 t2=104.26774\n",
      "Epoch (critic) 388 Loss=126.54551 t1=1.06662 t2=125.47888\n",
      "Epoch (critic) 389 Loss=117.28172 t1=0.16507 t2=117.11664\n",
      "Epoch (critic) 390 Loss=208.46773 t1=-1.63516 t2=210.10289\n",
      "Epoch (critic) 391 Loss=132.05203 t1=0.32316 t2=131.72887\n",
      "Epoch (critic) 392 Loss=37.20126 t1=-1.38002 t2=38.58129\n",
      "Epoch (critic) 393 Loss=117.61590 t1=0.97346 t2=116.64245\n",
      "Epoch (critic) 394 Loss=167.08310 t1=0.11271 t2=166.97041\n",
      "Epoch (critic) 395 Loss=61.33506 t1=-1.85308 t2=63.18815\n",
      "Epoch (critic) 396 Loss=129.14986 t1=1.41436 t2=127.73549\n",
      "Epoch (critic) 397 Loss=74.66589 t1=1.40597 t2=73.25993\n",
      "Epoch (score) 398 Loss=78.77907 t1=-1.48758 t2=80.26664\n",
      "Epoch (critic) 399 Loss=26.87072 t1=-2.03750 t2=28.90822\n",
      "Epoch (critic) 400 Loss=59.04000 t1=-1.26016 t2=60.30016\n",
      "Epoch (critic) 401 Loss=40.00206 t1=-0.97509 t2=40.97715\n",
      "Epoch (critic) 402 Loss=29.02409 t1=-0.58806 t2=29.61215\n",
      "Epoch (critic) 403 Loss=23.68119 t1=0.19795 t2=23.48324\n",
      "Epoch (critic) 404 Loss=27.71739 t1=0.67678 t2=27.04062\n",
      "Epoch (critic) 405 Loss=29.90231 t1=0.73580 t2=29.16650\n",
      "Epoch (critic) 406 Loss=44.04807 t1=0.90959 t2=43.13848\n",
      "Epoch (critic) 407 Loss=34.56484 t1=0.60225 t2=33.96258\n",
      "Epoch (critic) 408 Loss=37.31691 t1=-0.15689 t2=37.47380\n",
      "Epoch (critic) 409 Loss=44.00385 t1=-0.34554 t2=44.34939\n",
      "Epoch (critic) 410 Loss=44.54089 t1=-0.41166 t2=44.95255\n",
      "Epoch (critic) 411 Loss=54.72409 t1=0.10196 t2=54.62212\n",
      "Epoch (critic) 412 Loss=120.08498 t1=1.51387 t2=118.57112\n",
      "Epoch (critic) 413 Loss=62.44608 t1=-0.78588 t2=63.23196\n",
      "Epoch (critic) 414 Loss=57.60709 t1=-0.41898 t2=58.02607\n",
      "Epoch (critic) 415 Loss=74.24575 t1=-0.76616 t2=75.01192\n",
      "Epoch (critic) 416 Loss=57.65895 t1=-0.59252 t2=58.25148\n",
      "Epoch (critic) 417 Loss=63.63972 t1=0.11736 t2=63.52235\n",
      "Epoch (critic) 418 Loss=67.78888 t1=0.24647 t2=67.54241\n",
      "Epoch (score) 419 Loss=75.75677 t1=0.32310 t2=75.43368\n",
      "Epoch (critic) 420 Loss=62.94926 t1=-0.03901 t2=62.98827\n",
      "Epoch (critic) 421 Loss=78.33614 t1=0.24412 t2=78.09203\n",
      "Epoch (critic) 422 Loss=118.13346 t1=2.36951 t2=115.76394\n",
      "Epoch (critic) 423 Loss=80.10487 t1=0.06968 t2=80.03518\n",
      "Epoch (critic) 424 Loss=210.35698 t1=1.71044 t2=208.64659\n",
      "Epoch (critic) 425 Loss=167.11354 t1=1.47499 t2=165.63853\n",
      "Epoch (critic) 426 Loss=41.42401 t1=-0.77557 t2=42.19958\n",
      "Epoch (critic) 427 Loss=43.91301 t1=-0.64208 t2=44.55508\n",
      "Epoch (critic) 428 Loss=47.63234 t1=-0.59114 t2=48.22347\n",
      "Epoch (critic) 429 Loss=64.20516 t1=-0.99167 t2=65.19684\n",
      "Epoch (critic) 430 Loss=65.69948 t1=-1.07311 t2=66.77258\n",
      "Epoch (critic) 431 Loss=40.40693 t1=-0.60687 t2=41.01380\n",
      "Epoch (critic) 432 Loss=36.32124 t1=-0.88382 t2=37.20505\n",
      "Epoch (critic) 433 Loss=48.18782 t1=-0.59236 t2=48.78019\n",
      "Epoch (critic) 434 Loss=70.14834 t1=-0.22906 t2=70.37740\n",
      "Epoch (critic) 435 Loss=77.41157 t1=0.39988 t2=77.01168\n",
      "Epoch (critic) 436 Loss=68.42506 t1=-0.80627 t2=69.23133\n",
      "Epoch (critic) 437 Loss=53.36023 t1=-0.04400 t2=53.40423\n",
      "Epoch (critic) 438 Loss=101.08083 t1=0.35642 t2=100.72442\n",
      "Epoch (critic) 439 Loss=86.11292 t1=0.46018 t2=85.65273\n",
      "Epoch (score) 440 Loss=47.17877 t1=-1.26530 t2=48.44407\n",
      "Epoch (critic) 441 Loss=44.10788 t1=-1.74782 t2=45.85569\n",
      "Epoch (critic) 442 Loss=87.36205 t1=1.93130 t2=85.43076\n",
      "Epoch (critic) 443 Loss=116.08492 t1=0.39444 t2=115.69048\n",
      "Epoch (critic) 444 Loss=22.75432 t1=1.06477 t2=21.68955\n",
      "Epoch (critic) 445 Loss=17.26667 t1=0.56233 t2=16.70434\n",
      "Epoch (critic) 446 Loss=35.37606 t1=0.59121 t2=34.78486\n",
      "Epoch (critic) 447 Loss=41.11007 t1=0.21925 t2=40.89082\n",
      "Epoch (critic) 448 Loss=43.74688 t1=-0.06829 t2=43.81517\n",
      "Epoch (critic) 449 Loss=47.68291 t1=0.54617 t2=47.13673\n",
      "Epoch (critic) 450 Loss=52.54555 t1=0.68935 t2=51.85620\n",
      "Epoch (critic) 451 Loss=67.98832 t1=0.22364 t2=67.76467\n",
      "Epoch (critic) 452 Loss=72.07343 t1=0.65787 t2=71.41555\n",
      "Epoch (critic) 453 Loss=86.79064 t1=0.62860 t2=86.16205\n",
      "Epoch (critic) 454 Loss=93.93841 t1=0.79829 t2=93.14011\n",
      "Epoch (critic) 455 Loss=68.38074 t1=1.57681 t2=66.80393\n",
      "Epoch (critic) 456 Loss=34.18258 t1=2.06664 t2=32.11594\n",
      "Epoch (critic) 457 Loss=46.38975 t1=1.02452 t2=45.36523\n",
      "Epoch (critic) 458 Loss=33.33492 t1=0.41475 t2=32.92018\n",
      "Epoch (critic) 459 Loss=32.88061 t1=-0.10137 t2=32.98198\n",
      "Epoch (critic) 460 Loss=33.04069 t1=0.19628 t2=32.84441\n",
      "Epoch (score) 461 Loss=31.78977 t1=0.77092 t2=31.01885\n",
      "Epoch (critic) 462 Loss=37.97947 t1=0.46583 t2=37.51364\n",
      "Epoch (critic) 463 Loss=38.12874 t1=0.54869 t2=37.58005\n",
      "Epoch (critic) 464 Loss=45.33145 t1=0.25786 t2=45.07359\n",
      "Epoch (critic) 465 Loss=38.45864 t1=0.13165 t2=38.32700\n",
      "Epoch (critic) 466 Loss=51.73318 t1=-0.01723 t2=51.75042\n",
      "Epoch (critic) 467 Loss=42.09880 t1=0.21579 t2=41.88300\n",
      "Epoch (critic) 468 Loss=47.87899 t1=0.17107 t2=47.70792\n",
      "Epoch (critic) 469 Loss=50.42862 t1=0.30692 t2=50.12170\n",
      "Epoch (critic) 470 Loss=58.48825 t1=0.61824 t2=57.87001\n",
      "Epoch (critic) 471 Loss=59.25547 t1=0.78329 t2=58.47217\n",
      "Epoch (critic) 472 Loss=8.29963 t1=1.93085 t2=6.36878\n",
      "Epoch (critic) 473 Loss=66.69557 t1=1.23008 t2=65.46548\n",
      "Epoch (critic) 474 Loss=16.53752 t1=0.95480 t2=15.58272\n",
      "Epoch (critic) 475 Loss=4.03812 t1=1.53697 t2=2.50115\n",
      "Epoch (critic) 476 Loss=4.07734 t1=2.06041 t2=2.01694\n",
      "Epoch (critic) 477 Loss=6.58394 t1=2.26585 t2=4.31810\n",
      "Epoch (critic) 478 Loss=8.04984 t1=2.27051 t2=5.77933\n",
      "Epoch (critic) 479 Loss=9.02343 t1=2.01675 t2=7.00667\n",
      "Epoch (critic) 480 Loss=9.11733 t1=1.37516 t2=7.74216\n",
      "Epoch (critic) 481 Loss=9.78018 t1=0.94530 t2=8.83488\n",
      "Epoch (score) 482 Loss=7.59424 t1=0.66784 t2=6.92640\n",
      "Epoch (critic) 483 Loss=12.43769 t1=0.32838 t2=12.10931\n",
      "Epoch (critic) 484 Loss=7.34831 t1=0.21309 t2=7.13522\n",
      "Epoch (critic) 485 Loss=6.39292 t1=0.10301 t2=6.28990\n",
      "Epoch (critic) 486 Loss=7.09732 t1=-0.21130 t2=7.30862\n",
      "Epoch (critic) 487 Loss=8.58602 t1=0.00834 t2=8.57768\n",
      "Epoch (critic) 488 Loss=9.38539 t1=0.13890 t2=9.24650\n",
      "Epoch (critic) 489 Loss=9.87871 t1=0.24394 t2=9.63478\n",
      "Epoch (critic) 490 Loss=10.46539 t1=0.41499 t2=10.05039\n",
      "Epoch (critic) 491 Loss=13.22762 t1=0.41294 t2=12.81468\n",
      "Epoch (critic) 492 Loss=11.79896 t1=0.48949 t2=11.30947\n",
      "Epoch (critic) 493 Loss=9.15474 t1=0.52991 t2=8.62483\n",
      "Epoch (critic) 494 Loss=11.20987 t1=0.47569 t2=10.73418\n",
      "Epoch (critic) 495 Loss=20.89660 t1=0.32830 t2=20.56831\n",
      "Epoch (critic) 496 Loss=21.00912 t1=0.30346 t2=20.70566\n",
      "Epoch (critic) 497 Loss=32.19061 t1=0.07681 t2=32.11380\n",
      "Epoch (critic) 498 Loss=29.67223 t1=0.39010 t2=29.28213\n",
      "Epoch (critic) 499 Loss=37.88363 t1=0.33960 t2=37.54403\n",
      "Epoch (critic) 500 Loss=41.18829 t1=0.07119 t2=41.11710\n",
      "Epoch (critic) 501 Loss=55.50879 t1=0.90785 t2=54.60093\n",
      "Epoch (critic) 502 Loss=53.18385 t1=0.87594 t2=52.30790\n",
      "Epoch (score) 503 Loss=37.12583 t1=0.17330 t2=36.95253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 504 Loss=69.31344 t1=0.27659 t2=69.03683\n",
      "Epoch (critic) 505 Loss=32.49733 t1=1.26250 t2=31.23482\n",
      "Epoch (critic) 506 Loss=6.96052 t1=1.84472 t2=5.11580\n",
      "Epoch (critic) 507 Loss=45.40466 t1=0.54791 t2=44.85675\n",
      "Epoch (critic) 508 Loss=21.36867 t1=0.67373 t2=20.69495\n",
      "Epoch (critic) 509 Loss=5.71885 t1=-0.62098 t2=6.33983\n",
      "Epoch (critic) 510 Loss=23.86750 t1=-0.67888 t2=24.54638\n",
      "Epoch (critic) 511 Loss=47.04663 t1=0.54024 t2=46.50640\n",
      "Epoch (critic) 512 Loss=34.48725 t1=2.00221 t2=32.48504\n",
      "Epoch (critic) 513 Loss=68.49168 t1=2.50946 t2=65.98222\n",
      "Epoch (critic) 514 Loss=32.58523 t1=2.18901 t2=30.39622\n",
      "Epoch (critic) 515 Loss=42.23355 t1=0.75428 t2=41.47927\n",
      "Epoch (critic) 516 Loss=7.34086 t1=2.77455 t2=4.56631\n",
      "Epoch (critic) 517 Loss=3.04212 t1=2.94911 t2=0.09302\n",
      "Epoch (critic) 518 Loss=1.61189 t1=3.13236 t2=-1.52047\n",
      "Epoch (critic) 519 Loss=3.19660 t1=2.97476 t2=0.22184\n",
      "Epoch (critic) 520 Loss=3.24234 t1=2.74644 t2=0.49590\n",
      "Epoch (critic) 521 Loss=3.40223 t1=2.68678 t2=0.71546\n",
      "Epoch (critic) 522 Loss=2.69846 t1=2.53396 t2=0.16450\n",
      "Epoch (critic) 523 Loss=5.14800 t1=2.34817 t2=2.79983\n",
      "Epoch (score) 524 Loss=4.02588 t1=2.34492 t2=1.68096\n",
      "Epoch (critic) 525 Loss=1.88972 t1=2.29470 t2=-0.40498\n",
      "Epoch (critic) 526 Loss=3.96116 t1=2.13994 t2=1.82121\n",
      "Epoch (critic) 527 Loss=5.58926 t1=1.86687 t2=3.72239\n",
      "Epoch (critic) 528 Loss=6.73201 t1=1.67422 t2=5.05779\n",
      "Epoch (critic) 529 Loss=4.90966 t1=1.46874 t2=3.44093\n",
      "Epoch (critic) 530 Loss=5.96820 t1=1.15147 t2=4.81673\n",
      "Epoch (critic) 531 Loss=8.50485 t1=0.88100 t2=7.62386\n",
      "Epoch (critic) 532 Loss=7.79015 t1=0.52700 t2=7.26315\n",
      "Epoch (critic) 533 Loss=3.28325 t1=0.24448 t2=3.03878\n",
      "Epoch (critic) 534 Loss=8.78253 t1=0.39616 t2=8.38636\n",
      "Epoch (critic) 535 Loss=2.34801 t1=0.62866 t2=1.71935\n",
      "Epoch (critic) 536 Loss=3.60924 t1=0.66479 t2=2.94446\n",
      "Epoch (critic) 537 Loss=7.91505 t1=0.51437 t2=7.40069\n",
      "Epoch (critic) 538 Loss=9.13490 t1=0.33454 t2=8.80036\n",
      "Epoch (critic) 539 Loss=13.02132 t1=0.37532 t2=12.64600\n",
      "Epoch (critic) 540 Loss=5.50572 t1=0.50888 t2=4.99684\n",
      "Epoch (critic) 541 Loss=3.87756 t1=0.40979 t2=3.46777\n",
      "Epoch (critic) 542 Loss=9.61985 t1=0.07073 t2=9.54912\n",
      "Epoch (critic) 543 Loss=3.90362 t1=0.49676 t2=3.40686\n",
      "Epoch (critic) 544 Loss=19.92331 t1=0.23337 t2=19.68995\n",
      "Epoch (score) 545 Loss=19.62459 t1=0.24906 t2=19.37552\n",
      "Epoch (critic) 546 Loss=9.11593 t1=0.46538 t2=8.65055\n",
      "Epoch (critic) 547 Loss=21.48730 t1=0.23305 t2=21.25425\n",
      "Epoch (critic) 548 Loss=19.59589 t1=0.44456 t2=19.15133\n",
      "Epoch (critic) 549 Loss=16.29506 t1=0.77398 t2=15.52108\n",
      "Epoch (critic) 550 Loss=28.25273 t1=0.49152 t2=27.76120\n",
      "Epoch (critic) 551 Loss=25.69723 t1=0.35412 t2=25.34311\n",
      "Epoch (critic) 552 Loss=15.09662 t1=1.04064 t2=14.05598\n",
      "Epoch (critic) 553 Loss=27.13531 t1=0.43391 t2=26.70140\n",
      "Epoch (critic) 554 Loss=9.64062 t1=1.03622 t2=8.60440\n",
      "Epoch (critic) 555 Loss=29.79335 t1=1.04376 t2=28.74959\n",
      "Epoch (critic) 556 Loss=35.98158 t1=0.65615 t2=35.32543\n",
      "Epoch (critic) 557 Loss=25.15037 t1=0.70173 t2=24.44864\n",
      "Epoch (critic) 558 Loss=28.48748 t1=0.74176 t2=27.74571\n",
      "Epoch (critic) 559 Loss=30.74296 t1=0.73887 t2=30.00408\n",
      "Epoch (critic) 560 Loss=24.21459 t1=0.26748 t2=23.94711\n",
      "Epoch (critic) 561 Loss=18.00444 t1=0.55398 t2=17.45046\n",
      "Epoch (critic) 562 Loss=27.03613 t1=1.30728 t2=25.72886\n",
      "Epoch (critic) 563 Loss=26.76191 t1=1.35834 t2=25.40357\n",
      "Epoch (critic) 564 Loss=29.78296 t1=0.97827 t2=28.80471\n",
      "Epoch (critic) 565 Loss=37.09581 t1=1.34220 t2=35.75360\n",
      "Epoch (score) 566 Loss=23.84426 t1=0.67112 t2=23.17314\n",
      "Epoch (critic) 567 Loss=19.16643 t1=0.29635 t2=18.87008\n",
      "Epoch (critic) 568 Loss=34.67505 t1=1.36800 t2=33.30705\n",
      "Epoch (critic) 569 Loss=48.34929 t1=1.03545 t2=47.31383\n",
      "Epoch (critic) 570 Loss=48.49033 t1=0.90934 t2=47.58099\n",
      "Epoch (critic) 571 Loss=45.63273 t1=0.69761 t2=44.93512\n",
      "Epoch (critic) 572 Loss=55.78783 t1=0.43266 t2=55.35518\n",
      "Epoch (critic) 573 Loss=54.71447 t1=0.20048 t2=54.51398\n",
      "Epoch (critic) 574 Loss=51.91568 t1=-0.11460 t2=52.03027\n",
      "Epoch (critic) 575 Loss=34.83989 t1=2.13456 t2=32.70532\n",
      "Epoch (critic) 576 Loss=77.41702 t1=0.45823 t2=76.95879\n",
      "Epoch (critic) 577 Loss=25.76727 t1=1.35843 t2=24.40883\n",
      "Epoch (critic) 578 Loss=68.85641 t1=0.87999 t2=67.97641\n",
      "Epoch (critic) 579 Loss=42.23183 t1=1.33453 t2=40.89730\n",
      "Epoch (critic) 580 Loss=59.94197 t1=0.25812 t2=59.68386\n",
      "Epoch (critic) 581 Loss=62.17205 t1=0.74050 t2=61.43155\n",
      "Epoch (critic) 582 Loss=16.25470 t1=1.64775 t2=14.60695\n",
      "Epoch (critic) 583 Loss=59.91290 t1=0.12534 t2=59.78756\n",
      "Epoch (critic) 584 Loss=72.14025 t1=-0.13243 t2=72.27267\n",
      "Epoch (critic) 585 Loss=41.70156 t1=-0.90700 t2=42.60856\n",
      "Epoch (critic) 586 Loss=47.31178 t1=0.53564 t2=46.77613\n",
      "Epoch (score) 587 Loss=8.02994 t1=0.92404 t2=7.10590\n",
      "Epoch (critic) 588 Loss=11.22684 t1=0.69115 t2=10.53568\n",
      "Epoch (critic) 589 Loss=18.60769 t1=1.16861 t2=17.43909\n",
      "Epoch (critic) 590 Loss=37.73681 t1=1.97420 t2=35.76260\n",
      "Epoch (critic) 591 Loss=14.01549 t1=2.42710 t2=11.58838\n",
      "Epoch (critic) 592 Loss=31.79442 t1=1.23243 t2=30.56199\n",
      "Epoch (critic) 593 Loss=55.47980 t1=1.67130 t2=53.80848\n",
      "Epoch (critic) 594 Loss=49.32079 t1=0.95173 t2=48.36907\n",
      "Epoch (critic) 595 Loss=26.73507 t1=0.81862 t2=25.91645\n",
      "Epoch (critic) 596 Loss=58.72480 t1=0.56337 t2=58.16144\n",
      "Epoch (critic) 597 Loss=39.57312 t1=0.38011 t2=39.19301\n",
      "Epoch (critic) 598 Loss=51.23212 t1=0.00037 t2=51.23176\n",
      "Epoch (critic) 599 Loss=24.45440 t1=1.04857 t2=23.40583\n",
      "Epoch (critic) 600 Loss=51.48093 t1=-1.02064 t2=52.50158\n",
      "Epoch (critic) 601 Loss=73.39226 t1=-0.30746 t2=73.69974\n",
      "Epoch (critic) 602 Loss=38.56750 t1=1.35531 t2=37.21218\n",
      "Epoch (critic) 603 Loss=86.97157 t1=0.42448 t2=86.54708\n",
      "Epoch (critic) 604 Loss=68.18991 t1=0.02819 t2=68.16172\n",
      "Epoch (critic) 605 Loss=53.32801 t1=0.50177 t2=52.82624\n",
      "Epoch (critic) 606 Loss=93.28731 t1=-0.04409 t2=93.33141\n",
      "Epoch (critic) 607 Loss=23.65132 t1=1.86846 t2=21.78286\n",
      "Epoch (score) 608 Loss=57.73734 t1=1.72272 t2=56.01461\n",
      "Epoch (critic) 609 Loss=48.03896 t1=1.09563 t2=46.94334\n",
      "Epoch (critic) 610 Loss=29.32108 t1=1.72441 t2=27.59667\n",
      "Epoch (critic) 611 Loss=58.24697 t1=1.24832 t2=56.99865\n",
      "Epoch (critic) 612 Loss=52.11088 t1=-0.60844 t2=52.71931\n",
      "Epoch (critic) 613 Loss=58.00204 t1=0.66474 t2=57.33730\n",
      "Epoch (critic) 614 Loss=53.16740 t1=1.07278 t2=52.09463\n",
      "Epoch (critic) 615 Loss=72.62693 t1=0.48429 t2=72.14264\n",
      "Epoch (critic) 616 Loss=65.18223 t1=1.43683 t2=63.74540\n",
      "Epoch (critic) 617 Loss=60.73218 t1=-0.07994 t2=60.81212\n",
      "Epoch (critic) 618 Loss=60.69612 t1=-0.26818 t2=60.96429\n",
      "Epoch (critic) 619 Loss=68.42802 t1=-0.26677 t2=68.69479\n",
      "Epoch (critic) 620 Loss=80.12351 t1=0.31821 t2=79.80530\n",
      "Epoch (critic) 621 Loss=95.78728 t1=-0.31645 t2=96.10372\n",
      "Epoch (critic) 622 Loss=56.02910 t1=0.53083 t2=55.49827\n",
      "Epoch (critic) 623 Loss=48.82049 t1=0.72758 t2=48.09291\n",
      "Epoch (critic) 624 Loss=79.84214 t1=0.93904 t2=78.90310\n",
      "Epoch (critic) 625 Loss=56.04071 t1=-0.20833 t2=56.24903\n",
      "Epoch (critic) 626 Loss=98.77226 t1=0.81561 t2=97.95666\n",
      "Epoch (critic) 627 Loss=83.92098 t1=0.67822 t2=83.24276\n",
      "Epoch (critic) 628 Loss=150.05989 t1=0.76105 t2=149.29884\n",
      "Epoch (score) 629 Loss=191.45557 t1=0.68188 t2=190.77367\n",
      "Epoch (critic) 630 Loss=60.89991 t1=1.56475 t2=59.33516\n",
      "Epoch (critic) 631 Loss=84.36897 t1=-0.38389 t2=84.75285\n",
      "Epoch (critic) 632 Loss=128.60535 t1=-0.28438 t2=128.88974\n",
      "Epoch (critic) 633 Loss=94.64834 t1=0.09078 t2=94.55756\n",
      "Epoch (critic) 634 Loss=60.36614 t1=-0.75688 t2=61.12302\n",
      "Epoch (critic) 635 Loss=69.63015 t1=0.76174 t2=68.86841\n",
      "Epoch (critic) 636 Loss=71.64455 t1=0.42855 t2=71.21600\n",
      "Epoch (critic) 637 Loss=70.55478 t1=-0.50509 t2=71.05988\n",
      "Epoch (critic) 638 Loss=56.25723 t1=0.54716 t2=55.71008\n",
      "Epoch (critic) 639 Loss=107.39414 t1=-0.19269 t2=107.58681\n",
      "Epoch (critic) 640 Loss=101.08995 t1=-0.77665 t2=101.86661\n",
      "Epoch (critic) 641 Loss=29.51442 t1=-1.69012 t2=31.20455\n",
      "Epoch (critic) 642 Loss=45.15843 t1=-1.24999 t2=46.40842\n",
      "Epoch (critic) 643 Loss=83.84799 t1=-0.45527 t2=84.30327\n",
      "Epoch (critic) 644 Loss=50.76556 t1=-0.26997 t2=51.03553\n",
      "Epoch (critic) 645 Loss=59.64774 t1=0.16826 t2=59.47949\n",
      "Epoch (critic) 646 Loss=70.09457 t1=1.47148 t2=68.62310\n",
      "Epoch (critic) 647 Loss=64.12781 t1=1.14071 t2=62.98709\n",
      "Epoch (critic) 648 Loss=69.70293 t1=1.39196 t2=68.31097\n",
      "Epoch (critic) 649 Loss=64.22698 t1=2.20484 t2=62.02214\n",
      "Epoch (score) 650 Loss=61.55270 t1=1.73323 t2=59.81946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 651 Loss=68.72998 t1=1.25074 t2=67.47923\n",
      "Epoch (critic) 652 Loss=62.12374 t1=1.69989 t2=60.42386\n",
      "Epoch (critic) 653 Loss=70.35093 t1=2.45687 t2=67.89406\n",
      "Epoch (critic) 654 Loss=47.43303 t1=1.08324 t2=46.34979\n",
      "Epoch (critic) 655 Loss=50.25041 t1=1.14131 t2=49.10910\n",
      "Epoch (critic) 656 Loss=60.03160 t1=-0.07924 t2=60.11084\n",
      "Epoch (critic) 657 Loss=68.39242 t1=0.59004 t2=67.80238\n",
      "Epoch (critic) 658 Loss=52.63983 t1=1.29552 t2=51.34431\n",
      "Epoch (critic) 659 Loss=39.16830 t1=0.81130 t2=38.35700\n",
      "Epoch (critic) 660 Loss=35.83537 t1=0.65012 t2=35.18525\n",
      "Epoch (critic) 661 Loss=45.09742 t1=0.48962 t2=44.60780\n",
      "Epoch (critic) 662 Loss=56.18547 t1=0.40695 t2=55.77852\n",
      "Epoch (critic) 663 Loss=48.08339 t1=1.10297 t2=46.98042\n",
      "Epoch (critic) 664 Loss=56.58051 t1=0.79675 t2=55.78375\n",
      "Epoch (critic) 665 Loss=67.61380 t1=0.83038 t2=66.78341\n",
      "Epoch (critic) 666 Loss=34.98766 t1=-0.08065 t2=35.06831\n",
      "Epoch (critic) 667 Loss=52.03860 t1=0.94464 t2=51.09397\n",
      "Epoch (critic) 668 Loss=54.07127 t1=0.28311 t2=53.78815\n",
      "Epoch (critic) 669 Loss=66.54182 t1=1.23558 t2=65.30624\n",
      "Epoch (critic) 670 Loss=67.08584 t1=0.90099 t2=66.18485\n",
      "Epoch (score) 671 Loss=44.44901 t1=0.02297 t2=44.42604\n",
      "Epoch (critic) 672 Loss=43.82162 t1=-0.28698 t2=44.10859\n",
      "Epoch (critic) 673 Loss=96.92828 t1=1.90351 t2=95.02477\n"
     ]
    }
   ],
   "source": [
    "# score net\n",
    "score_net = Scorenet()\n",
    "critic_net = Criticnet()\n",
    "critic_net.to(device)\n",
    "score_net.to(device)\n",
    "\n",
    "print(score_net)\n",
    "print(critic_net)\n",
    "\n",
    "# optimizer\n",
    "opt_scorenet, scheduler_scorenet = get_opt(score_net.parameters(), cfg.trainer.opt_scorenet)\n",
    "opt_criticnet, scheduler_criticnet = get_opt(critic_net.parameters(), cfg.trainer.opt_scorenet)\n",
    "\n",
    "# training\n",
    "start_epoch = 0\n",
    "n_epochs = 1000\n",
    "print(\"Start epoch: %d End epoch: %d\" % (start_epoch, cfg.trainer.epochs))\n",
    "k_iters = 20\n",
    "e_iters = 1\n",
    "# for epoch in range(start_epoch, cfg.trainer.epochs):\n",
    "sigmas = sigmas[-5:]\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    score_net.train()\n",
    "    critic_net.train()\n",
    "    opt_scorenet.zero_grad()\n",
    "    opt_criticnet.zero_grad()\n",
    "    \n",
    "    perturbed_points = tr_pts + torch.randn_like(tr_pts) * sigmas[..., None]\n",
    "    \n",
    "    score_pred = score_net(perturbed_points, sigmas)\n",
    "    critic_output = critic_net(perturbed_points, sigmas)\n",
    "    \n",
    "    t1 = (score_pred * critic_output).sum(-1)\n",
    "    t2 = exact_jacobian_trace(critic_output, perturbed_points)\n",
    "    \n",
    "    stein = t1 + t2\n",
    "    l2_penalty = (critic_output * critic_output).sum(-1).mean()\n",
    "    loss = stein.mean()\n",
    "    \n",
    "    cycle_iter = epoch % (k_iters + e_iters)\n",
    "    cpu_loss = loss.detach().cpu().item()\n",
    "    cpu_t1 = t1.mean().detach().cpu().item()\n",
    "    cpu_t2 = t2.mean().detach().cpu().item()\n",
    "    if cycle_iter < k_iters:\n",
    "        (-loss + l2_penalty).backward()\n",
    "        opt_criticnet.step()\n",
    "        print(\"Epoch (critic) %d Loss=%2.5f t1=%2.5f t2=%2.5f\" % (epoch, cpu_loss, cpu_t1, cpu_t2))\n",
    "    else:\n",
    "        loss.backward()\n",
    "        opt_scorenet.step()\n",
    "        print(\"Epoch (score) %d Loss=%2.5f t1=%2.5f t2=%2.5f\" % (epoch, cpu_loss, cpu_t1, cpu_t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_cl, _ = langevin_dynamics(score_net, sigmas, eps=1e-4, num_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "visualize(pt_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(tr_pts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
