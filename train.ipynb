{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import argparse\n",
    "from torch.backends import cudnn\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def delete_module(name):\n",
    "    import sys\n",
    "    del sys.modules[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_module('scorenet')\n",
    "# from scorenet import Scorenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_module('critic')\n",
    "# from critic import Criticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pkgs/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(data=Namespace(batch_size=32, cates=['airplane'], data_dir='data/ShapeNetCore.v2.PC15k', dataset_scale=1, dataset_type='shapenet15k', normalize_per_shape=False, normalize_std_per_axis=False, num_workers=4, recenter_per_shape=True, te_max_sample_points=2048, tr_max_sample_points=2048, type='datasets.pointflow_datasets'), inference=Namespace(num_points=2048, num_steps=10, step_size_ratio=1, weight=1), models=Namespace(scorenet=Namespace(dim=3, hidden_size=256, n_blocks=24, out_dim=3, param_likelihood=False, sigma_condition=True, type='models.decoders.resnet_add', xyz_condition=True, z_dim=128)), trainer=Namespace(epochs=2000, opt_criticnet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), opt_scorenet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), seed=100, sigma_begin=1, sigma_end=0.01, sigma_num=10, type='trainers.ae_trainer_3D'), viz=Namespace(log_freq=10, save_freq=100, val_freq=100, viz_freq=5000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = './test_config.yaml'\n",
    "\n",
    "def dict2namespace(config):\n",
    "    namespace = argparse.Namespace()\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            new_value = dict2namespace(value)\n",
    "        else:\n",
    "            new_value = value\n",
    "        setattr(namespace, key, new_value)\n",
    "    return namespace\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = yaml.load(f)\n",
    "        \n",
    "cfg = dict2namespace(config)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=32, cates=['airplane'], data_dir='data/ShapeNetCore.v2.PC15k', dataset_scale=1, dataset_type='shapenet15k', normalize_per_shape=False, normalize_std_per_axis=False, num_workers=4, recenter_per_shape=True, te_max_sample_points=2048, tr_max_sample_points=2048, type='datasets.pointflow_datasets')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(epochs=2000, opt_criticnet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), opt_scorenet=Namespace(beta1=0.9, beta2=0.999, lr='1e-3', momentum=0.9, scheduler='linear', step_epoch=2000, type='adam', weight_decay=0.0), seed=100, sigma_begin=1, sigma_end=0.01, sigma_num=10, type='trainers.ae_trainer_3D')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(scorenet=Namespace(dim=3, hidden_size=256, n_blocks=24, out_dim=3, param_likelihood=False, sigma_condition=True, type='models.decoders.resnet_add', xyz_condition=True, z_dim=128))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(num_points=2048, num_steps=10, step_size_ratio=1, weight=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(log_freq=10, save_freq=100, val_freq=100, viz_freq=5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scorenet import Scorenet\n",
    "from critic import Criticnet\n",
    "from utils import get_opt, approx_jacobian_trace, set_random_seed, exact_jacobian_trace\n",
    "from data_loader import get_data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data:2832\n",
      "Min number of points: (train)2048 (test)2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0832, -0.0008, -0.0106],\n",
       "         [ 0.2124, -0.1133,  0.3423],\n",
       "         [ 0.3726, -0.1521, -0.1293],\n",
       "         ...,\n",
       "         [-0.0380,  0.0209, -0.6827],\n",
       "         [ 0.0835, -0.0098, -0.6379],\n",
       "         [ 0.6067, -0.1337, -0.0316]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_random_seed(getattr(cfg.trainer, \"seed\", 666))\n",
    "\n",
    "# load data\n",
    "train_data = get_data(cfg.data, 0)\n",
    "tr_pts = train_data['tr_points'].unsqueeze(0)\n",
    "te_pts = train_data['te_points'].unsqueeze(0)\n",
    "tr_pts = tr_pts.to(device)\n",
    "te_pts = te_pts.to(device)\n",
    "tr_pts.requires_grad_()\n",
    "te_pts.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma:,  [1.         0.59948425 0.35938137 0.21544347 0.12915497 0.07742637\n",
      " 0.04641589 0.02782559 0.01668101 0.01      ]\n"
     ]
    }
   ],
   "source": [
    "# sigmas\n",
    "if hasattr(cfg.trainer, \"sigmas\"):\n",
    "    sigmas = cfg.trainer.sigmas\n",
    "else:\n",
    "    sigma_begin = float(cfg.trainer.sigma_begin)\n",
    "    sigma_end = float(cfg.trainer.sigma_end)\n",
    "    num_classes = int(cfg.trainer.sigma_num)\n",
    "    sigmas = np.exp(np.linspace(np.log(sigma_begin), np.log(sigma_end), num_classes))\n",
    "print(\"Sigma:, \", sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorenet(\n",
      "  (conv_p): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (1): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (2): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (3): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (4): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (bn_out): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_out): Conv1d(256, 3, kernel_size=(1,), stride=(1,))\n",
      "  (actvn_out): ReLU()\n",
      ")\n",
      "Criticnet(\n",
      "  (conv_p): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (1): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (2): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (3): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (4): ResnetBlockConv1d(\n",
      "      (bn_0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_c): Conv1d(4, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (bn_out): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_out): Conv1d(256, 3, kernel_size=(1,), stride=(1,))\n",
      "  (actvn_out): ReLU()\n",
      ")\n",
      "Start epoch: 0 End epoch: 2000\n",
      "Epoch (critic) 0 Loss=-0.04976 t1=-0.01419 t2=-0.03557\n",
      "Epoch (critic) 1 Loss=0.28511 t1=0.12246 t2=0.16265\n",
      "Epoch (critic) 2 Loss=0.54572 t1=0.12496 t2=0.42076\n",
      "Epoch (critic) 3 Loss=0.78919 t1=0.07294 t2=0.71625\n",
      "Epoch (critic) 4 Loss=1.53824 t1=0.03497 t2=1.50327\n",
      "Epoch (critic) 5 Loss=2.95804 t1=0.02246 t2=2.93558\n",
      "Epoch (critic) 6 Loss=5.21201 t1=0.03557 t2=5.17644\n",
      "Epoch (critic) 7 Loss=7.33609 t1=0.04027 t2=7.29582\n",
      "Epoch (critic) 8 Loss=5.34392 t1=0.04194 t2=5.30198\n",
      "Epoch (critic) 9 Loss=6.51709 t1=0.09579 t2=6.42129\n",
      "Epoch (score) 10 Loss=10.28092 t1=0.06799 t2=10.21293\n",
      "Epoch (score) 11 Loss=9.74048 t1=-0.57013 t2=10.31061\n",
      "Epoch (score) 12 Loss=8.56008 t1=-0.95186 t2=9.51194\n",
      "Epoch (score) 13 Loss=8.91946 t1=-1.22274 t2=10.14220\n",
      "Epoch (score) 14 Loss=8.26210 t1=-1.44742 t2=9.70952\n",
      "Epoch (score) 15 Loss=8.27475 t1=-1.65419 t2=9.92894\n",
      "Epoch (score) 16 Loss=8.05723 t1=-1.89903 t2=9.95626\n",
      "Epoch (score) 17 Loss=7.90192 t1=-2.13000 t2=10.03192\n",
      "Epoch (score) 18 Loss=7.83631 t1=-2.32296 t2=10.15927\n",
      "Epoch (score) 19 Loss=7.57720 t1=-2.55396 t2=10.13116\n",
      "Epoch (critic) 20 Loss=7.17686 t1=-2.64505 t2=9.82191\n",
      "Epoch (critic) 21 Loss=17.81920 t1=1.42339 t2=16.39581\n",
      "Epoch (critic) 22 Loss=29.85573 t1=2.83529 t2=27.02043\n",
      "Epoch (critic) 23 Loss=16.14637 t1=5.07842 t2=11.06795\n",
      "Epoch (critic) 24 Loss=21.45583 t1=4.13063 t2=17.32520\n",
      "Epoch (critic) 25 Loss=21.54210 t1=4.49643 t2=17.04568\n",
      "Epoch (critic) 26 Loss=23.02069 t1=5.87972 t2=17.14097\n",
      "Epoch (critic) 27 Loss=26.14829 t1=6.95018 t2=19.19811\n",
      "Epoch (critic) 28 Loss=38.54647 t1=7.14293 t2=31.40353\n",
      "Epoch (critic) 29 Loss=29.86877 t1=6.60094 t2=23.26783\n",
      "Epoch (score) 30 Loss=18.26058 t1=6.34246 t2=11.91812\n",
      "Epoch (score) 31 Loss=17.60994 t1=4.13977 t2=13.47017\n",
      "Epoch (score) 32 Loss=12.11147 t1=0.39346 t2=11.71801\n",
      "Epoch (score) 33 Loss=47.37838 t1=-2.22602 t2=49.60440\n",
      "Epoch (score) 34 Loss=11.91871 t1=-3.81832 t2=15.73702\n",
      "Epoch (score) 35 Loss=15.03059 t1=-4.82814 t2=19.85873\n",
      "Epoch (score) 36 Loss=10.32724 t1=-5.63616 t2=15.96340\n",
      "Epoch (score) 37 Loss=6.68495 t1=-6.50460 t2=13.18954\n",
      "Epoch (score) 38 Loss=7.76814 t1=-7.22275 t2=14.99089\n",
      "Epoch (score) 39 Loss=7.87553 t1=-7.76117 t2=15.63670\n",
      "Epoch (critic) 40 Loss=11.03803 t1=-8.23922 t2=19.27725\n",
      "Epoch (critic) 41 Loss=29.45952 t1=-0.33759 t2=29.79711\n",
      "Epoch (critic) 42 Loss=38.92728 t1=4.31276 t2=34.61452\n",
      "Epoch (critic) 43 Loss=29.97515 t1=5.65064 t2=24.32450\n",
      "Epoch (critic) 44 Loss=38.49726 t1=7.93403 t2=30.56323\n",
      "Epoch (critic) 45 Loss=43.33384 t1=7.70415 t2=35.62968\n",
      "Epoch (critic) 46 Loss=44.02082 t1=9.66431 t2=34.35651\n",
      "Epoch (critic) 47 Loss=54.05698 t1=10.22279 t2=43.83419\n",
      "Epoch (critic) 48 Loss=62.44331 t1=8.51826 t2=53.92506\n",
      "Epoch (critic) 49 Loss=66.71511 t1=10.45895 t2=56.25616\n",
      "Epoch (score) 50 Loss=78.00829 t1=11.29839 t2=66.70989\n",
      "Epoch (score) 51 Loss=64.48471 t1=9.69774 t2=54.78696\n",
      "Epoch (score) 52 Loss=58.92600 t1=6.80808 t2=52.11791\n",
      "Epoch (score) 53 Loss=79.37486 t1=4.65582 t2=74.71904\n",
      "Epoch (score) 54 Loss=55.49167 t1=2.93974 t2=52.55193\n",
      "Epoch (score) 55 Loss=75.65775 t1=0.20565 t2=75.45210\n",
      "Epoch (score) 56 Loss=44.69204 t1=-2.20011 t2=46.89215\n",
      "Epoch (score) 57 Loss=52.35765 t1=-3.82553 t2=56.18318\n",
      "Epoch (score) 58 Loss=72.65411 t1=-4.58374 t2=77.23784\n",
      "Epoch (score) 59 Loss=41.40208 t1=-6.06286 t2=47.46495\n",
      "Epoch (critic) 60 Loss=49.32222 t1=-7.17018 t2=56.49240\n",
      "Epoch (critic) 61 Loss=85.61137 t1=1.14518 t2=84.46620\n",
      "Epoch (critic) 62 Loss=59.02772 t1=3.04497 t2=55.98275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 63 Loss=16.13451 t1=7.23345 t2=8.90106\n",
      "Epoch (critic) 64 Loss=19.63734 t1=8.94100 t2=10.69634\n",
      "Epoch (critic) 65 Loss=47.16737 t1=8.99096 t2=38.17641\n",
      "Epoch (critic) 66 Loss=41.10990 t1=10.88998 t2=30.21992\n",
      "Epoch (critic) 67 Loss=70.01431 t1=12.09841 t2=57.91589\n",
      "Epoch (critic) 68 Loss=15.29436 t1=13.82187 t2=1.47250\n",
      "Epoch (critic) 69 Loss=14.89438 t1=13.66047 t2=1.23390\n",
      "Epoch (score) 70 Loss=14.11214 t1=13.03277 t2=1.07937\n",
      "Epoch (score) 71 Loss=13.50854 t1=12.02393 t2=1.48461\n",
      "Epoch (score) 72 Loss=10.90442 t1=9.71315 t2=1.19127\n",
      "Epoch (score) 73 Loss=7.11056 t1=6.11233 t2=0.99823\n",
      "Epoch (score) 74 Loss=2.91001 t1=1.89366 t2=1.01636\n",
      "Epoch (score) 75 Loss=-0.62954 t1=-1.63089 t2=1.00135\n",
      "Epoch (score) 76 Loss=-2.84753 t1=-3.90047 t2=1.05295\n",
      "Epoch (score) 77 Loss=-4.30332 t1=-5.39706 t2=1.09374\n",
      "Epoch (score) 78 Loss=-5.77427 t1=-6.85742 t2=1.08315\n",
      "Epoch (score) 79 Loss=-6.65411 t1=-8.00768 t2=1.35357\n",
      "Epoch (critic) 80 Loss=-7.70371 t1=-8.90047 t2=1.19675\n",
      "Epoch (critic) 81 Loss=-2.57426 t1=-3.59117 t2=1.01690\n",
      "Epoch (critic) 82 Loss=2.49900 t1=1.57916 t2=0.91985\n",
      "Epoch (critic) 83 Loss=5.11051 t1=4.28980 t2=0.82071\n",
      "Epoch (critic) 84 Loss=6.70640 t1=5.83430 t2=0.87209\n",
      "Epoch (critic) 85 Loss=7.67246 t1=6.82141 t2=0.85105\n",
      "Epoch (critic) 86 Loss=8.40071 t1=7.49335 t2=0.90736\n",
      "Epoch (critic) 87 Loss=9.07318 t1=7.99142 t2=1.08177\n",
      "Epoch (critic) 88 Loss=9.42592 t1=8.39108 t2=1.03484\n",
      "Epoch (critic) 89 Loss=9.89220 t1=8.76500 t2=1.12720\n",
      "Epoch (score) 90 Loss=10.30150 t1=9.12955 t2=1.17195\n",
      "Epoch (score) 91 Loss=10.00481 t1=8.86592 t2=1.13889\n",
      "Epoch (score) 92 Loss=9.25534 t1=8.05438 t2=1.20096\n",
      "Epoch (score) 93 Loss=7.76205 t1=6.59462 t2=1.16743\n",
      "Epoch (score) 94 Loss=5.81260 t1=4.58449 t2=1.22810\n",
      "Epoch (score) 95 Loss=4.12570 t1=2.94575 t2=1.17994\n",
      "Epoch (score) 96 Loss=2.76377 t1=1.61814 t2=1.14563\n",
      "Epoch (score) 97 Loss=1.53684 t1=0.38411 t2=1.15273\n",
      "Epoch (score) 98 Loss=0.27502 t1=-0.87727 t2=1.15229\n",
      "Epoch (score) 99 Loss=-1.13984 t1=-2.27804 t2=1.13820\n",
      "Epoch (critic) 100 Loss=-2.68803 t1=-3.83938 t2=1.15135\n",
      "Epoch (critic) 101 Loss=-2.10733 t1=-3.25984 t2=1.15251\n",
      "Epoch (critic) 102 Loss=-1.10870 t1=-2.27997 t2=1.17127\n",
      "Epoch (critic) 103 Loss=0.04382 t1=-1.11184 t2=1.15566\n",
      "Epoch (critic) 104 Loss=1.30229 t1=0.06153 t2=1.24077\n",
      "Epoch (critic) 105 Loss=2.29454 t1=1.12819 t2=1.16636\n",
      "Epoch (critic) 106 Loss=3.43457 t1=2.06993 t2=1.36464\n",
      "Epoch (critic) 107 Loss=4.23765 t1=2.87836 t2=1.35929\n",
      "Epoch (critic) 108 Loss=4.89236 t1=3.57713 t2=1.31523\n",
      "Epoch (critic) 109 Loss=5.52854 t1=4.13829 t2=1.39025\n",
      "Epoch (score) 110 Loss=6.05957 t1=4.64565 t2=1.41392\n",
      "Epoch (score) 111 Loss=6.12803 t1=4.69609 t2=1.43194\n",
      "Epoch (score) 112 Loss=5.83250 t1=4.41553 t2=1.41696\n",
      "Epoch (score) 113 Loss=5.41274 t1=3.89995 t2=1.51279\n",
      "Epoch (score) 114 Loss=4.77311 t1=3.26448 t2=1.50863\n",
      "Epoch (score) 115 Loss=3.99668 t1=2.53034 t2=1.46634\n",
      "Epoch (score) 116 Loss=3.18911 t1=1.76660 t2=1.42250\n",
      "Epoch (score) 117 Loss=2.48405 t1=0.99364 t2=1.49042\n",
      "Epoch (score) 118 Loss=1.64763 t1=0.22207 t2=1.42556\n",
      "Epoch (score) 119 Loss=1.06206 t1=-0.43598 t2=1.49804\n",
      "Epoch (critic) 120 Loss=0.44962 t1=-0.94501 t2=1.39462\n",
      "Epoch (critic) 121 Loss=0.83272 t1=-0.70733 t2=1.54005\n",
      "Epoch (critic) 122 Loss=1.29957 t1=-0.31644 t2=1.61601\n",
      "Epoch (critic) 123 Loss=1.90331 t1=0.15624 t2=1.74706\n",
      "Epoch (critic) 124 Loss=2.66004 t1=0.71387 t2=1.94617\n",
      "Epoch (critic) 125 Loss=3.45494 t1=1.29042 t2=2.16451\n",
      "Epoch (critic) 126 Loss=4.28504 t1=1.90182 t2=2.38322\n",
      "Epoch (critic) 127 Loss=5.09872 t1=2.48780 t2=2.61092\n",
      "Epoch (critic) 128 Loss=5.98230 t1=3.07230 t2=2.91000\n",
      "Epoch (critic) 129 Loss=6.92446 t1=3.60800 t2=3.31647\n",
      "Epoch (score) 130 Loss=7.67209 t1=4.13004 t2=3.54205\n",
      "Epoch (score) 131 Loss=7.37914 t1=3.89054 t2=3.48860\n",
      "Epoch (score) 132 Loss=7.05160 t1=3.44036 t2=3.61124\n",
      "Epoch (score) 133 Loss=6.32277 t1=2.76423 t2=3.55853\n",
      "Epoch (score) 134 Loss=5.45271 t1=1.94819 t2=3.50452\n",
      "Epoch (score) 135 Loss=4.85588 t1=1.13463 t2=3.72125\n",
      "Epoch (score) 136 Loss=4.21096 t1=0.46059 t2=3.75037\n",
      "Epoch (score) 137 Loss=3.72100 t1=-0.05115 t2=3.77214\n",
      "Epoch (score) 138 Loss=3.11291 t1=-0.46138 t2=3.57429\n",
      "Epoch (score) 139 Loss=2.66380 t1=-0.82693 t2=3.49074\n",
      "Epoch (critic) 140 Loss=2.18272 t1=-1.16040 t2=3.34313\n",
      "Epoch (critic) 141 Loss=2.97133 t1=-1.10460 t2=4.07593\n",
      "Epoch (critic) 142 Loss=4.23422 t1=-0.84241 t2=5.07663\n",
      "Epoch (critic) 143 Loss=6.09757 t1=-0.41617 t2=6.51374\n",
      "Epoch (critic) 144 Loss=7.69474 t1=0.18066 t2=7.51408\n",
      "Epoch (critic) 145 Loss=10.35450 t1=0.99686 t2=9.35763\n",
      "Epoch (critic) 146 Loss=13.76533 t1=1.82813 t2=11.93720\n",
      "Epoch (critic) 147 Loss=6.51943 t1=1.63629 t2=4.88314\n",
      "Epoch (critic) 148 Loss=6.64254 t1=1.81681 t2=4.82574\n",
      "Epoch (critic) 149 Loss=8.05571 t1=2.26187 t2=5.79384\n",
      "Epoch (score) 150 Loss=10.63236 t1=2.81092 t2=7.82144\n",
      "Epoch (score) 151 Loss=9.23949 t1=2.49811 t2=6.74137\n",
      "Epoch (score) 152 Loss=9.38272 t1=2.01231 t2=7.37041\n",
      "Epoch (score) 153 Loss=8.96809 t1=1.44387 t2=7.52422\n",
      "Epoch (score) 154 Loss=8.33401 t1=0.91680 t2=7.41722\n",
      "Epoch (score) 155 Loss=7.68383 t1=0.46824 t2=7.21560\n",
      "Epoch (score) 156 Loss=6.81337 t1=0.07824 t2=6.73513\n",
      "Epoch (score) 157 Loss=6.24877 t1=-0.32780 t2=6.57657\n",
      "Epoch (score) 158 Loss=7.43797 t1=-0.76052 t2=8.19850\n",
      "Epoch (score) 159 Loss=7.74547 t1=-1.23738 t2=8.98285\n",
      "Epoch (critic) 160 Loss=5.48104 t1=-1.75101 t2=7.23205\n",
      "Epoch (critic) 161 Loss=7.13790 t1=-1.23132 t2=8.36922\n",
      "Epoch (critic) 162 Loss=11.08730 t1=-0.52079 t2=11.60809\n",
      "Epoch (critic) 163 Loss=15.59045 t1=0.25720 t2=15.33326\n",
      "Epoch (critic) 164 Loss=16.93300 t1=0.98158 t2=15.95142\n",
      "Epoch (critic) 165 Loss=16.06422 t1=1.55320 t2=14.51102\n",
      "Epoch (critic) 166 Loss=24.69386 t1=1.99544 t2=22.69841\n",
      "Epoch (critic) 167 Loss=26.57850 t1=2.19421 t2=24.38429\n",
      "Epoch (critic) 168 Loss=15.91884 t1=2.49094 t2=13.42790\n",
      "Epoch (critic) 169 Loss=28.05606 t1=2.13401 t2=25.92205\n",
      "Epoch (score) 170 Loss=20.30164 t1=1.74547 t2=18.55617\n",
      "Epoch (score) 171 Loss=21.12641 t1=1.61463 t2=19.51177\n",
      "Epoch (score) 172 Loss=22.86440 t1=1.21434 t2=21.65006\n",
      "Epoch (score) 173 Loss=22.55976 t1=0.62931 t2=21.93044\n",
      "Epoch (score) 174 Loss=19.81395 t1=-0.21288 t2=20.02683\n",
      "Epoch (score) 175 Loss=17.93667 t1=-1.08264 t2=19.01931\n",
      "Epoch (score) 176 Loss=16.75435 t1=-1.67791 t2=18.43226\n",
      "Epoch (score) 177 Loss=17.81957 t1=-2.04673 t2=19.86630\n",
      "Epoch (score) 178 Loss=12.60867 t1=-2.37497 t2=14.98364\n",
      "Epoch (score) 179 Loss=13.26493 t1=-2.75405 t2=16.01898\n",
      "Epoch (critic) 180 Loss=19.17270 t1=-3.08512 t2=22.25783\n",
      "Epoch (critic) 181 Loss=19.24523 t1=-1.85324 t2=21.09847\n",
      "Epoch (critic) 182 Loss=24.70883 t1=-0.45704 t2=25.16586\n",
      "Epoch (critic) 183 Loss=25.82690 t1=0.30053 t2=25.52637\n",
      "Epoch (critic) 184 Loss=32.71778 t1=1.38143 t2=31.33635\n",
      "Epoch (critic) 185 Loss=53.80644 t1=2.27033 t2=51.53611\n",
      "Epoch (critic) 186 Loss=27.26198 t1=2.35105 t2=24.91092\n",
      "Epoch (critic) 187 Loss=44.50261 t1=2.22432 t2=42.27828\n",
      "Epoch (critic) 188 Loss=45.01412 t1=2.05018 t2=42.96394\n",
      "Epoch (critic) 189 Loss=16.82845 t1=3.55759 t2=13.27085\n",
      "Epoch (score) 190 Loss=8.31141 t1=3.88834 t2=4.42308\n",
      "Epoch (score) 191 Loss=8.54759 t1=3.87161 t2=4.67599\n",
      "Epoch (score) 192 Loss=10.16771 t1=3.70925 t2=6.45846\n",
      "Epoch (score) 193 Loss=9.00556 t1=3.36116 t2=5.64441\n",
      "Epoch (score) 194 Loss=8.15316 t1=2.79749 t2=5.35566\n",
      "Epoch (score) 195 Loss=7.29015 t1=2.06481 t2=5.22534\n",
      "Epoch (score) 196 Loss=6.92373 t1=1.14363 t2=5.78011\n",
      "Epoch (score) 197 Loss=4.61316 t1=0.22443 t2=4.38873\n",
      "Epoch (score) 198 Loss=4.07945 t1=-0.39059 t2=4.47003\n",
      "Epoch (score) 199 Loss=4.27517 t1=-0.81420 t2=5.08937\n",
      "Epoch (critic) 200 Loss=7.24720 t1=-1.10499 t2=8.35220\n",
      "Epoch (critic) 201 Loss=8.77677 t1=-0.55668 t2=9.33345\n",
      "Epoch (critic) 202 Loss=6.58619 t1=0.13336 t2=6.45284\n",
      "Epoch (critic) 203 Loss=14.76428 t1=0.76634 t2=13.99795\n",
      "Epoch (critic) 204 Loss=12.95994 t1=1.62781 t2=11.33213\n",
      "Epoch (critic) 205 Loss=14.54278 t1=2.26905 t2=12.27373\n",
      "Epoch (critic) 206 Loss=26.65841 t1=2.72268 t2=23.93573\n",
      "Epoch (critic) 207 Loss=48.96423 t1=3.44225 t2=45.52197\n",
      "Epoch (critic) 208 Loss=65.82665 t1=2.97442 t2=62.85223\n",
      "Epoch (critic) 209 Loss=70.60571 t1=3.97342 t2=66.63229\n",
      "Epoch (score) 210 Loss=13.79389 t1=3.45443 t2=10.33946\n",
      "Epoch (score) 211 Loss=21.08639 t1=2.73302 t2=18.35336\n",
      "Epoch (score) 212 Loss=19.86778 t1=1.69622 t2=18.17156\n",
      "Epoch (score) 213 Loss=13.80289 t1=0.50529 t2=13.29760\n",
      "Epoch (score) 214 Loss=16.76431 t1=-0.44381 t2=17.20811\n",
      "Epoch (score) 215 Loss=4.53417 t1=-0.81491 t2=5.34908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 216 Loss=7.54088 t1=-1.11864 t2=8.65952\n",
      "Epoch (score) 217 Loss=6.91474 t1=-1.67509 t2=8.58983\n",
      "Epoch (score) 218 Loss=15.64135 t1=-2.43766 t2=18.07901\n",
      "Epoch (score) 219 Loss=2.88045 t1=-2.75199 t2=5.63244\n",
      "Epoch (critic) 220 Loss=9.75872 t1=-3.33294 t2=13.09166\n",
      "Epoch (critic) 221 Loss=19.30830 t1=-2.18922 t2=21.49752\n",
      "Epoch (critic) 222 Loss=32.97780 t1=-1.73415 t2=34.71194\n",
      "Epoch (critic) 223 Loss=48.11803 t1=-1.68143 t2=49.79946\n",
      "Epoch (critic) 224 Loss=38.60541 t1=-1.79477 t2=40.40018\n",
      "Epoch (critic) 225 Loss=40.48336 t1=-0.43520 t2=40.91856\n",
      "Epoch (critic) 226 Loss=24.33853 t1=0.98494 t2=23.35360\n",
      "Epoch (critic) 227 Loss=56.22799 t1=1.35095 t2=54.87703\n",
      "Epoch (critic) 228 Loss=39.05786 t1=1.41220 t2=37.64566\n",
      "Epoch (critic) 229 Loss=56.44006 t1=2.25206 t2=54.18800\n",
      "Epoch (score) 230 Loss=33.33117 t1=3.90467 t2=29.42650\n",
      "Epoch (score) 231 Loss=44.17001 t1=3.11591 t2=41.05410\n",
      "Epoch (score) 232 Loss=42.05246 t1=2.89813 t2=39.15432\n",
      "Epoch (score) 233 Loss=12.07530 t1=2.39629 t2=9.67901\n",
      "Epoch (score) 234 Loss=39.35471 t1=0.74109 t2=38.61363\n",
      "Epoch (score) 235 Loss=43.57378 t1=-0.00098 t2=43.57477\n",
      "Epoch (score) 236 Loss=36.13425 t1=-0.80995 t2=36.94421\n",
      "Epoch (score) 237 Loss=36.62537 t1=-1.48781 t2=38.11317\n",
      "Epoch (score) 238 Loss=36.66185 t1=-2.12618 t2=38.78804\n",
      "Epoch (score) 239 Loss=40.58375 t1=-2.60257 t2=43.18632\n",
      "Epoch (critic) 240 Loss=29.21393 t1=-3.13917 t2=32.35310\n",
      "Epoch (critic) 241 Loss=64.68752 t1=-0.54747 t2=65.23499\n",
      "Epoch (critic) 242 Loss=0.15400 t1=-1.05540 t2=1.20940\n",
      "Epoch (critic) 243 Loss=18.10706 t1=0.27097 t2=17.83609\n",
      "Epoch (critic) 244 Loss=107.40688 t1=4.48348 t2=102.92338\n",
      "Epoch (critic) 245 Loss=112.97614 t1=0.48197 t2=112.49419\n",
      "Epoch (critic) 246 Loss=27.25684 t1=0.28717 t2=26.96967\n",
      "Epoch (critic) 247 Loss=25.87311 t1=0.34422 t2=25.52888\n",
      "Epoch (critic) 248 Loss=26.43380 t1=0.44207 t2=25.99173\n",
      "Epoch (critic) 249 Loss=20.37263 t1=0.72812 t2=19.64452\n",
      "Epoch (score) 250 Loss=20.42162 t1=0.98938 t2=19.43223\n",
      "Epoch (score) 251 Loss=19.97567 t1=0.86522 t2=19.11045\n",
      "Epoch (score) 252 Loss=21.27087 t1=0.71547 t2=20.55540\n",
      "Epoch (score) 253 Loss=21.77071 t1=0.58162 t2=21.18910\n",
      "Epoch (score) 254 Loss=19.77694 t1=0.46655 t2=19.31039\n",
      "Epoch (score) 255 Loss=19.05190 t1=0.24824 t2=18.80366\n",
      "Epoch (score) 256 Loss=20.38805 t1=0.13231 t2=20.25574\n",
      "Epoch (score) 257 Loss=16.05076 t1=-0.14447 t2=16.19523\n",
      "Epoch (score) 258 Loss=20.06705 t1=-0.25034 t2=20.31739\n",
      "Epoch (score) 259 Loss=22.25712 t1=-0.49848 t2=22.75560\n",
      "Epoch (critic) 260 Loss=21.35913 t1=-0.76036 t2=22.11949\n",
      "Epoch (critic) 261 Loss=17.62264 t1=-0.50386 t2=18.12650\n",
      "Epoch (critic) 262 Loss=21.43112 t1=-0.10664 t2=21.53776\n",
      "Epoch (critic) 263 Loss=20.88490 t1=0.18954 t2=20.69536\n",
      "Epoch (critic) 264 Loss=26.25224 t1=0.51529 t2=25.73695\n",
      "Epoch (critic) 265 Loss=21.69742 t1=0.79524 t2=20.90218\n",
      "Epoch (critic) 266 Loss=22.39801 t1=1.12881 t2=21.26921\n",
      "Epoch (critic) 267 Loss=38.99330 t1=1.65249 t2=37.34081\n",
      "Epoch (critic) 268 Loss=25.67868 t1=2.05167 t2=23.62701\n",
      "Epoch (critic) 269 Loss=26.86395 t1=2.46946 t2=24.39449\n",
      "Epoch (score) 270 Loss=42.43671 t1=2.98571 t2=39.45100\n",
      "Epoch (score) 271 Loss=51.95430 t1=2.71834 t2=49.23597\n",
      "Epoch (score) 272 Loss=45.07258 t1=2.44266 t2=42.62993\n",
      "Epoch (score) 273 Loss=40.44107 t1=2.03743 t2=38.40365\n",
      "Epoch (score) 274 Loss=30.85778 t1=1.51786 t2=29.33992\n",
      "Epoch (score) 275 Loss=41.22469 t1=0.87750 t2=40.34720\n",
      "Epoch (score) 276 Loss=45.38736 t1=0.18150 t2=45.20585\n",
      "Epoch (score) 277 Loss=34.34481 t1=-0.31046 t2=34.65528\n",
      "Epoch (score) 278 Loss=25.09939 t1=-0.81359 t2=25.91299\n",
      "Epoch (score) 279 Loss=34.76760 t1=-1.41424 t2=36.18184\n",
      "Epoch (critic) 280 Loss=21.71589 t1=-1.76063 t2=23.47653\n",
      "Epoch (critic) 281 Loss=37.41148 t1=-1.65326 t2=39.06474\n",
      "Epoch (critic) 282 Loss=42.12824 t1=-1.64087 t2=43.76911\n",
      "Epoch (critic) 283 Loss=40.13145 t1=-1.05821 t2=41.18966\n",
      "Epoch (critic) 284 Loss=54.88829 t1=-0.85537 t2=55.74366\n",
      "Epoch (critic) 285 Loss=40.21493 t1=-0.11454 t2=40.32947\n",
      "Epoch (critic) 286 Loss=54.43920 t1=-0.49997 t2=54.93916\n",
      "Epoch (critic) 287 Loss=62.47826 t1=-0.95063 t2=63.42889\n",
      "Epoch (critic) 288 Loss=86.08284 t1=-0.41063 t2=86.49347\n",
      "Epoch (critic) 289 Loss=88.07314 t1=0.57366 t2=87.49950\n",
      "Epoch (score) 290 Loss=114.77111 t1=0.01474 t2=114.75638\n",
      "Epoch (score) 291 Loss=90.13435 t1=0.11548 t2=90.01888\n",
      "Epoch (score) 292 Loss=117.79449 t1=-0.53186 t2=118.32635\n",
      "Epoch (score) 293 Loss=79.25237 t1=-3.01240 t2=82.26477\n",
      "Epoch (score) 294 Loss=98.81171 t1=-1.94597 t2=100.75768\n",
      "Epoch (score) 295 Loss=84.64567 t1=-4.22659 t2=88.87227\n",
      "Epoch (score) 296 Loss=86.09283 t1=-5.55503 t2=91.64784\n",
      "Epoch (score) 297 Loss=101.94563 t1=-3.87685 t2=105.82248\n",
      "Epoch (score) 298 Loss=95.09874 t1=-7.62310 t2=102.72183\n",
      "Epoch (score) 299 Loss=102.70737 t1=-5.84497 t2=108.55233\n",
      "Epoch (critic) 300 Loss=59.38558 t1=-11.10264 t2=70.48821\n",
      "Epoch (critic) 301 Loss=93.73367 t1=4.61752 t2=89.11615\n",
      "Epoch (critic) 302 Loss=125.87690 t1=3.75532 t2=122.12156\n",
      "Epoch (critic) 303 Loss=23.79882 t1=3.02775 t2=20.77107\n",
      "Epoch (critic) 304 Loss=89.94509 t1=5.37132 t2=84.57378\n",
      "Epoch (critic) 305 Loss=150.37418 t1=7.64541 t2=142.72876\n",
      "Epoch (critic) 306 Loss=146.44878 t1=8.22136 t2=138.22742\n",
      "Epoch (critic) 307 Loss=222.21587 t1=9.03341 t2=213.18246\n",
      "Epoch (critic) 308 Loss=14.89208 t1=7.80584 t2=7.08624\n",
      "Epoch (critic) 309 Loss=13.36574 t1=8.54777 t2=4.81797\n",
      "Epoch (score) 310 Loss=13.11882 t1=8.92862 t2=4.19019\n",
      "Epoch (score) 311 Loss=13.44499 t1=8.72145 t2=4.72354\n",
      "Epoch (score) 312 Loss=15.77111 t1=9.04667 t2=6.72444\n",
      "Epoch (score) 313 Loss=13.08054 t1=8.71872 t2=4.36183\n",
      "Epoch (score) 314 Loss=13.93418 t1=8.38822 t2=5.54596\n",
      "Epoch (score) 315 Loss=13.27825 t1=8.21449 t2=5.06376\n",
      "Epoch (score) 316 Loss=13.82255 t1=7.11622 t2=6.70633\n",
      "Epoch (score) 317 Loss=13.80651 t1=6.48722 t2=7.31929\n",
      "Epoch (score) 318 Loss=11.31598 t1=5.99286 t2=5.32312\n",
      "Epoch (score) 319 Loss=11.46699 t1=5.19951 t2=6.26748\n",
      "Epoch (critic) 320 Loss=10.13233 t1=3.84083 t2=6.29149\n",
      "Epoch (critic) 321 Loss=8.44647 t1=4.73831 t2=3.70817\n",
      "Epoch (critic) 322 Loss=11.56474 t1=5.12407 t2=6.44066\n",
      "Epoch (critic) 323 Loss=9.51344 t1=3.87539 t2=5.63806\n",
      "Epoch (critic) 324 Loss=12.31413 t1=4.76488 t2=7.54924\n",
      "Epoch (critic) 325 Loss=16.19794 t1=4.95398 t2=11.24396\n",
      "Epoch (critic) 326 Loss=18.28888 t1=5.18998 t2=13.09890\n",
      "Epoch (critic) 327 Loss=23.36622 t1=5.24087 t2=18.12536\n",
      "Epoch (critic) 328 Loss=31.32069 t1=5.30605 t2=26.01464\n",
      "Epoch (critic) 329 Loss=24.46313 t1=2.54475 t2=21.91838\n",
      "Epoch (score) 330 Loss=39.44871 t1=5.45262 t2=33.99609\n",
      "Epoch (score) 331 Loss=43.26962 t1=3.05152 t2=40.21810\n",
      "Epoch (score) 332 Loss=44.64713 t1=3.04496 t2=41.60217\n",
      "Epoch (score) 333 Loss=63.72167 t1=3.57341 t2=60.14825\n",
      "Epoch (score) 334 Loss=51.99706 t1=4.07737 t2=47.91969\n",
      "Epoch (score) 335 Loss=49.25317 t1=3.05910 t2=46.19408\n",
      "Epoch (score) 336 Loss=47.28745 t1=1.06898 t2=46.21847\n",
      "Epoch (score) 337 Loss=65.67224 t1=1.69272 t2=63.97952\n",
      "Epoch (score) 338 Loss=41.45041 t1=-0.17423 t2=41.62464\n",
      "Epoch (score) 339 Loss=54.71619 t1=0.36834 t2=54.34785\n",
      "Epoch (critic) 340 Loss=33.06301 t1=1.98221 t2=31.08080\n",
      "Epoch (critic) 341 Loss=35.67374 t1=0.93275 t2=34.74099\n",
      "Epoch (critic) 342 Loss=63.61718 t1=1.68377 t2=61.93340\n",
      "Epoch (critic) 343 Loss=6.99766 t1=5.51199 t2=1.48567\n",
      "Epoch (critic) 344 Loss=6.80262 t1=5.36481 t2=1.43781\n",
      "Epoch (critic) 345 Loss=6.59697 t1=5.05258 t2=1.54439\n",
      "Epoch (critic) 346 Loss=6.27049 t1=4.74158 t2=1.52891\n",
      "Epoch (critic) 347 Loss=5.85500 t1=4.47877 t2=1.37622\n",
      "Epoch (critic) 348 Loss=5.81585 t1=4.07228 t2=1.74357\n",
      "Epoch (critic) 349 Loss=5.25447 t1=3.77672 t2=1.47776\n",
      "Epoch (score) 350 Loss=4.97505 t1=3.54710 t2=1.42795\n",
      "Epoch (score) 351 Loss=6.11079 t1=3.46827 t2=2.64252\n",
      "Epoch (score) 352 Loss=4.52716 t1=3.30953 t2=1.21764\n",
      "Epoch (score) 353 Loss=4.58017 t1=3.09330 t2=1.48687\n",
      "Epoch (score) 354 Loss=4.24960 t1=2.90980 t2=1.33981\n",
      "Epoch (score) 355 Loss=4.12556 t1=2.63212 t2=1.49343\n",
      "Epoch (score) 356 Loss=4.02600 t1=2.33389 t2=1.69210\n",
      "Epoch (score) 357 Loss=3.61552 t1=2.08767 t2=1.52785\n",
      "Epoch (score) 358 Loss=3.42165 t1=1.77762 t2=1.64403\n",
      "Epoch (score) 359 Loss=2.93975 t1=1.47873 t2=1.46102\n",
      "Epoch (critic) 360 Loss=2.49951 t1=1.21022 t2=1.28929\n",
      "Epoch (critic) 361 Loss=2.53721 t1=1.11020 t2=1.42701\n",
      "Epoch (critic) 362 Loss=2.86350 t1=1.04101 t2=1.82249\n",
      "Epoch (critic) 363 Loss=2.44271 t1=1.01096 t2=1.43175\n",
      "Epoch (critic) 364 Loss=2.79734 t1=0.92164 t2=1.87570\n",
      "Epoch (critic) 365 Loss=2.39018 t1=0.90920 t2=1.48098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 366 Loss=2.81148 t1=0.89079 t2=1.92069\n",
      "Epoch (critic) 367 Loss=2.48245 t1=0.92259 t2=1.55987\n",
      "Epoch (critic) 368 Loss=3.00827 t1=0.93541 t2=2.07285\n",
      "Epoch (critic) 369 Loss=2.98616 t1=0.95991 t2=2.02625\n",
      "Epoch (score) 370 Loss=2.88414 t1=1.00086 t2=1.88328\n",
      "Epoch (score) 371 Loss=2.61689 t1=0.89967 t2=1.71722\n",
      "Epoch (score) 372 Loss=2.70927 t1=0.78209 t2=1.92718\n",
      "Epoch (score) 373 Loss=2.52414 t1=0.66657 t2=1.85757\n",
      "Epoch (score) 374 Loss=2.48362 t1=0.55167 t2=1.93196\n",
      "Epoch (score) 375 Loss=2.54757 t1=0.43070 t2=2.11688\n",
      "Epoch (score) 376 Loss=2.37770 t1=0.30291 t2=2.07479\n",
      "Epoch (score) 377 Loss=2.02542 t1=0.18697 t2=1.83845\n",
      "Epoch (score) 378 Loss=2.03824 t1=0.06587 t2=1.97237\n",
      "Epoch (score) 379 Loss=2.39509 t1=-0.05842 t2=2.45351\n",
      "Epoch (critic) 380 Loss=1.81140 t1=-0.16877 t2=1.98017\n",
      "Epoch (critic) 381 Loss=1.78134 t1=-0.11270 t2=1.89403\n",
      "Epoch (critic) 382 Loss=2.14005 t1=-0.02965 t2=2.16970\n",
      "Epoch (critic) 383 Loss=2.23782 t1=0.07366 t2=2.16415\n",
      "Epoch (critic) 384 Loss=2.64116 t1=0.17824 t2=2.46293\n",
      "Epoch (critic) 385 Loss=2.58482 t1=0.28052 t2=2.30430\n",
      "Epoch (critic) 386 Loss=2.77185 t1=0.35903 t2=2.41282\n",
      "Epoch (critic) 387 Loss=2.93878 t1=0.40627 t2=2.53251\n",
      "Epoch (critic) 388 Loss=4.01739 t1=0.43734 t2=3.58005\n",
      "Epoch (critic) 389 Loss=3.88865 t1=0.44737 t2=3.44128\n",
      "Epoch (score) 390 Loss=4.34476 t1=0.43859 t2=3.90616\n",
      "Epoch (score) 391 Loss=4.05529 t1=0.43501 t2=3.62027\n",
      "Epoch (score) 392 Loss=4.72504 t1=0.42393 t2=4.30110\n",
      "Epoch (score) 393 Loss=4.50389 t1=0.42721 t2=4.07667\n",
      "Epoch (score) 394 Loss=4.96683 t1=0.38819 t2=4.57864\n",
      "Epoch (score) 395 Loss=4.23505 t1=0.36834 t2=3.86670\n",
      "Epoch (score) 396 Loss=4.79191 t1=0.32941 t2=4.46250\n",
      "Epoch (score) 397 Loss=3.81606 t1=0.30242 t2=3.51364\n",
      "Epoch (score) 398 Loss=4.26176 t1=0.25852 t2=4.00324\n",
      "Epoch (score) 399 Loss=3.79049 t1=0.24536 t2=3.54513\n",
      "Epoch (critic) 400 Loss=4.02408 t1=0.19974 t2=3.82435\n",
      "Epoch (critic) 401 Loss=3.66349 t1=0.15940 t2=3.50408\n",
      "Epoch (critic) 402 Loss=4.54337 t1=0.15442 t2=4.38895\n",
      "Epoch (critic) 403 Loss=4.88460 t1=0.14437 t2=4.74023\n",
      "Epoch (critic) 404 Loss=6.42079 t1=0.21408 t2=6.20670\n",
      "Epoch (critic) 405 Loss=6.62019 t1=0.30283 t2=6.31736\n",
      "Epoch (critic) 406 Loss=7.02156 t1=0.38955 t2=6.63201\n",
      "Epoch (critic) 407 Loss=8.93164 t1=0.44788 t2=8.48376\n",
      "Epoch (critic) 408 Loss=8.61274 t1=0.42655 t2=8.18619\n",
      "Epoch (critic) 409 Loss=8.55702 t1=0.33119 t2=8.22583\n",
      "Epoch (score) 410 Loss=14.14035 t1=0.27074 t2=13.86961\n",
      "Epoch (score) 411 Loss=18.54901 t1=0.29482 t2=18.25419\n",
      "Epoch (score) 412 Loss=15.40295 t1=0.29671 t2=15.10624\n",
      "Epoch (score) 413 Loss=19.15313 t1=0.25191 t2=18.90121\n",
      "Epoch (score) 414 Loss=14.70459 t1=0.15994 t2=14.54465\n",
      "Epoch (score) 415 Loss=14.19520 t1=0.12557 t2=14.06963\n",
      "Epoch (score) 416 Loss=14.28485 t1=0.09226 t2=14.19259\n",
      "Epoch (score) 417 Loss=17.32649 t1=0.02902 t2=17.29747\n",
      "Epoch (score) 418 Loss=19.37080 t1=-0.03615 t2=19.40695\n",
      "Epoch (score) 419 Loss=14.57116 t1=-0.19956 t2=14.77073\n",
      "Epoch (critic) 420 Loss=18.26170 t1=-0.18988 t2=18.45158\n",
      "Epoch (critic) 421 Loss=11.96172 t1=0.04524 t2=11.91648\n",
      "Epoch (critic) 422 Loss=30.33786 t1=-0.31105 t2=30.64891\n",
      "Epoch (critic) 423 Loss=8.56207 t1=0.58450 t2=7.97757\n",
      "Epoch (critic) 424 Loss=52.62988 t1=-0.28626 t2=52.91613\n",
      "Epoch (critic) 425 Loss=21.66278 t1=-0.74341 t2=22.40620\n",
      "Epoch (critic) 426 Loss=12.47678 t1=1.57566 t2=10.90112\n",
      "Epoch (critic) 427 Loss=28.74793 t1=0.14954 t2=28.59840\n",
      "Epoch (critic) 428 Loss=17.73085 t1=-1.19751 t2=18.92836\n",
      "Epoch (critic) 429 Loss=54.30878 t1=-1.07382 t2=55.38260\n",
      "Epoch (score) 430 Loss=0.76040 t1=0.04808 t2=0.71232\n",
      "Epoch (score) 431 Loss=0.71539 t1=-0.11184 t2=0.82723\n",
      "Epoch (score) 432 Loss=0.49348 t1=-0.33007 t2=0.82356\n",
      "Epoch (score) 433 Loss=0.20223 t1=-0.61155 t2=0.81379\n",
      "Epoch (score) 434 Loss=-0.10636 t1=-0.95240 t2=0.84604\n",
      "Epoch (score) 435 Loss=-0.47868 t1=-1.27181 t2=0.79313\n",
      "Epoch (score) 436 Loss=-0.92234 t1=-1.68738 t2=0.76504\n",
      "Epoch (score) 437 Loss=-1.31140 t1=-2.10354 t2=0.79214\n",
      "Epoch (score) 438 Loss=-1.82475 t1=-2.56951 t2=0.74477\n",
      "Epoch (score) 439 Loss=-2.37974 t1=-3.10310 t2=0.72336\n",
      "Epoch (critic) 440 Loss=-2.83720 t1=-3.63308 t2=0.79588\n",
      "Epoch (critic) 441 Loss=2.75042 t1=1.14880 t2=1.60161\n",
      "Epoch (critic) 442 Loss=4.49714 t1=2.34621 t2=2.15093\n",
      "Epoch (critic) 443 Loss=4.63075 t1=2.52777 t2=2.10298\n",
      "Epoch (critic) 444 Loss=4.36010 t1=2.49675 t2=1.86335\n",
      "Epoch (critic) 445 Loss=3.94989 t1=2.39366 t2=1.55623\n",
      "Epoch (critic) 446 Loss=3.66103 t1=2.25033 t2=1.41070\n",
      "Epoch (critic) 447 Loss=3.45324 t1=2.05613 t2=1.39711\n",
      "Epoch (critic) 448 Loss=3.38319 t1=1.85909 t2=1.52410\n",
      "Epoch (critic) 449 Loss=3.08150 t1=1.68986 t2=1.39164\n",
      "Epoch (score) 450 Loss=2.99200 t1=1.52243 t2=1.46958\n",
      "Epoch (score) 451 Loss=3.12642 t1=1.62198 t2=1.50444\n",
      "Epoch (score) 452 Loss=3.10901 t1=1.70586 t2=1.40316\n",
      "Epoch (score) 453 Loss=3.13302 t1=1.72154 t2=1.41147\n",
      "Epoch (score) 454 Loss=3.08674 t1=1.66645 t2=1.42029\n",
      "Epoch (score) 455 Loss=3.11651 t1=1.58129 t2=1.53522\n",
      "Epoch (score) 456 Loss=2.92150 t1=1.46881 t2=1.45269\n",
      "Epoch (score) 457 Loss=2.67631 t1=1.29578 t2=1.38053\n",
      "Epoch (score) 458 Loss=2.56595 t1=1.06743 t2=1.49852\n",
      "Epoch (score) 459 Loss=2.37464 t1=0.83605 t2=1.53859\n",
      "Epoch (critic) 460 Loss=2.05697 t1=0.54687 t2=1.51010\n",
      "Epoch (critic) 461 Loss=1.97244 t1=0.58388 t2=1.38856\n",
      "Epoch (critic) 462 Loss=2.10263 t1=0.70006 t2=1.40257\n",
      "Epoch (critic) 463 Loss=2.31615 t1=0.92856 t2=1.38759\n",
      "Epoch (critic) 464 Loss=2.70833 t1=1.19646 t2=1.51187\n",
      "Epoch (critic) 465 Loss=2.98569 t1=1.49213 t2=1.49355\n",
      "Epoch (critic) 466 Loss=3.27031 t1=1.78528 t2=1.48503\n",
      "Epoch (critic) 467 Loss=3.51684 t1=2.03251 t2=1.48433\n",
      "Epoch (critic) 468 Loss=3.65595 t1=2.22817 t2=1.42778\n",
      "Epoch (critic) 469 Loss=3.69570 t1=2.35558 t2=1.34012\n",
      "Epoch (score) 470 Loss=3.87957 t1=2.44019 t2=1.43937\n",
      "Epoch (score) 471 Loss=3.93703 t1=2.38588 t2=1.55115\n",
      "Epoch (score) 472 Loss=3.89930 t1=2.29325 t2=1.60605\n",
      "Epoch (score) 473 Loss=3.78813 t1=2.15123 t2=1.63690\n",
      "Epoch (score) 474 Loss=3.46648 t1=1.96076 t2=1.50572\n",
      "Epoch (score) 475 Loss=3.40560 t1=1.76232 t2=1.64328\n",
      "Epoch (score) 476 Loss=3.02183 t1=1.51137 t2=1.51046\n",
      "Epoch (score) 477 Loss=2.79028 t1=1.26994 t2=1.52034\n",
      "Epoch (score) 478 Loss=2.44780 t1=1.02885 t2=1.41895\n",
      "Epoch (score) 479 Loss=2.30155 t1=0.77532 t2=1.52622\n",
      "Epoch (critic) 480 Loss=2.12635 t1=0.55094 t2=1.57541\n",
      "Epoch (critic) 481 Loss=2.13244 t1=0.53783 t2=1.59461\n",
      "Epoch (critic) 482 Loss=2.09694 t1=0.52348 t2=1.57346\n",
      "Epoch (critic) 483 Loss=2.15115 t1=0.49030 t2=1.66085\n",
      "Epoch (critic) 484 Loss=2.04554 t1=0.46008 t2=1.58546\n",
      "Epoch (critic) 485 Loss=2.08508 t1=0.43463 t2=1.65045\n",
      "Epoch (critic) 486 Loss=2.23200 t1=0.41273 t2=1.81927\n",
      "Epoch (critic) 487 Loss=2.22995 t1=0.39253 t2=1.83743\n",
      "Epoch (critic) 488 Loss=2.32689 t1=0.38060 t2=1.94629\n",
      "Epoch (critic) 489 Loss=2.38662 t1=0.36491 t2=2.02170\n",
      "Epoch (score) 490 Loss=2.39302 t1=0.34714 t2=2.04588\n",
      "Epoch (score) 491 Loss=2.42616 t1=0.34865 t2=2.07751\n",
      "Epoch (score) 492 Loss=2.44718 t1=0.32649 t2=2.12069\n",
      "Epoch (score) 493 Loss=2.44228 t1=0.29137 t2=2.15092\n",
      "Epoch (score) 494 Loss=2.29224 t1=0.25929 t2=2.03295\n",
      "Epoch (score) 495 Loss=2.33191 t1=0.22184 t2=2.11007\n",
      "Epoch (score) 496 Loss=2.40596 t1=0.18954 t2=2.21641\n",
      "Epoch (score) 497 Loss=2.23154 t1=0.15438 t2=2.07716\n",
      "Epoch (score) 498 Loss=2.21423 t1=0.11690 t2=2.09732\n",
      "Epoch (score) 499 Loss=2.15401 t1=0.07786 t2=2.07615\n",
      "Epoch (critic) 500 Loss=2.07733 t1=0.03439 t2=2.04294\n",
      "Epoch (critic) 501 Loss=2.21027 t1=0.05243 t2=2.15784\n",
      "Epoch (critic) 502 Loss=2.14693 t1=0.07746 t2=2.06948\n",
      "Epoch (critic) 503 Loss=2.25498 t1=0.10338 t2=2.15160\n",
      "Epoch (critic) 504 Loss=2.45028 t1=0.11600 t2=2.33428\n",
      "Epoch (critic) 505 Loss=2.03121 t1=0.13101 t2=1.90020\n",
      "Epoch (critic) 506 Loss=2.09917 t1=0.13686 t2=1.96232\n",
      "Epoch (critic) 507 Loss=2.01922 t1=0.14641 t2=1.87281\n",
      "Epoch (critic) 508 Loss=2.03782 t1=0.15392 t2=1.88391\n",
      "Epoch (critic) 509 Loss=2.07674 t1=0.15952 t2=1.91722\n",
      "Epoch (score) 510 Loss=2.09546 t1=0.17017 t2=1.92529\n",
      "Epoch (score) 511 Loss=2.19223 t1=0.16230 t2=2.02992\n",
      "Epoch (score) 512 Loss=2.19215 t1=0.15531 t2=2.03685\n",
      "Epoch (score) 513 Loss=2.16921 t1=0.15034 t2=2.01887\n",
      "Epoch (score) 514 Loss=2.02906 t1=0.14846 t2=1.88060\n",
      "Epoch (score) 515 Loss=2.11090 t1=0.13179 t2=1.97911\n",
      "Epoch (score) 516 Loss=2.06355 t1=0.12329 t2=1.94027\n",
      "Epoch (score) 517 Loss=2.26921 t1=0.11345 t2=2.15576\n",
      "Epoch (score) 518 Loss=1.93387 t1=0.10059 t2=1.83328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 519 Loss=2.03681 t1=0.09217 t2=1.94464\n",
      "Epoch (critic) 520 Loss=2.07601 t1=0.07387 t2=2.00213\n",
      "Epoch (critic) 521 Loss=2.35566 t1=0.08810 t2=2.26756\n",
      "Epoch (critic) 522 Loss=2.69717 t1=0.09182 t2=2.60536\n",
      "Epoch (critic) 523 Loss=2.90164 t1=0.09979 t2=2.80184\n",
      "Epoch (critic) 524 Loss=3.10447 t1=0.11081 t2=2.99366\n",
      "Epoch (critic) 525 Loss=3.97533 t1=0.11908 t2=3.85625\n",
      "Epoch (critic) 526 Loss=4.20133 t1=0.13006 t2=4.07127\n",
      "Epoch (critic) 527 Loss=4.91886 t1=0.13561 t2=4.78325\n",
      "Epoch (critic) 528 Loss=5.51141 t1=0.13924 t2=5.37217\n",
      "Epoch (critic) 529 Loss=6.36855 t1=0.14474 t2=6.22381\n",
      "Epoch (score) 530 Loss=8.40068 t1=0.16140 t2=8.23928\n",
      "Epoch (score) 531 Loss=8.04774 t1=0.15727 t2=7.89047\n",
      "Epoch (score) 532 Loss=8.71952 t1=0.15912 t2=8.56040\n",
      "Epoch (score) 533 Loss=8.36503 t1=0.14755 t2=8.21748\n",
      "Epoch (score) 534 Loss=7.16243 t1=0.13436 t2=7.02807\n",
      "Epoch (score) 535 Loss=8.73102 t1=0.13433 t2=8.59669\n",
      "Epoch (score) 536 Loss=8.14272 t1=0.12350 t2=8.01923\n",
      "Epoch (score) 537 Loss=8.88515 t1=0.11174 t2=8.77341\n",
      "Epoch (score) 538 Loss=8.47554 t1=0.10655 t2=8.36899\n",
      "Epoch (score) 539 Loss=7.70018 t1=0.08624 t2=7.61394\n",
      "Epoch (critic) 540 Loss=8.10085 t1=0.08305 t2=8.01780\n",
      "Epoch (critic) 541 Loss=10.46679 t1=0.08551 t2=10.38127\n",
      "Epoch (critic) 542 Loss=12.34014 t1=0.10011 t2=12.24003\n",
      "Epoch (critic) 543 Loss=15.84081 t1=0.05379 t2=15.78702\n",
      "Epoch (critic) 544 Loss=13.87085 t1=0.08149 t2=13.78936\n",
      "Epoch (critic) 545 Loss=21.56936 t1=0.02895 t2=21.54041\n",
      "Epoch (critic) 546 Loss=19.31929 t1=-0.02253 t2=19.34182\n",
      "Epoch (critic) 547 Loss=40.63729 t1=-0.09616 t2=40.73345\n",
      "Epoch (critic) 548 Loss=9.88817 t1=0.11614 t2=9.77202\n",
      "Epoch (critic) 549 Loss=15.88304 t1=0.12575 t2=15.75730\n",
      "Epoch (score) 550 Loss=16.01816 t1=0.12629 t2=15.89186\n",
      "Epoch (score) 551 Loss=17.04431 t1=0.10824 t2=16.93607\n",
      "Epoch (score) 552 Loss=15.76220 t1=0.08398 t2=15.67823\n",
      "Epoch (score) 553 Loss=15.29415 t1=0.03823 t2=15.25592\n",
      "Epoch (score) 554 Loss=15.37654 t1=-0.01321 t2=15.38974\n",
      "Epoch (score) 555 Loss=17.00403 t1=-0.06875 t2=17.07278\n",
      "Epoch (score) 556 Loss=14.24030 t1=-0.15054 t2=14.39084\n",
      "Epoch (score) 557 Loss=14.69202 t1=-0.21426 t2=14.90628\n",
      "Epoch (score) 558 Loss=16.90918 t1=-0.31336 t2=17.22254\n",
      "Epoch (score) 559 Loss=16.70420 t1=-0.38574 t2=17.08994\n",
      "Epoch (critic) 560 Loss=15.05064 t1=-0.47112 t2=15.52176\n",
      "Epoch (critic) 561 Loss=23.11147 t1=-0.27253 t2=23.38400\n",
      "Epoch (critic) 562 Loss=25.93315 t1=-0.48326 t2=26.41641\n",
      "Epoch (critic) 563 Loss=20.99140 t1=-0.71174 t2=21.70314\n",
      "Epoch (critic) 564 Loss=14.57598 t1=0.94651 t2=13.62947\n",
      "Epoch (critic) 565 Loss=15.69554 t1=0.92172 t2=14.77382\n",
      "Epoch (critic) 566 Loss=16.16796 t1=0.59032 t2=15.57764\n",
      "Epoch (critic) 567 Loss=19.57069 t1=-0.12161 t2=19.69230\n",
      "Epoch (critic) 568 Loss=15.52830 t1=-0.52755 t2=16.05585\n",
      "Epoch (critic) 569 Loss=17.79407 t1=-0.26912 t2=18.06319\n",
      "Epoch (score) 570 Loss=25.97549 t1=0.19757 t2=25.77792\n",
      "Epoch (score) 571 Loss=23.17983 t1=0.18086 t2=22.99897\n",
      "Epoch (score) 572 Loss=21.60273 t1=0.10484 t2=21.49789\n",
      "Epoch (score) 573 Loss=20.91590 t1=0.03831 t2=20.87759\n",
      "Epoch (score) 574 Loss=22.10086 t1=-0.03496 t2=22.13582\n",
      "Epoch (score) 575 Loss=20.13275 t1=-0.12225 t2=20.25500\n",
      "Epoch (score) 576 Loss=26.03987 t1=-0.24894 t2=26.28881\n",
      "Epoch (score) 577 Loss=24.05872 t1=-0.29669 t2=24.35541\n",
      "Epoch (score) 578 Loss=21.36250 t1=-0.39901 t2=21.76151\n",
      "Epoch (score) 579 Loss=22.68099 t1=-0.59339 t2=23.27438\n",
      "Epoch (critic) 580 Loss=25.33391 t1=-0.73710 t2=26.07101\n",
      "Epoch (critic) 581 Loss=38.10146 t1=-0.55406 t2=38.65552\n",
      "Epoch (critic) 582 Loss=30.62147 t1=-1.00883 t2=31.63029\n",
      "Epoch (critic) 583 Loss=54.84326 t1=0.45973 t2=54.38352\n",
      "Epoch (critic) 584 Loss=5.30908 t1=-2.20081 t2=7.50989\n",
      "Epoch (critic) 585 Loss=13.46239 t1=-0.55698 t2=14.01938\n",
      "Epoch (critic) 586 Loss=7.07936 t1=1.30267 t2=5.77670\n",
      "Epoch (critic) 587 Loss=7.26659 t1=1.56677 t2=5.69983\n",
      "Epoch (critic) 588 Loss=16.02208 t1=1.11625 t2=14.90583\n",
      "Epoch (critic) 589 Loss=16.62027 t1=0.88716 t2=15.73311\n",
      "Epoch (score) 590 Loss=21.29467 t1=1.17478 t2=20.11989\n",
      "Epoch (score) 591 Loss=20.46287 t1=1.17138 t2=19.29149\n",
      "Epoch (score) 592 Loss=11.27936 t1=1.15357 t2=10.12579\n",
      "Epoch (score) 593 Loss=37.71438 t1=1.04376 t2=36.67062\n",
      "Epoch (score) 594 Loss=18.51321 t1=0.99275 t2=17.52045\n",
      "Epoch (score) 595 Loss=17.76810 t1=0.99345 t2=16.77465\n",
      "Epoch (score) 596 Loss=19.00991 t1=0.88386 t2=18.12605\n",
      "Epoch (score) 597 Loss=15.43751 t1=0.82225 t2=14.61527\n",
      "Epoch (score) 598 Loss=32.80399 t1=0.74304 t2=32.06095\n",
      "Epoch (score) 599 Loss=31.51565 t1=0.61124 t2=30.90441\n",
      "Epoch (critic) 600 Loss=12.09491 t1=0.49304 t2=11.60187\n",
      "Epoch (critic) 601 Loss=9.60230 t1=0.44725 t2=9.15505\n",
      "Epoch (critic) 602 Loss=24.97134 t1=0.20228 t2=24.76906\n",
      "Epoch (critic) 603 Loss=3.14789 t1=1.03640 t2=2.11149\n",
      "Epoch (critic) 604 Loss=3.22150 t1=1.53520 t2=1.68630\n",
      "Epoch (critic) 605 Loss=3.69477 t1=1.88059 t2=1.81418\n",
      "Epoch (critic) 606 Loss=3.66267 t1=1.82819 t2=1.83449\n",
      "Epoch (critic) 607 Loss=3.05336 t1=1.51756 t2=1.53580\n",
      "Epoch (critic) 608 Loss=2.80317 t1=1.14952 t2=1.65365\n",
      "Epoch (critic) 609 Loss=2.25377 t1=0.84318 t2=1.41058\n",
      "Epoch (score) 610 Loss=2.13888 t1=0.67354 t2=1.46534\n",
      "Epoch (score) 611 Loss=2.06768 t1=0.65589 t2=1.41179\n",
      "Epoch (score) 612 Loss=2.00290 t1=0.61465 t2=1.38825\n",
      "Epoch (score) 613 Loss=1.92435 t1=0.57211 t2=1.35224\n",
      "Epoch (score) 614 Loss=2.09529 t1=0.52942 t2=1.56588\n",
      "Epoch (score) 615 Loss=1.89952 t1=0.47567 t2=1.42385\n",
      "Epoch (score) 616 Loss=1.77684 t1=0.43087 t2=1.34597\n",
      "Epoch (score) 617 Loss=1.86611 t1=0.35948 t2=1.50663\n",
      "Epoch (score) 618 Loss=1.64878 t1=0.26683 t2=1.38195\n",
      "Epoch (score) 619 Loss=1.58333 t1=0.17852 t2=1.40481\n",
      "Epoch (critic) 620 Loss=1.62897 t1=0.07987 t2=1.54910\n",
      "Epoch (critic) 621 Loss=1.47035 t1=0.12688 t2=1.34347\n",
      "Epoch (critic) 622 Loss=1.56235 t1=0.21434 t2=1.34801\n",
      "Epoch (critic) 623 Loss=1.55597 t1=0.30893 t2=1.24705\n",
      "Epoch (critic) 624 Loss=1.67703 t1=0.37901 t2=1.29803\n",
      "Epoch (critic) 625 Loss=1.66856 t1=0.41088 t2=1.25768\n",
      "Epoch (critic) 626 Loss=1.73089 t1=0.41379 t2=1.31709\n",
      "Epoch (critic) 627 Loss=1.61180 t1=0.38727 t2=1.22453\n",
      "Epoch (critic) 628 Loss=1.60586 t1=0.35111 t2=1.25475\n",
      "Epoch (critic) 629 Loss=1.55320 t1=0.30970 t2=1.24350\n",
      "Epoch (score) 630 Loss=1.56855 t1=0.26900 t2=1.29955\n",
      "Epoch (score) 631 Loss=1.66209 t1=0.27112 t2=1.39098\n",
      "Epoch (score) 632 Loss=1.46850 t1=0.27114 t2=1.19736\n",
      "Epoch (score) 633 Loss=1.42226 t1=0.26586 t2=1.15640\n",
      "Epoch (score) 634 Loss=1.45933 t1=0.26627 t2=1.19306\n",
      "Epoch (score) 635 Loss=1.50336 t1=0.26130 t2=1.24206\n",
      "Epoch (score) 636 Loss=1.51798 t1=0.25201 t2=1.26597\n",
      "Epoch (score) 637 Loss=1.50408 t1=0.25069 t2=1.25339\n",
      "Epoch (score) 638 Loss=1.46584 t1=0.23383 t2=1.23201\n",
      "Epoch (score) 639 Loss=1.57801 t1=0.22481 t2=1.35320\n",
      "Epoch (critic) 640 Loss=1.47747 t1=0.21421 t2=1.26326\n",
      "Epoch (critic) 641 Loss=1.38829 t1=0.19897 t2=1.18932\n",
      "Epoch (critic) 642 Loss=1.43799 t1=0.20327 t2=1.23472\n",
      "Epoch (critic) 643 Loss=1.45656 t1=0.21991 t2=1.23665\n",
      "Epoch (critic) 644 Loss=1.61544 t1=0.25209 t2=1.36336\n",
      "Epoch (critic) 645 Loss=1.72965 t1=0.28646 t2=1.44319\n",
      "Epoch (critic) 646 Loss=1.74575 t1=0.31999 t2=1.42575\n",
      "Epoch (critic) 647 Loss=2.12807 t1=0.34502 t2=1.78305\n",
      "Epoch (critic) 648 Loss=2.09239 t1=0.35545 t2=1.73694\n",
      "Epoch (critic) 649 Loss=2.33214 t1=0.35318 t2=1.97896\n",
      "Epoch (score) 650 Loss=2.92194 t1=0.33287 t2=2.58907\n",
      "Epoch (score) 651 Loss=2.71703 t1=0.32041 t2=2.39662\n",
      "Epoch (score) 652 Loss=2.72985 t1=0.31714 t2=2.41270\n",
      "Epoch (score) 653 Loss=2.62712 t1=0.29505 t2=2.33207\n",
      "Epoch (score) 654 Loss=2.58214 t1=0.27238 t2=2.30976\n",
      "Epoch (score) 655 Loss=2.63761 t1=0.26046 t2=2.37715\n",
      "Epoch (score) 656 Loss=2.62910 t1=0.23102 t2=2.39808\n",
      "Epoch (score) 657 Loss=2.53148 t1=0.20473 t2=2.32675\n",
      "Epoch (score) 658 Loss=2.53472 t1=0.18117 t2=2.35355\n",
      "Epoch (score) 659 Loss=2.61632 t1=0.14940 t2=2.46692\n",
      "Epoch (critic) 660 Loss=2.43484 t1=0.12047 t2=2.31437\n",
      "Epoch (critic) 661 Loss=2.89959 t1=0.10925 t2=2.79034\n",
      "Epoch (critic) 662 Loss=3.54267 t1=0.09382 t2=3.44885\n",
      "Epoch (critic) 663 Loss=4.31735 t1=0.07990 t2=4.23745\n",
      "Epoch (critic) 664 Loss=5.16098 t1=0.07512 t2=5.08586\n",
      "Epoch (critic) 665 Loss=6.06487 t1=0.08164 t2=5.98323\n",
      "Epoch (critic) 666 Loss=7.00613 t1=0.10453 t2=6.90161\n",
      "Epoch (critic) 667 Loss=8.00707 t1=0.16355 t2=7.84352\n",
      "Epoch (critic) 668 Loss=10.43213 t1=0.23994 t2=10.19219\n",
      "Epoch (critic) 669 Loss=12.60552 t1=0.30742 t2=12.29811\n",
      "Epoch (score) 670 Loss=14.45011 t1=0.30607 t2=14.14404\n",
      "Epoch (score) 671 Loss=13.93087 t1=0.29193 t2=13.63894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 672 Loss=13.56387 t1=0.26154 t2=13.30233\n",
      "Epoch (score) 673 Loss=14.89110 t1=0.22717 t2=14.66393\n",
      "Epoch (score) 674 Loss=14.36528 t1=0.18469 t2=14.18059\n",
      "Epoch (score) 675 Loss=14.15414 t1=0.14140 t2=14.01274\n",
      "Epoch (score) 676 Loss=14.35186 t1=0.10773 t2=14.24413\n",
      "Epoch (score) 677 Loss=14.31831 t1=0.05362 t2=14.26468\n",
      "Epoch (score) 678 Loss=14.53427 t1=0.00414 t2=14.53013\n",
      "Epoch (score) 679 Loss=14.59657 t1=-0.04132 t2=14.63789\n",
      "Epoch (critic) 680 Loss=13.92447 t1=-0.09789 t2=14.02236\n",
      "Epoch (critic) 681 Loss=-0.27695 t1=0.25628 t2=-0.53324\n",
      "Epoch (critic) 682 Loss=3.24927 t1=-0.40546 t2=3.65472\n",
      "Epoch (critic) 683 Loss=1.71449 t1=-0.45478 t2=2.16927\n",
      "Epoch (critic) 684 Loss=1.18429 t1=-0.41092 t2=1.59522\n",
      "Epoch (critic) 685 Loss=0.87251 t1=-0.32200 t2=1.19451\n",
      "Epoch (critic) 686 Loss=1.42416 t1=-0.22850 t2=1.65265\n",
      "Epoch (critic) 687 Loss=1.21995 t1=-0.16597 t2=1.38592\n",
      "Epoch (critic) 688 Loss=0.93825 t1=-0.11049 t2=1.04874\n",
      "Epoch (critic) 689 Loss=0.65827 t1=-0.05964 t2=0.71791\n",
      "Epoch (score) 690 Loss=0.81933 t1=-0.01745 t2=0.83679\n",
      "Epoch (score) 691 Loss=1.02249 t1=-0.00292 t2=1.02542\n",
      "Epoch (score) 692 Loss=0.86722 t1=-0.00115 t2=0.86836\n",
      "Epoch (score) 693 Loss=1.03919 t1=-0.00136 t2=1.04056\n",
      "Epoch (score) 694 Loss=0.69643 t1=-0.01726 t2=0.71369\n",
      "Epoch (score) 695 Loss=0.99417 t1=-0.03847 t2=1.03265\n",
      "Epoch (score) 696 Loss=0.92097 t1=-0.06417 t2=0.98514\n",
      "Epoch (score) 697 Loss=0.64208 t1=-0.10345 t2=0.74553\n",
      "Epoch (score) 698 Loss=0.81055 t1=-0.13986 t2=0.95042\n",
      "Epoch (score) 699 Loss=0.59705 t1=-0.17869 t2=0.77574\n",
      "Epoch (critic) 700 Loss=0.55139 t1=-0.21729 t2=0.76867\n",
      "Epoch (critic) 701 Loss=0.71410 t1=-0.13375 t2=0.84785\n",
      "Epoch (critic) 702 Loss=0.82155 t1=-0.02579 t2=0.84734\n",
      "Epoch (critic) 703 Loss=0.97707 t1=0.05080 t2=0.92627\n",
      "Epoch (critic) 704 Loss=1.19379 t1=0.14699 t2=1.04681\n",
      "Epoch (critic) 705 Loss=1.21716 t1=0.21941 t2=0.99775\n",
      "Epoch (critic) 706 Loss=1.53503 t1=0.27393 t2=1.26110\n",
      "Epoch (critic) 707 Loss=1.41099 t1=0.31638 t2=1.09461\n",
      "Epoch (critic) 708 Loss=1.88531 t1=0.34248 t2=1.54283\n",
      "Epoch (critic) 709 Loss=2.01093 t1=0.36891 t2=1.64202\n",
      "Epoch (score) 710 Loss=2.52051 t1=0.37508 t2=2.14543\n",
      "Epoch (score) 711 Loss=2.48862 t1=0.39452 t2=2.09410\n",
      "Epoch (score) 712 Loss=2.37773 t1=0.41143 t2=1.96629\n",
      "Epoch (score) 713 Loss=2.21175 t1=0.41475 t2=1.79701\n",
      "Epoch (score) 714 Loss=2.25340 t1=0.40971 t2=1.84369\n",
      "Epoch (score) 715 Loss=2.62676 t1=0.40352 t2=2.22324\n",
      "Epoch (score) 716 Loss=2.34906 t1=0.38132 t2=1.96774\n",
      "Epoch (score) 717 Loss=2.20916 t1=0.36428 t2=1.84488\n",
      "Epoch (score) 718 Loss=2.05728 t1=0.33068 t2=1.72661\n",
      "Epoch (score) 719 Loss=2.04697 t1=0.30909 t2=1.73787\n",
      "Epoch (critic) 720 Loss=2.35258 t1=0.26572 t2=2.08686\n",
      "Epoch (critic) 721 Loss=2.52911 t1=0.27410 t2=2.25501\n",
      "Epoch (critic) 722 Loss=2.88536 t1=0.26140 t2=2.62396\n",
      "Epoch (critic) 723 Loss=3.51277 t1=0.24815 t2=3.26461\n",
      "Epoch (critic) 724 Loss=3.19347 t1=0.24539 t2=2.94808\n",
      "Epoch (critic) 725 Loss=4.10435 t1=0.23231 t2=3.87203\n",
      "Epoch (critic) 726 Loss=5.25626 t1=0.23196 t2=5.02429\n",
      "Epoch (critic) 727 Loss=3.99018 t1=0.25031 t2=3.73986\n",
      "Epoch (critic) 728 Loss=4.79995 t1=0.25129 t2=4.54866\n",
      "Epoch (critic) 729 Loss=3.85021 t1=0.24983 t2=3.60037\n",
      "Epoch (score) 730 Loss=4.28238 t1=0.20721 t2=4.07516\n",
      "Epoch (score) 731 Loss=4.28130 t1=0.20122 t2=4.08008\n",
      "Epoch (score) 732 Loss=4.22018 t1=0.16343 t2=4.05675\n",
      "Epoch (score) 733 Loss=4.24813 t1=0.15608 t2=4.09205\n",
      "Epoch (score) 734 Loss=4.23303 t1=0.11575 t2=4.11727\n",
      "Epoch (score) 735 Loss=4.33669 t1=0.09534 t2=4.24135\n",
      "Epoch (score) 736 Loss=6.85951 t1=0.07186 t2=6.78765\n",
      "Epoch (score) 737 Loss=7.13645 t1=0.05233 t2=7.08411\n",
      "Epoch (score) 738 Loss=6.72482 t1=0.02697 t2=6.69785\n",
      "Epoch (score) 739 Loss=4.05718 t1=0.00143 t2=4.05575\n",
      "Epoch (critic) 740 Loss=4.15616 t1=-0.02092 t2=4.17708\n",
      "Epoch (critic) 741 Loss=4.27896 t1=-0.02355 t2=4.30251\n",
      "Epoch (critic) 742 Loss=5.28085 t1=-0.02356 t2=5.30441\n",
      "Epoch (critic) 743 Loss=5.99204 t1=-0.02094 t2=6.01298\n",
      "Epoch (critic) 744 Loss=6.91185 t1=-0.01912 t2=6.93096\n",
      "Epoch (critic) 745 Loss=8.25376 t1=-0.01773 t2=8.27148\n",
      "Epoch (critic) 746 Loss=9.20632 t1=-0.01645 t2=9.22277\n",
      "Epoch (critic) 747 Loss=10.99134 t1=-0.01425 t2=11.00559\n",
      "Epoch (critic) 748 Loss=13.36806 t1=-0.01365 t2=13.38170\n",
      "Epoch (critic) 749 Loss=20.11115 t1=0.01226 t2=20.09889\n",
      "Epoch (score) 750 Loss=8.62657 t1=0.04485 t2=8.58172\n",
      "Epoch (score) 751 Loss=8.94228 t1=-0.08971 t2=9.03199\n",
      "Epoch (score) 752 Loss=8.15364 t1=-0.25827 t2=8.41191\n",
      "Epoch (score) 753 Loss=7.06975 t1=-0.44978 t2=7.51954\n",
      "Epoch (score) 754 Loss=7.32661 t1=-0.66290 t2=7.98951\n",
      "Epoch (score) 755 Loss=7.47906 t1=-0.92853 t2=8.40759\n",
      "Epoch (score) 756 Loss=9.33531 t1=-1.19043 t2=10.52574\n",
      "Epoch (score) 757 Loss=6.46268 t1=-1.44401 t2=7.90669\n",
      "Epoch (score) 758 Loss=5.45866 t1=-1.75563 t2=7.21428\n",
      "Epoch (score) 759 Loss=9.48712 t1=-2.15688 t2=11.64400\n",
      "Epoch (critic) 760 Loss=5.52479 t1=-2.56118 t2=8.08597\n",
      "Epoch (critic) 761 Loss=19.23721 t1=-1.43681 t2=20.67402\n",
      "Epoch (critic) 762 Loss=4.79527 t1=0.51262 t2=4.28265\n",
      "Epoch (critic) 763 Loss=30.76589 t1=0.39338 t2=30.37251\n",
      "Epoch (critic) 764 Loss=14.65239 t1=-1.35156 t2=16.00394\n",
      "Epoch (critic) 765 Loss=18.39225 t1=-0.65616 t2=19.04842\n",
      "Epoch (critic) 766 Loss=22.07145 t1=1.07453 t2=20.99692\n",
      "Epoch (critic) 767 Loss=31.94554 t1=1.36199 t2=30.58355\n",
      "Epoch (critic) 768 Loss=37.75407 t1=-0.38387 t2=38.13794\n",
      "Epoch (critic) 769 Loss=31.98795 t1=1.53915 t2=30.44880\n",
      "Epoch (score) 770 Loss=77.02324 t1=0.18281 t2=76.84042\n",
      "Epoch (score) 771 Loss=62.60983 t1=-0.11767 t2=62.72750\n",
      "Epoch (score) 772 Loss=69.82758 t1=0.05691 t2=69.77066\n",
      "Epoch (score) 773 Loss=50.56006 t1=-0.44342 t2=51.00348\n",
      "Epoch (score) 774 Loss=53.57784 t1=-0.90528 t2=54.48313\n",
      "Epoch (score) 775 Loss=63.18973 t1=-0.45739 t2=63.64712\n",
      "Epoch (score) 776 Loss=63.66821 t1=-0.62589 t2=64.29410\n",
      "Epoch (score) 777 Loss=70.51856 t1=-0.77153 t2=71.29009\n",
      "Epoch (score) 778 Loss=60.52985 t1=-1.12561 t2=61.65546\n",
      "Epoch (score) 779 Loss=54.41413 t1=-0.99232 t2=55.40645\n",
      "Epoch (critic) 780 Loss=61.15806 t1=-1.12305 t2=62.28111\n",
      "Epoch (critic) 781 Loss=11.45696 t1=4.58540 t2=6.87156\n",
      "Epoch (critic) 782 Loss=7.59828 t1=4.89253 t2=2.70574\n",
      "Epoch (critic) 783 Loss=7.85633 t1=2.70816 t2=5.14817\n",
      "Epoch (critic) 784 Loss=5.80142 t1=2.15998 t2=3.64144\n",
      "Epoch (critic) 785 Loss=5.68847 t1=2.34401 t2=3.34446\n",
      "Epoch (critic) 786 Loss=6.06933 t1=2.83099 t2=3.23835\n",
      "Epoch (critic) 787 Loss=6.43604 t1=2.66716 t2=3.76888\n",
      "Epoch (critic) 788 Loss=7.88676 t1=2.60643 t2=5.28033\n",
      "Epoch (critic) 789 Loss=10.01414 t1=2.46573 t2=7.54840\n",
      "Epoch (score) 790 Loss=11.65024 t1=2.31042 t2=9.33981\n",
      "Epoch (score) 791 Loss=9.36401 t1=2.38477 t2=6.97923\n",
      "Epoch (score) 792 Loss=10.28738 t1=2.46937 t2=7.81801\n",
      "Epoch (score) 793 Loss=12.34777 t1=2.38543 t2=9.96234\n",
      "Epoch (score) 794 Loss=9.22755 t1=2.28021 t2=6.94733\n",
      "Epoch (score) 795 Loss=11.60799 t1=2.27830 t2=9.32969\n",
      "Epoch (score) 796 Loss=9.88359 t1=2.31164 t2=7.57195\n",
      "Epoch (score) 797 Loss=12.07249 t1=2.17133 t2=9.90115\n",
      "Epoch (score) 798 Loss=9.47364 t1=1.87222 t2=7.60142\n",
      "Epoch (score) 799 Loss=10.53840 t1=1.74331 t2=8.79509\n",
      "Epoch (critic) 800 Loss=9.29199 t1=1.50214 t2=7.78984\n",
      "Epoch (critic) 801 Loss=12.11288 t1=2.87011 t2=9.24277\n",
      "Epoch (critic) 802 Loss=8.63834 t1=-0.31365 t2=8.95199\n",
      "Epoch (critic) 803 Loss=17.10429 t1=4.33845 t2=12.76584\n",
      "Epoch (critic) 804 Loss=17.24449 t1=1.87619 t2=15.36830\n",
      "Epoch (critic) 805 Loss=19.82794 t1=2.91147 t2=16.91648\n",
      "Epoch (critic) 806 Loss=12.80620 t1=0.29908 t2=12.50712\n",
      "Epoch (critic) 807 Loss=30.14515 t1=4.54535 t2=25.59980\n",
      "Epoch (critic) 808 Loss=23.29052 t1=0.17320 t2=23.11732\n",
      "Epoch (critic) 809 Loss=37.43063 t1=2.37841 t2=35.05222\n",
      "Epoch (score) 810 Loss=52.89700 t1=3.24789 t2=49.64912\n",
      "Epoch (score) 811 Loss=30.74776 t1=3.05324 t2=27.69451\n",
      "Epoch (score) 812 Loss=54.16821 t1=2.81303 t2=51.35518\n",
      "Epoch (score) 813 Loss=31.79533 t1=2.21686 t2=29.57847\n",
      "Epoch (score) 814 Loss=33.94577 t1=1.88096 t2=32.06481\n",
      "Epoch (score) 815 Loss=42.03923 t1=1.55977 t2=40.47947\n",
      "Epoch (score) 816 Loss=37.06506 t1=1.33270 t2=35.73237\n",
      "Epoch (score) 817 Loss=38.34027 t1=1.02233 t2=37.31794\n",
      "Epoch (score) 818 Loss=40.96553 t1=0.77578 t2=40.18975\n",
      "Epoch (score) 819 Loss=34.01305 t1=0.46092 t2=33.55213\n",
      "Epoch (critic) 820 Loss=30.67789 t1=0.24607 t2=30.43181\n",
      "Epoch (critic) 821 Loss=7.76550 t1=-2.57367 t2=10.33917\n",
      "Epoch (critic) 822 Loss=26.93983 t1=0.48685 t2=26.45297\n",
      "Epoch (critic) 823 Loss=17.79911 t1=4.98513 t2=12.81398\n",
      "Epoch (critic) 824 Loss=13.83061 t1=5.43987 t2=8.39073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 825 Loss=12.00709 t1=4.00284 t2=8.00425\n",
      "Epoch (critic) 826 Loss=11.81166 t1=2.54211 t2=9.26955\n",
      "Epoch (critic) 827 Loss=14.96974 t1=1.42619 t2=13.54356\n",
      "Epoch (critic) 828 Loss=17.22552 t1=0.95964 t2=16.26588\n",
      "Epoch (critic) 829 Loss=21.33997 t1=1.42424 t2=19.91572\n",
      "Epoch (score) 830 Loss=23.75080 t1=1.14995 t2=22.60084\n",
      "Epoch (score) 831 Loss=19.78999 t1=1.11741 t2=18.67258\n",
      "Epoch (score) 832 Loss=19.57207 t1=0.95322 t2=18.61885\n",
      "Epoch (score) 833 Loss=21.89864 t1=1.12228 t2=20.77636\n",
      "Epoch (score) 834 Loss=36.37086 t1=1.06735 t2=35.30351\n",
      "Epoch (score) 835 Loss=25.00457 t1=1.01782 t2=23.98675\n",
      "Epoch (score) 836 Loss=23.44004 t1=0.96966 t2=22.47039\n",
      "Epoch (score) 837 Loss=19.57954 t1=0.92207 t2=18.65746\n",
      "Epoch (score) 838 Loss=25.10772 t1=0.88468 t2=24.22304\n",
      "Epoch (score) 839 Loss=23.17212 t1=0.81947 t2=22.35265\n",
      "Epoch (critic) 840 Loss=28.80353 t1=0.72199 t2=28.08154\n",
      "Epoch (critic) 841 Loss=8.87062 t1=0.43437 t2=8.43624\n",
      "Epoch (critic) 842 Loss=21.37926 t1=0.88849 t2=20.49077\n",
      "Epoch (critic) 843 Loss=33.86416 t1=1.18115 t2=32.68301\n",
      "Epoch (critic) 844 Loss=12.80125 t1=0.63516 t2=12.16609\n",
      "Epoch (critic) 845 Loss=42.53967 t1=1.24335 t2=41.29632\n",
      "Epoch (critic) 846 Loss=18.05499 t1=0.30091 t2=17.75407\n",
      "Epoch (critic) 847 Loss=23.70116 t1=1.33676 t2=22.36440\n",
      "Epoch (critic) 848 Loss=34.40074 t1=1.75273 t2=32.64802\n",
      "Epoch (critic) 849 Loss=32.29350 t1=-0.10352 t2=32.39701\n",
      "Epoch (score) 850 Loss=24.40425 t1=1.99327 t2=22.41098\n",
      "Epoch (score) 851 Loss=26.07155 t1=1.71211 t2=24.35944\n",
      "Epoch (score) 852 Loss=28.41903 t1=1.27827 t2=27.14075\n",
      "Epoch (score) 853 Loss=18.25966 t1=0.82713 t2=17.43254\n",
      "Epoch (score) 854 Loss=22.67870 t1=0.27663 t2=22.40207\n",
      "Epoch (score) 855 Loss=18.04633 t1=-0.18819 t2=18.23452\n",
      "Epoch (score) 856 Loss=20.33554 t1=-0.69029 t2=21.02583\n",
      "Epoch (score) 857 Loss=22.32973 t1=-1.19033 t2=23.52005\n",
      "Epoch (score) 858 Loss=28.98685 t1=-1.73702 t2=30.72386\n",
      "Epoch (score) 859 Loss=20.96218 t1=-2.44750 t2=23.40969\n",
      "Epoch (critic) 860 Loss=17.84917 t1=-3.21306 t2=21.06223\n",
      "Epoch (critic) 861 Loss=19.29491 t1=-1.35061 t2=20.64552\n",
      "Epoch (critic) 862 Loss=12.01531 t1=0.61698 t2=11.39833\n",
      "Epoch (critic) 863 Loss=27.12955 t1=0.48284 t2=26.64671\n",
      "Epoch (critic) 864 Loss=14.48191 t1=-0.26760 t2=14.74951\n",
      "Epoch (critic) 865 Loss=17.34002 t1=0.84529 t2=16.49473\n",
      "Epoch (critic) 866 Loss=17.56187 t1=1.01542 t2=16.54645\n",
      "Epoch (critic) 867 Loss=21.21469 t1=0.32917 t2=20.88552\n",
      "Epoch (critic) 868 Loss=23.33468 t1=1.41672 t2=21.91795\n",
      "Epoch (critic) 869 Loss=4.40786 t1=0.52684 t2=3.88102\n",
      "Epoch (score) 870 Loss=3.10750 t1=0.36552 t2=2.74198\n",
      "Epoch (score) 871 Loss=4.02527 t1=0.32776 t2=3.69751\n",
      "Epoch (score) 872 Loss=2.80383 t1=0.19270 t2=2.61113\n",
      "Epoch (score) 873 Loss=3.64784 t1=0.04686 t2=3.60097\n",
      "Epoch (score) 874 Loss=2.37409 t1=-0.16846 t2=2.54255\n",
      "Epoch (score) 875 Loss=3.34509 t1=-0.36708 t2=3.71217\n",
      "Epoch (score) 876 Loss=0.77393 t1=-0.61553 t2=1.38945\n",
      "Epoch (score) 877 Loss=3.08713 t1=-0.78795 t2=3.87508\n",
      "Epoch (score) 878 Loss=2.85025 t1=-0.95547 t2=3.80572\n",
      "Epoch (score) 879 Loss=2.55335 t1=-1.15355 t2=3.70690\n",
      "Epoch (critic) 880 Loss=2.18264 t1=-1.37332 t2=3.55596\n",
      "Epoch (critic) 881 Loss=2.39528 t1=-0.10255 t2=2.49783\n",
      "Epoch (critic) 882 Loss=4.23113 t1=1.14682 t2=3.08431\n",
      "Epoch (critic) 883 Loss=5.04808 t1=1.88037 t2=3.16770\n",
      "Epoch (critic) 884 Loss=5.42991 t1=1.99963 t2=3.43028\n",
      "Epoch (critic) 885 Loss=5.85551 t1=1.92194 t2=3.93357\n",
      "Epoch (critic) 886 Loss=6.71316 t1=1.95659 t2=4.75658\n",
      "Epoch (critic) 887 Loss=7.32819 t1=2.11929 t2=5.20890\n",
      "Epoch (critic) 888 Loss=7.47408 t1=2.06687 t2=5.40721\n",
      "Epoch (critic) 889 Loss=9.22018 t1=2.18245 t2=7.03773\n",
      "Epoch (score) 890 Loss=10.26270 t1=1.97446 t2=8.28824\n",
      "Epoch (score) 891 Loss=9.93486 t1=2.01727 t2=7.91759\n",
      "Epoch (score) 892 Loss=9.96968 t1=1.86552 t2=8.10416\n",
      "Epoch (score) 893 Loss=10.06132 t1=1.87281 t2=8.18851\n",
      "Epoch (score) 894 Loss=10.08081 t1=1.93327 t2=8.14755\n",
      "Epoch (score) 895 Loss=9.77429 t1=1.70126 t2=8.07302\n",
      "Epoch (score) 896 Loss=10.24970 t1=1.71796 t2=8.53174\n",
      "Epoch (score) 897 Loss=9.60891 t1=1.54807 t2=8.06084\n",
      "Epoch (score) 898 Loss=9.55737 t1=1.44673 t2=8.11064\n",
      "Epoch (score) 899 Loss=9.22449 t1=1.22218 t2=8.00232\n",
      "Epoch (critic) 900 Loss=9.14537 t1=1.11520 t2=8.03018\n",
      "Epoch (critic) 901 Loss=11.00408 t1=1.10317 t2=9.90091\n",
      "Epoch (critic) 902 Loss=12.63937 t1=1.03045 t2=11.60892\n",
      "Epoch (critic) 903 Loss=16.03367 t1=1.45247 t2=14.58121\n",
      "Epoch (critic) 904 Loss=21.57750 t1=1.66482 t2=19.91268\n",
      "Epoch (critic) 905 Loss=25.13259 t1=1.25509 t2=23.87750\n",
      "Epoch (critic) 906 Loss=34.18152 t1=-0.02366 t2=34.20518\n",
      "Epoch (critic) 907 Loss=16.54344 t1=4.84641 t2=11.69703\n",
      "Epoch (critic) 908 Loss=17.30073 t1=1.12416 t2=16.17657\n",
      "Epoch (critic) 909 Loss=12.77544 t1=-1.96347 t2=14.73891\n",
      "Epoch (score) 910 Loss=23.23776 t1=1.93853 t2=21.29923\n",
      "Epoch (score) 911 Loss=21.64900 t1=1.94088 t2=19.70812\n",
      "Epoch (score) 912 Loss=23.59097 t1=1.87728 t2=21.71369\n",
      "Epoch (score) 913 Loss=21.49712 t1=1.65462 t2=19.84249\n",
      "Epoch (score) 914 Loss=20.24978 t1=1.45157 t2=18.79821\n",
      "Epoch (score) 915 Loss=19.44510 t1=1.40853 t2=18.03656\n",
      "Epoch (score) 916 Loss=21.42316 t1=1.14039 t2=20.28276\n",
      "Epoch (score) 917 Loss=22.00522 t1=1.14000 t2=20.86522\n",
      "Epoch (score) 918 Loss=23.44945 t1=0.93616 t2=22.51329\n",
      "Epoch (score) 919 Loss=20.16615 t1=0.76736 t2=19.39880\n",
      "Epoch (critic) 920 Loss=20.48642 t1=0.67605 t2=19.81037\n",
      "Epoch (critic) 921 Loss=21.77024 t1=1.39143 t2=20.37881\n",
      "Epoch (critic) 922 Loss=10.15258 t1=-0.58357 t2=10.73615\n",
      "Epoch (critic) 923 Loss=19.62094 t1=2.30399 t2=17.31695\n",
      "Epoch (critic) 924 Loss=27.05802 t1=1.50109 t2=25.55693\n",
      "Epoch (critic) 925 Loss=15.66809 t1=-0.56302 t2=16.23112\n",
      "Epoch (critic) 926 Loss=38.85473 t1=1.09433 t2=37.76040\n",
      "Epoch (critic) 927 Loss=89.82417 t1=1.45044 t2=88.37373\n",
      "Epoch (critic) 928 Loss=5.73526 t1=-0.78813 t2=6.52339\n",
      "Epoch (critic) 929 Loss=6.44861 t1=-0.51799 t2=6.96660\n",
      "Epoch (score) 930 Loss=6.92140 t1=-0.00844 t2=6.92984\n",
      "Epoch (score) 931 Loss=6.67648 t1=-0.00893 t2=6.68542\n",
      "Epoch (score) 932 Loss=6.93265 t1=-0.03974 t2=6.97239\n",
      "Epoch (score) 933 Loss=7.30904 t1=-0.05992 t2=7.36896\n",
      "Epoch (score) 934 Loss=6.73251 t1=-0.13738 t2=6.86989\n",
      "Epoch (score) 935 Loss=6.92576 t1=-0.24990 t2=7.17566\n",
      "Epoch (score) 936 Loss=6.46815 t1=-0.35116 t2=6.81930\n",
      "Epoch (score) 937 Loss=6.38780 t1=-0.46519 t2=6.85299\n",
      "Epoch (score) 938 Loss=6.55998 t1=-0.59186 t2=7.15184\n",
      "Epoch (score) 939 Loss=6.15564 t1=-0.67755 t2=6.83319\n",
      "Epoch (critic) 940 Loss=5.89776 t1=-0.81346 t2=6.71122\n",
      "Epoch (critic) 941 Loss=7.59458 t1=0.25613 t2=7.33845\n",
      "Epoch (critic) 942 Loss=8.55685 t1=0.81251 t2=7.74435\n",
      "Epoch (critic) 943 Loss=10.78393 t1=0.89462 t2=9.88931\n",
      "Epoch (critic) 944 Loss=12.10947 t1=0.86590 t2=11.24357\n",
      "Epoch (critic) 945 Loss=16.48317 t1=0.63924 t2=15.84393\n",
      "Epoch (critic) 946 Loss=18.52021 t1=0.47152 t2=18.04870\n",
      "Epoch (critic) 947 Loss=19.61620 t1=1.28820 t2=18.32799\n",
      "Epoch (critic) 948 Loss=21.12135 t1=-1.41824 t2=22.53959\n",
      "Epoch (critic) 949 Loss=10.09806 t1=2.49218 t2=7.60588\n",
      "Epoch (score) 950 Loss=12.48577 t1=1.26492 t2=11.22084\n",
      "Epoch (score) 951 Loss=11.42544 t1=1.21753 t2=10.20791\n",
      "Epoch (score) 952 Loss=11.03946 t1=1.13166 t2=9.90781\n",
      "Epoch (score) 953 Loss=11.89145 t1=1.02206 t2=10.86939\n",
      "Epoch (score) 954 Loss=11.56506 t1=0.96538 t2=10.59967\n",
      "Epoch (score) 955 Loss=11.36727 t1=0.91329 t2=10.45398\n",
      "Epoch (score) 956 Loss=11.48540 t1=0.84823 t2=10.63717\n",
      "Epoch (score) 957 Loss=11.23829 t1=0.78951 t2=10.44878\n",
      "Epoch (score) 958 Loss=12.13445 t1=0.73521 t2=11.39923\n",
      "Epoch (score) 959 Loss=11.93289 t1=0.67951 t2=11.25338\n",
      "Epoch (critic) 960 Loss=11.43954 t1=0.58139 t2=10.85814\n",
      "Epoch (critic) 961 Loss=10.54493 t1=0.10748 t2=10.43745\n",
      "Epoch (critic) 962 Loss=12.69329 t1=1.13251 t2=11.56078\n",
      "Epoch (critic) 963 Loss=11.12698 t1=0.00424 t2=11.12274\n",
      "Epoch (critic) 964 Loss=13.14502 t1=-0.18645 t2=13.33147\n",
      "Epoch (critic) 965 Loss=14.44901 t1=0.30175 t2=14.14726\n",
      "Epoch (critic) 966 Loss=18.97192 t1=0.46177 t2=18.51015\n",
      "Epoch (critic) 967 Loss=9.55426 t1=-0.04702 t2=9.60128\n",
      "Epoch (critic) 968 Loss=17.67343 t1=1.30505 t2=16.36837\n",
      "Epoch (critic) 969 Loss=22.95169 t1=0.53101 t2=22.42068\n",
      "Epoch (score) 970 Loss=22.93667 t1=0.21699 t2=22.71967\n",
      "Epoch (score) 971 Loss=21.01317 t1=0.14097 t2=20.87220\n",
      "Epoch (score) 972 Loss=22.43481 t1=0.18400 t2=22.25080\n",
      "Epoch (score) 973 Loss=23.15866 t1=-0.01648 t2=23.17514\n",
      "Epoch (score) 974 Loss=21.91004 t1=-0.04206 t2=21.95210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 975 Loss=23.88355 t1=-0.02564 t2=23.90919\n",
      "Epoch (score) 976 Loss=21.21980 t1=-0.13622 t2=21.35602\n",
      "Epoch (score) 977 Loss=22.09981 t1=-0.12176 t2=22.22157\n",
      "Epoch (score) 978 Loss=21.95132 t1=-0.19127 t2=22.14259\n",
      "Epoch (score) 979 Loss=21.00567 t1=-0.27274 t2=21.27842\n",
      "Epoch (critic) 980 Loss=21.80310 t1=-0.26891 t2=22.07201\n",
      "Epoch (critic) 981 Loss=15.61245 t1=1.72562 t2=13.88684\n",
      "Epoch (critic) 982 Loss=19.73370 t1=-0.15113 t2=19.88483\n",
      "Epoch (critic) 983 Loss=18.86824 t1=-0.11783 t2=18.98607\n",
      "Epoch (critic) 984 Loss=16.36283 t1=0.31684 t2=16.04599\n",
      "Epoch (critic) 985 Loss=17.01002 t1=0.55680 t2=16.45322\n",
      "Epoch (critic) 986 Loss=23.57064 t1=0.52249 t2=23.04815\n",
      "Epoch (critic) 987 Loss=33.62715 t1=-0.01125 t2=33.63840\n",
      "Epoch (critic) 988 Loss=45.19860 t1=0.06013 t2=45.13847\n",
      "Epoch (critic) 989 Loss=3.98409 t1=0.71723 t2=3.26686\n",
      "Epoch (score) 990 Loss=9.74079 t1=0.63075 t2=9.11003\n",
      "Epoch (score) 991 Loss=10.50595 t1=0.61133 t2=9.89462\n",
      "Epoch (score) 992 Loss=9.77994 t1=0.60365 t2=9.17629\n",
      "Epoch (score) 993 Loss=11.05528 t1=0.53757 t2=10.51771\n",
      "Epoch (score) 994 Loss=10.19881 t1=0.42434 t2=9.77447\n",
      "Epoch (score) 995 Loss=10.78682 t1=0.39513 t2=10.39168\n",
      "Epoch (score) 996 Loss=10.75893 t1=0.31796 t2=10.44097\n",
      "Epoch (score) 997 Loss=9.77053 t1=0.23380 t2=9.53673\n",
      "Epoch (score) 998 Loss=9.76857 t1=0.13678 t2=9.63180\n",
      "Epoch (score) 999 Loss=12.07411 t1=-0.01224 t2=12.08635\n",
      "Epoch (critic) 1000 Loss=9.82957 t1=-0.09980 t2=9.92937\n",
      "Epoch (critic) 1001 Loss=20.99209 t1=-0.00060 t2=20.99270\n",
      "Epoch (critic) 1002 Loss=24.11482 t1=0.46575 t2=23.64907\n",
      "Epoch (critic) 1003 Loss=28.00853 t1=0.47292 t2=27.53561\n",
      "Epoch (critic) 1004 Loss=15.34890 t1=0.30493 t2=15.04396\n",
      "Epoch (critic) 1005 Loss=26.17669 t1=0.32917 t2=25.84752\n",
      "Epoch (critic) 1006 Loss=40.82021 t1=-0.28967 t2=41.10989\n",
      "Epoch (critic) 1007 Loss=36.41341 t1=0.21678 t2=36.19663\n",
      "Epoch (critic) 1008 Loss=43.16893 t1=-0.85249 t2=44.02142\n",
      "Epoch (critic) 1009 Loss=20.37737 t1=0.71216 t2=19.66522\n",
      "Epoch (score) 1010 Loss=29.15032 t1=0.10399 t2=29.04634\n",
      "Epoch (score) 1011 Loss=36.93381 t1=0.09144 t2=36.84236\n",
      "Epoch (score) 1012 Loss=41.26120 t1=0.10662 t2=41.15458\n",
      "Epoch (score) 1013 Loss=39.22926 t1=0.04590 t2=39.18336\n",
      "Epoch (score) 1014 Loss=33.02041 t1=0.08096 t2=32.93945\n",
      "Epoch (score) 1015 Loss=44.28775 t1=-0.10323 t2=44.39098\n",
      "Epoch (score) 1016 Loss=37.89491 t1=-0.07822 t2=37.97313\n",
      "Epoch (score) 1017 Loss=36.64875 t1=-0.14316 t2=36.79190\n",
      "Epoch (score) 1018 Loss=36.93333 t1=-0.31586 t2=37.24918\n",
      "Epoch (score) 1019 Loss=29.70545 t1=-0.39482 t2=30.10027\n",
      "Epoch (critic) 1020 Loss=26.66505 t1=-0.45287 t2=27.11792\n",
      "Epoch (critic) 1021 Loss=34.87328 t1=-0.71492 t2=35.58820\n",
      "Epoch (critic) 1022 Loss=34.23603 t1=0.15803 t2=34.07800\n",
      "Epoch (critic) 1023 Loss=47.92046 t1=-0.38158 t2=48.30204\n",
      "Epoch (critic) 1024 Loss=113.36012 t1=0.25022 t2=113.10991\n",
      "Epoch (critic) 1025 Loss=78.10406 t1=-0.49113 t2=78.59518\n",
      "Epoch (critic) 1026 Loss=4.12511 t1=0.18081 t2=3.94430\n",
      "Epoch (critic) 1027 Loss=9.22234 t1=-0.48979 t2=9.71213\n",
      "Epoch (critic) 1028 Loss=35.40057 t1=0.28195 t2=35.11862\n",
      "Epoch (critic) 1029 Loss=15.52649 t1=-1.03518 t2=16.56167\n",
      "Epoch (score) 1030 Loss=6.15979 t1=-0.76412 t2=6.92392\n",
      "Epoch (score) 1031 Loss=10.79037 t1=-0.97271 t2=11.76308\n",
      "Epoch (score) 1032 Loss=10.62874 t1=-1.19100 t2=11.81975\n",
      "Epoch (score) 1033 Loss=9.60937 t1=-1.35230 t2=10.96166\n",
      "Epoch (score) 1034 Loss=9.34705 t1=-1.59984 t2=10.94689\n",
      "Epoch (score) 1035 Loss=8.67313 t1=-1.86776 t2=10.54089\n",
      "Epoch (score) 1036 Loss=7.49548 t1=-2.24548 t2=9.74096\n",
      "Epoch (score) 1037 Loss=8.61970 t1=-2.62048 t2=11.24018\n",
      "Epoch (score) 1038 Loss=8.28352 t1=-2.91066 t2=11.19419\n",
      "Epoch (score) 1039 Loss=5.02150 t1=-3.31860 t2=8.34010\n",
      "Epoch (critic) 1040 Loss=5.69621 t1=-3.85564 t2=9.55184\n",
      "Epoch (critic) 1041 Loss=7.13465 t1=-2.25506 t2=9.38971\n",
      "Epoch (critic) 1042 Loss=5.74957 t1=-0.96209 t2=6.71166\n",
      "Epoch (critic) 1043 Loss=5.76795 t1=0.25779 t2=5.51016\n",
      "Epoch (critic) 1044 Loss=5.85521 t1=1.11468 t2=4.74053\n",
      "Epoch (critic) 1045 Loss=7.63508 t1=2.05257 t2=5.58251\n",
      "Epoch (critic) 1046 Loss=8.21760 t1=2.81348 t2=5.40411\n",
      "Epoch (critic) 1047 Loss=8.57446 t1=3.38377 t2=5.19069\n",
      "Epoch (critic) 1048 Loss=9.59191 t1=3.69179 t2=5.90012\n",
      "Epoch (critic) 1049 Loss=12.18025 t1=4.00754 t2=8.17272\n",
      "Epoch (score) 1050 Loss=11.28653 t1=3.89716 t2=7.38937\n",
      "Epoch (score) 1051 Loss=11.87999 t1=4.30888 t2=7.57111\n",
      "Epoch (score) 1052 Loss=11.99591 t1=4.27467 t2=7.72124\n",
      "Epoch (score) 1053 Loss=13.40814 t1=4.35516 t2=9.05298\n",
      "Epoch (score) 1054 Loss=11.57000 t1=4.10779 t2=7.46220\n",
      "Epoch (score) 1055 Loss=11.62914 t1=3.99394 t2=7.63520\n",
      "Epoch (score) 1056 Loss=11.48497 t1=3.72399 t2=7.76098\n",
      "Epoch (score) 1057 Loss=11.25976 t1=3.49601 t2=7.76376\n",
      "Epoch (score) 1058 Loss=11.03614 t1=3.34301 t2=7.69314\n",
      "Epoch (score) 1059 Loss=9.94229 t1=3.06430 t2=6.87799\n",
      "Epoch (critic) 1060 Loss=13.13613 t1=2.93911 t2=10.19701\n",
      "Epoch (critic) 1061 Loss=12.47225 t1=2.67898 t2=9.79326\n",
      "Epoch (critic) 1062 Loss=12.64904 t1=2.46468 t2=10.18436\n",
      "Epoch (critic) 1063 Loss=13.95176 t1=2.08074 t2=11.87102\n",
      "Epoch (critic) 1064 Loss=20.39909 t1=2.18445 t2=18.21465\n",
      "Epoch (critic) 1065 Loss=24.73864 t1=2.32279 t2=22.41585\n",
      "Epoch (critic) 1066 Loss=32.43249 t1=2.93386 t2=29.49863\n",
      "Epoch (critic) 1067 Loss=29.24514 t1=3.26889 t2=25.97625\n",
      "Epoch (critic) 1068 Loss=33.30486 t1=3.96058 t2=29.34428\n",
      "Epoch (critic) 1069 Loss=47.43749 t1=0.77129 t2=46.66620\n",
      "Epoch (score) 1070 Loss=56.87481 t1=1.31145 t2=55.56337\n",
      "Epoch (score) 1071 Loss=58.76207 t1=1.58944 t2=57.17263\n",
      "Epoch (score) 1072 Loss=61.53569 t1=1.16001 t2=60.37567\n",
      "Epoch (score) 1073 Loss=52.70925 t1=0.56614 t2=52.14311\n",
      "Epoch (score) 1074 Loss=55.23129 t1=0.67351 t2=54.55777\n",
      "Epoch (score) 1075 Loss=63.25752 t1=0.79193 t2=62.46559\n",
      "Epoch (score) 1076 Loss=51.40994 t1=0.73093 t2=50.67901\n",
      "Epoch (score) 1077 Loss=63.94313 t1=0.62116 t2=63.32198\n",
      "Epoch (score) 1078 Loss=52.39166 t1=0.57483 t2=51.81683\n",
      "Epoch (score) 1079 Loss=62.34258 t1=0.65462 t2=61.68795\n",
      "Epoch (critic) 1080 Loss=53.46895 t1=0.39048 t2=53.07848\n",
      "Epoch (critic) 1081 Loss=9.53774 t1=1.49002 t2=8.04773\n",
      "Epoch (critic) 1082 Loss=6.77797 t1=1.30371 t2=5.47426\n",
      "Epoch (critic) 1083 Loss=33.04650 t1=0.86980 t2=32.17669\n",
      "Epoch (critic) 1084 Loss=67.82133 t1=-0.65872 t2=68.48004\n",
      "Epoch (critic) 1085 Loss=81.69543 t1=0.66261 t2=81.03282\n",
      "Epoch (critic) 1086 Loss=6.86411 t1=1.24125 t2=5.62286\n",
      "Epoch (critic) 1087 Loss=1.62659 t1=0.77989 t2=0.84670\n",
      "Epoch (critic) 1088 Loss=1.07568 t1=0.51914 t2=0.55653\n",
      "Epoch (critic) 1089 Loss=0.78340 t1=0.34097 t2=0.44243\n",
      "Epoch (score) 1090 Loss=1.30865 t1=0.20306 t2=1.10558\n",
      "Epoch (score) 1091 Loss=1.04580 t1=0.19662 t2=0.84918\n",
      "Epoch (score) 1092 Loss=1.27310 t1=0.17490 t2=1.09820\n",
      "Epoch (score) 1093 Loss=1.30748 t1=0.17329 t2=1.13419\n",
      "Epoch (score) 1094 Loss=1.30556 t1=0.14527 t2=1.16029\n",
      "Epoch (score) 1095 Loss=1.03952 t1=0.12715 t2=0.91238\n",
      "Epoch (score) 1096 Loss=1.42765 t1=0.11225 t2=1.31540\n",
      "Epoch (score) 1097 Loss=1.25931 t1=0.08450 t2=1.17482\n",
      "Epoch (score) 1098 Loss=1.43370 t1=0.07300 t2=1.36070\n",
      "Epoch (score) 1099 Loss=1.07069 t1=0.04179 t2=1.02890\n",
      "Epoch (critic) 1100 Loss=1.94647 t1=0.00675 t2=1.93972\n",
      "Epoch (critic) 1101 Loss=1.92626 t1=0.02164 t2=1.90463\n",
      "Epoch (critic) 1102 Loss=3.18160 t1=0.07191 t2=3.10970\n",
      "Epoch (critic) 1103 Loss=5.34150 t1=0.09456 t2=5.24694\n",
      "Epoch (critic) 1104 Loss=6.30976 t1=0.06908 t2=6.24068\n",
      "Epoch (critic) 1105 Loss=8.58475 t1=0.04831 t2=8.53644\n",
      "Epoch (critic) 1106 Loss=10.08137 t1=0.02426 t2=10.05711\n",
      "Epoch (critic) 1107 Loss=14.77855 t1=0.02790 t2=14.75065\n",
      "Epoch (critic) 1108 Loss=24.66913 t1=0.00333 t2=24.66580\n",
      "Epoch (critic) 1109 Loss=38.40723 t1=0.00159 t2=38.40564\n",
      "Epoch (score) 1110 Loss=63.62847 t1=0.02173 t2=63.60675\n",
      "Epoch (score) 1111 Loss=48.93045 t1=0.00332 t2=48.92713\n",
      "Epoch (score) 1112 Loss=46.80379 t1=-0.00935 t2=46.81314\n",
      "Epoch (score) 1113 Loss=56.57164 t1=0.03130 t2=56.54034\n",
      "Epoch (score) 1114 Loss=54.52131 t1=-0.01632 t2=54.53762\n",
      "Epoch (score) 1115 Loss=45.61333 t1=-0.02994 t2=45.64327\n",
      "Epoch (score) 1116 Loss=52.54703 t1=-0.00810 t2=52.55512\n",
      "Epoch (score) 1117 Loss=47.92036 t1=-0.02142 t2=47.94178\n",
      "Epoch (score) 1118 Loss=52.61519 t1=-0.03846 t2=52.65364\n",
      "Epoch (score) 1119 Loss=43.08931 t1=-0.00347 t2=43.09278\n",
      "Epoch (critic) 1120 Loss=47.82476 t1=-0.05687 t2=47.88163\n",
      "Epoch (critic) 1121 Loss=57.39110 t1=-0.07052 t2=57.46162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 1122 Loss=87.64816 t1=-0.02706 t2=87.67522\n",
      "Epoch (critic) 1123 Loss=88.06274 t1=-0.06411 t2=88.12684\n",
      "Epoch (critic) 1124 Loss=45.94762 t1=-0.20157 t2=46.14919\n",
      "Epoch (critic) 1125 Loss=40.66897 t1=-0.08402 t2=40.75299\n",
      "Epoch (critic) 1126 Loss=35.00948 t1=0.00975 t2=34.99972\n",
      "Epoch (critic) 1127 Loss=5.70201 t1=-0.05687 t2=5.75888\n",
      "Epoch (critic) 1128 Loss=2.95908 t1=-0.10608 t2=3.06515\n",
      "Epoch (critic) 1129 Loss=9.08715 t1=-0.09181 t2=9.17896\n",
      "Epoch (score) 1130 Loss=10.83413 t1=-0.10363 t2=10.93775\n",
      "Epoch (score) 1131 Loss=11.53030 t1=-0.10268 t2=11.63297\n",
      "Epoch (score) 1132 Loss=10.19377 t1=-0.11746 t2=10.31122\n",
      "Epoch (score) 1133 Loss=9.69872 t1=-0.13401 t2=9.83273\n",
      "Epoch (score) 1134 Loss=10.19088 t1=-0.15617 t2=10.34705\n",
      "Epoch (score) 1135 Loss=10.75740 t1=-0.17615 t2=10.93355\n",
      "Epoch (score) 1136 Loss=10.32595 t1=-0.21001 t2=10.53596\n",
      "Epoch (score) 1137 Loss=10.12622 t1=-0.24976 t2=10.37598\n",
      "Epoch (score) 1138 Loss=9.69902 t1=-0.29461 t2=9.99362\n",
      "Epoch (score) 1139 Loss=10.14954 t1=-0.31498 t2=10.46452\n",
      "Epoch (critic) 1140 Loss=10.20163 t1=-0.39118 t2=10.59281\n",
      "Epoch (critic) 1141 Loss=11.79409 t1=-0.29564 t2=12.08973\n",
      "Epoch (critic) 1142 Loss=9.43409 t1=-0.15072 t2=9.58480\n",
      "Epoch (critic) 1143 Loss=11.05971 t1=0.00043 t2=11.05927\n",
      "Epoch (critic) 1144 Loss=11.97006 t1=0.05960 t2=11.91047\n",
      "Epoch (critic) 1145 Loss=12.50699 t1=-0.03267 t2=12.53966\n",
      "Epoch (critic) 1146 Loss=14.06273 t1=-0.12323 t2=14.18596\n",
      "Epoch (critic) 1147 Loss=18.50804 t1=0.24310 t2=18.26494\n",
      "Epoch (critic) 1148 Loss=21.45186 t1=0.23045 t2=21.22141\n",
      "Epoch (critic) 1149 Loss=-1.02957 t1=0.18227 t2=-1.21184\n",
      "Epoch (score) 1150 Loss=-2.03649 t1=0.37163 t2=-2.40812\n",
      "Epoch (score) 1151 Loss=-1.91148 t1=0.38680 t2=-2.29828\n",
      "Epoch (score) 1152 Loss=-1.45305 t1=0.39414 t2=-1.84719\n",
      "Epoch (score) 1153 Loss=-1.27211 t1=0.41370 t2=-1.68581\n",
      "Epoch (score) 1154 Loss=-1.83544 t1=0.39293 t2=-2.22837\n",
      "Epoch (score) 1155 Loss=-1.89731 t1=0.36849 t2=-2.26580\n",
      "Epoch (score) 1156 Loss=-2.18673 t1=0.35420 t2=-2.54094\n",
      "Epoch (score) 1157 Loss=-2.05188 t1=0.32836 t2=-2.38024\n",
      "Epoch (score) 1158 Loss=-1.76751 t1=0.28684 t2=-2.05435\n",
      "Epoch (score) 1159 Loss=-1.74592 t1=0.24133 t2=-1.98725\n",
      "Epoch (critic) 1160 Loss=-2.53412 t1=0.19607 t2=-2.73018\n",
      "Epoch (critic) 1161 Loss=6.67274 t1=0.23308 t2=6.43966\n",
      "Epoch (critic) 1162 Loss=10.77404 t1=0.56607 t2=10.20797\n",
      "Epoch (critic) 1163 Loss=9.93324 t1=0.55058 t2=9.38266\n",
      "Epoch (critic) 1164 Loss=9.75176 t1=0.49281 t2=9.25896\n",
      "Epoch (critic) 1165 Loss=10.13842 t1=0.38396 t2=9.75446\n",
      "Epoch (critic) 1166 Loss=10.84733 t1=0.25585 t2=10.59148\n",
      "Epoch (critic) 1167 Loss=11.07964 t1=0.13951 t2=10.94012\n",
      "Epoch (critic) 1168 Loss=13.55248 t1=0.04656 t2=13.50592\n",
      "Epoch (critic) 1169 Loss=18.52006 t1=-0.00039 t2=18.52045\n",
      "Epoch (score) 1170 Loss=22.88566 t1=-0.02727 t2=22.91293\n",
      "Epoch (score) 1171 Loss=23.19398 t1=-0.01804 t2=23.21202\n",
      "Epoch (score) 1172 Loss=24.76400 t1=-0.01547 t2=24.77947\n",
      "Epoch (score) 1173 Loss=24.96172 t1=-0.03736 t2=24.99908\n",
      "Epoch (score) 1174 Loss=23.65673 t1=-0.04312 t2=23.69986\n",
      "Epoch (score) 1175 Loss=24.49555 t1=-0.05074 t2=24.54629\n",
      "Epoch (score) 1176 Loss=23.72677 t1=-0.07375 t2=23.80052\n",
      "Epoch (score) 1177 Loss=23.24382 t1=-0.09920 t2=23.34302\n",
      "Epoch (score) 1178 Loss=25.01800 t1=-0.11560 t2=25.13359\n",
      "Epoch (score) 1179 Loss=24.07376 t1=-0.15270 t2=24.22646\n",
      "Epoch (critic) 1180 Loss=22.98381 t1=-0.17508 t2=23.15889\n",
      "Epoch (critic) 1181 Loss=29.38215 t1=-0.13613 t2=29.51829\n",
      "Epoch (critic) 1182 Loss=50.55566 t1=-0.08351 t2=50.63918\n",
      "Epoch (critic) 1183 Loss=2.23057 t1=-0.09466 t2=2.32523\n",
      "Epoch (critic) 1184 Loss=10.45860 t1=-0.30607 t2=10.76467\n",
      "Epoch (critic) 1185 Loss=7.92678 t1=-0.27270 t2=8.19948\n",
      "Epoch (critic) 1186 Loss=9.52657 t1=-0.07288 t2=9.59945\n",
      "Epoch (critic) 1187 Loss=7.95800 t1=0.05293 t2=7.90507\n",
      "Epoch (critic) 1188 Loss=10.36264 t1=0.14454 t2=10.21810\n",
      "Epoch (critic) 1189 Loss=10.44664 t1=0.18810 t2=10.25854\n",
      "Epoch (score) 1190 Loss=11.58782 t1=0.19411 t2=11.39371\n",
      "Epoch (score) 1191 Loss=11.58679 t1=0.20314 t2=11.38365\n",
      "Epoch (score) 1192 Loss=12.42670 t1=0.19025 t2=12.23645\n",
      "Epoch (score) 1193 Loss=11.50810 t1=0.18483 t2=11.32327\n",
      "Epoch (score) 1194 Loss=11.52674 t1=0.15522 t2=11.37152\n",
      "Epoch (score) 1195 Loss=11.31719 t1=0.13979 t2=11.17740\n",
      "Epoch (score) 1196 Loss=11.12440 t1=0.11363 t2=11.01077\n",
      "Epoch (score) 1197 Loss=11.19442 t1=0.09917 t2=11.09525\n",
      "Epoch (score) 1198 Loss=12.44709 t1=0.07113 t2=12.37596\n",
      "Epoch (score) 1199 Loss=10.58342 t1=0.03914 t2=10.54428\n",
      "Epoch (critic) 1200 Loss=11.54702 t1=0.00629 t2=11.54072\n",
      "Epoch (critic) 1201 Loss=12.71074 t1=-0.04134 t2=12.75208\n",
      "Epoch (critic) 1202 Loss=11.82840 t1=-0.10151 t2=11.92991\n",
      "Epoch (critic) 1203 Loss=15.69196 t1=-0.15701 t2=15.84898\n",
      "Epoch (critic) 1204 Loss=17.48582 t1=-0.16495 t2=17.65077\n",
      "Epoch (critic) 1205 Loss=20.75938 t1=-0.17062 t2=20.93000\n",
      "Epoch (critic) 1206 Loss=29.00796 t1=-0.12849 t2=29.13645\n",
      "Epoch (critic) 1207 Loss=31.81317 t1=0.00401 t2=31.80916\n",
      "Epoch (critic) 1208 Loss=42.37106 t1=0.17340 t2=42.19767\n",
      "Epoch (critic) 1209 Loss=62.23700 t1=0.19694 t2=62.04007\n",
      "Epoch (score) 1210 Loss=10.76953 t1=0.40027 t2=10.36926\n",
      "Epoch (score) 1211 Loss=12.49473 t1=0.38491 t2=12.10982\n",
      "Epoch (score) 1212 Loss=9.48338 t1=0.33557 t2=9.14781\n",
      "Epoch (score) 1213 Loss=13.43321 t1=0.29186 t2=13.14135\n",
      "Epoch (score) 1214 Loss=10.95819 t1=0.22536 t2=10.73283\n",
      "Epoch (score) 1215 Loss=12.59650 t1=0.14919 t2=12.44731\n",
      "Epoch (score) 1216 Loss=10.36094 t1=0.10176 t2=10.25919\n",
      "Epoch (score) 1217 Loss=12.58360 t1=0.00049 t2=12.58311\n",
      "Epoch (score) 1218 Loss=12.71885 t1=-0.08628 t2=12.80513\n",
      "Epoch (score) 1219 Loss=13.18723 t1=-0.19039 t2=13.37762\n",
      "Epoch (critic) 1220 Loss=11.28334 t1=-0.27903 t2=11.56237\n",
      "Epoch (critic) 1221 Loss=9.54570 t1=-0.33700 t2=9.88270\n",
      "Epoch (critic) 1222 Loss=18.27658 t1=-0.17524 t2=18.45182\n",
      "Epoch (critic) 1223 Loss=18.58331 t1=-0.02310 t2=18.60641\n",
      "Epoch (critic) 1224 Loss=16.50683 t1=0.06888 t2=16.43795\n",
      "Epoch (critic) 1225 Loss=18.57000 t1=0.14763 t2=18.42237\n",
      "Epoch (critic) 1226 Loss=21.83355 t1=0.12424 t2=21.70931\n",
      "Epoch (critic) 1227 Loss=22.07716 t1=0.09021 t2=21.98695\n",
      "Epoch (critic) 1228 Loss=24.75002 t1=0.12037 t2=24.62965\n",
      "Epoch (critic) 1229 Loss=31.70826 t1=0.20857 t2=31.49969\n",
      "Epoch (score) 1230 Loss=37.89743 t1=0.32728 t2=37.57015\n",
      "Epoch (score) 1231 Loss=32.50713 t1=0.29205 t2=32.21508\n",
      "Epoch (score) 1232 Loss=33.64214 t1=0.26289 t2=33.37925\n",
      "Epoch (score) 1233 Loss=31.85868 t1=0.21849 t2=31.64019\n",
      "Epoch (score) 1234 Loss=34.97413 t1=0.16628 t2=34.80785\n",
      "Epoch (score) 1235 Loss=36.66066 t1=0.07618 t2=36.58448\n",
      "Epoch (score) 1236 Loss=31.67703 t1=0.04755 t2=31.62948\n",
      "Epoch (score) 1237 Loss=32.24604 t1=-0.04712 t2=32.29317\n",
      "Epoch (score) 1238 Loss=32.60508 t1=-0.19379 t2=32.79887\n",
      "Epoch (score) 1239 Loss=26.52212 t1=-0.31700 t2=26.83912\n",
      "Epoch (critic) 1240 Loss=32.53059 t1=-0.47237 t2=33.00296\n",
      "Epoch (critic) 1241 Loss=48.45274 t1=0.13681 t2=48.31593\n",
      "Epoch (critic) 1242 Loss=43.63969 t1=0.03316 t2=43.60653\n",
      "Epoch (critic) 1243 Loss=43.48412 t1=-0.29123 t2=43.77535\n",
      "Epoch (critic) 1244 Loss=24.43183 t1=1.95499 t2=22.47684\n",
      "Epoch (critic) 1245 Loss=27.94990 t1=1.46303 t2=26.48686\n",
      "Epoch (critic) 1246 Loss=30.42924 t1=1.11744 t2=29.31180\n",
      "Epoch (critic) 1247 Loss=39.35958 t1=0.84798 t2=38.51160\n",
      "Epoch (critic) 1248 Loss=60.56421 t1=-0.05491 t2=60.61913\n",
      "Epoch (critic) 1249 Loss=30.10616 t1=-1.01666 t2=31.12283\n",
      "Epoch (score) 1250 Loss=30.64832 t1=0.00747 t2=30.64085\n",
      "Epoch (score) 1251 Loss=23.39132 t1=-0.07774 t2=23.46906\n",
      "Epoch (score) 1252 Loss=23.15648 t1=-0.18960 t2=23.34608\n",
      "Epoch (score) 1253 Loss=22.83855 t1=-0.18894 t2=23.02749\n",
      "Epoch (score) 1254 Loss=22.91191 t1=-0.36772 t2=23.27963\n",
      "Epoch (score) 1255 Loss=19.30442 t1=-0.41207 t2=19.71649\n",
      "Epoch (score) 1256 Loss=22.48235 t1=-0.53560 t2=23.01794\n",
      "Epoch (score) 1257 Loss=20.95687 t1=-0.81894 t2=21.77581\n",
      "Epoch (score) 1258 Loss=21.17619 t1=-0.79596 t2=21.97215\n",
      "Epoch (score) 1259 Loss=24.62201 t1=-0.85956 t2=25.48157\n",
      "Epoch (critic) 1260 Loss=14.23784 t1=-1.10342 t2=15.34126\n",
      "Epoch (critic) 1261 Loss=27.75811 t1=1.69141 t2=26.06670\n",
      "Epoch (critic) 1262 Loss=41.24225 t1=0.36138 t2=40.88087\n",
      "Epoch (critic) 1263 Loss=54.75640 t1=2.26692 t2=52.48949\n",
      "Epoch (critic) 1264 Loss=48.76373 t1=3.09455 t2=45.66918\n",
      "Epoch (critic) 1265 Loss=59.50901 t1=0.96510 t2=58.54391\n",
      "Epoch (critic) 1266 Loss=34.10249 t1=-0.53301 t2=34.63550\n",
      "Epoch (critic) 1267 Loss=27.89499 t1=0.18582 t2=27.70918\n",
      "Epoch (critic) 1268 Loss=44.01490 t1=0.81875 t2=43.19615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 1269 Loss=58.55017 t1=0.26248 t2=58.28769\n",
      "Epoch (score) 1270 Loss=41.93622 t1=3.41634 t2=38.51988\n",
      "Epoch (score) 1271 Loss=45.11171 t1=3.36251 t2=41.74921\n",
      "Epoch (score) 1272 Loss=42.84227 t1=3.23310 t2=39.60918\n",
      "Epoch (score) 1273 Loss=42.44507 t1=2.92721 t2=39.51787\n",
      "Epoch (score) 1274 Loss=41.38221 t1=2.51442 t2=38.86779\n",
      "Epoch (score) 1275 Loss=39.87738 t1=2.07728 t2=37.80009\n",
      "Epoch (score) 1276 Loss=39.65256 t1=1.63624 t2=38.01633\n",
      "Epoch (score) 1277 Loss=39.67358 t1=1.17671 t2=38.49687\n",
      "Epoch (score) 1278 Loss=39.31362 t1=0.83543 t2=38.47819\n",
      "Epoch (score) 1279 Loss=39.57492 t1=0.62839 t2=38.94653\n",
      "Epoch (critic) 1280 Loss=39.14182 t1=0.46897 t2=38.67285\n",
      "Epoch (critic) 1281 Loss=48.40767 t1=-0.33599 t2=48.74366\n",
      "Epoch (critic) 1282 Loss=44.87888 t1=-0.51515 t2=45.39403\n",
      "Epoch (critic) 1283 Loss=62.29413 t1=-0.26148 t2=62.55561\n",
      "Epoch (critic) 1284 Loss=36.92955 t1=0.34253 t2=36.58701\n",
      "Epoch (critic) 1285 Loss=43.01465 t1=-0.20208 t2=43.21672\n",
      "Epoch (critic) 1286 Loss=43.59048 t1=-0.00590 t2=43.59638\n",
      "Epoch (critic) 1287 Loss=19.03789 t1=-0.50197 t2=19.53986\n",
      "Epoch (critic) 1288 Loss=7.66505 t1=-0.16963 t2=7.83468\n",
      "Epoch (critic) 1289 Loss=22.94844 t1=-0.03281 t2=22.98124\n",
      "Epoch (score) 1290 Loss=38.54130 t1=0.28436 t2=38.25694\n",
      "Epoch (score) 1291 Loss=38.68691 t1=0.21165 t2=38.47527\n",
      "Epoch (score) 1292 Loss=38.83086 t1=0.13499 t2=38.69587\n",
      "Epoch (score) 1293 Loss=38.79479 t1=0.07378 t2=38.72102\n",
      "Epoch (score) 1294 Loss=35.67934 t1=0.00741 t2=35.67193\n",
      "Epoch (score) 1295 Loss=39.78748 t1=-0.04368 t2=39.83116\n",
      "Epoch (score) 1296 Loss=37.90303 t1=-0.09513 t2=37.99816\n",
      "Epoch (score) 1297 Loss=43.94844 t1=-0.13692 t2=44.08535\n",
      "Epoch (score) 1298 Loss=37.29769 t1=-0.18650 t2=37.48419\n",
      "Epoch (score) 1299 Loss=37.00141 t1=-0.23019 t2=37.23160\n",
      "Epoch (critic) 1300 Loss=37.92638 t1=-0.29110 t2=38.21748\n",
      "Epoch (critic) 1301 Loss=58.59008 t1=-0.18024 t2=58.77032\n",
      "Epoch (critic) 1302 Loss=51.05384 t1=-0.00315 t2=51.05698\n",
      "Epoch (critic) 1303 Loss=50.67495 t1=-0.02333 t2=50.69828\n",
      "Epoch (critic) 1304 Loss=65.81439 t1=-0.13320 t2=65.94759\n",
      "Epoch (critic) 1305 Loss=121.53722 t1=-0.22349 t2=121.76071\n",
      "Epoch (critic) 1306 Loss=32.40009 t1=-0.05779 t2=32.45789\n",
      "Epoch (critic) 1307 Loss=60.45976 t1=-0.61872 t2=61.07848\n",
      "Epoch (critic) 1308 Loss=60.90235 t1=-0.78135 t2=61.68370\n",
      "Epoch (critic) 1309 Loss=55.99796 t1=-0.30087 t2=56.29882\n",
      "Epoch (score) 1310 Loss=58.21347 t1=-0.13672 t2=58.35020\n",
      "Epoch (score) 1311 Loss=60.92590 t1=-0.20649 t2=61.13239\n",
      "Epoch (score) 1312 Loss=73.90485 t1=-0.15640 t2=74.06125\n",
      "Epoch (score) 1313 Loss=56.33068 t1=-0.30660 t2=56.63728\n",
      "Epoch (score) 1314 Loss=49.75497 t1=-0.31021 t2=50.06518\n",
      "Epoch (score) 1315 Loss=66.63577 t1=-0.36092 t2=66.99669\n",
      "Epoch (score) 1316 Loss=58.08015 t1=-0.38634 t2=58.46648\n",
      "Epoch (score) 1317 Loss=47.77606 t1=-0.35986 t2=48.13593\n",
      "Epoch (score) 1318 Loss=63.13268 t1=-0.53159 t2=63.66426\n",
      "Epoch (score) 1319 Loss=57.38722 t1=-0.44470 t2=57.83192\n",
      "Epoch (critic) 1320 Loss=51.23673 t1=-0.43139 t2=51.66813\n",
      "Epoch (critic) 1321 Loss=39.95782 t1=-0.04478 t2=40.00260\n",
      "Epoch (critic) 1322 Loss=41.41917 t1=0.71111 t2=40.70806\n",
      "Epoch (critic) 1323 Loss=35.78687 t1=0.64401 t2=35.14285\n",
      "Epoch (critic) 1324 Loss=45.45232 t1=0.14583 t2=45.30650\n",
      "Epoch (critic) 1325 Loss=42.34484 t1=0.06269 t2=42.28215\n",
      "Epoch (critic) 1326 Loss=42.99251 t1=0.09516 t2=42.89735\n",
      "Epoch (critic) 1327 Loss=56.00742 t1=0.22914 t2=55.77827\n",
      "Epoch (critic) 1328 Loss=57.15945 t1=0.31974 t2=56.83970\n",
      "Epoch (critic) 1329 Loss=62.13457 t1=0.17865 t2=61.95592\n",
      "Epoch (score) 1330 Loss=90.35736 t1=0.15870 t2=90.19867\n",
      "Epoch (score) 1331 Loss=85.70891 t1=0.26803 t2=85.44087\n",
      "Epoch (score) 1332 Loss=92.29433 t1=0.23345 t2=92.06087\n",
      "Epoch (score) 1333 Loss=90.53488 t1=0.47348 t2=90.06140\n",
      "Epoch (score) 1334 Loss=89.48717 t1=0.07188 t2=89.41528\n",
      "Epoch (score) 1335 Loss=79.52816 t1=0.06541 t2=79.46275\n",
      "Epoch (score) 1336 Loss=83.51917 t1=0.09836 t2=83.42081\n",
      "Epoch (score) 1337 Loss=91.41641 t1=0.22747 t2=91.18895\n",
      "Epoch (score) 1338 Loss=82.07623 t1=0.13953 t2=81.93671\n",
      "Epoch (score) 1339 Loss=92.24239 t1=-0.24451 t2=92.48689\n",
      "Epoch (critic) 1340 Loss=92.73570 t1=-0.01185 t2=92.74756\n",
      "Epoch (critic) 1341 Loss=85.49188 t1=-1.46525 t2=86.95712\n",
      "Epoch (critic) 1342 Loss=98.72249 t1=0.43373 t2=98.28876\n",
      "Epoch (critic) 1343 Loss=145.26175 t1=-0.77083 t2=146.03259\n",
      "Epoch (critic) 1344 Loss=38.78658 t1=2.66081 t2=36.12576\n",
      "Epoch (critic) 1345 Loss=29.32783 t1=1.09144 t2=28.23639\n",
      "Epoch (critic) 1346 Loss=22.17089 t1=0.13396 t2=22.03694\n",
      "Epoch (critic) 1347 Loss=32.45577 t1=1.96080 t2=30.49498\n",
      "Epoch (critic) 1348 Loss=28.49021 t1=0.93189 t2=27.55833\n",
      "Epoch (critic) 1349 Loss=69.97638 t1=0.46679 t2=69.50959\n",
      "Epoch (score) 1350 Loss=100.12495 t1=0.26854 t2=99.85641\n",
      "Epoch (score) 1351 Loss=89.69450 t1=0.19734 t2=89.49717\n",
      "Epoch (score) 1352 Loss=89.45185 t1=0.14188 t2=89.30997\n",
      "Epoch (score) 1353 Loss=63.68229 t1=0.39600 t2=63.28629\n",
      "Epoch (score) 1354 Loss=66.58479 t1=0.19576 t2=66.38904\n",
      "Epoch (score) 1355 Loss=103.07770 t1=0.21241 t2=102.86530\n",
      "Epoch (score) 1356 Loss=85.18946 t1=0.18914 t2=85.00032\n",
      "Epoch (score) 1357 Loss=82.53614 t1=0.13576 t2=82.40038\n",
      "Epoch (score) 1358 Loss=66.39944 t1=-0.15944 t2=66.55888\n",
      "Epoch (score) 1359 Loss=69.82523 t1=-0.04469 t2=69.86993\n",
      "Epoch (critic) 1360 Loss=52.61748 t1=-0.28104 t2=52.89853\n",
      "Epoch (critic) 1361 Loss=104.11835 t1=0.24142 t2=103.87691\n",
      "Epoch (critic) 1362 Loss=130.63206 t1=-1.13870 t2=131.77077\n",
      "Epoch (critic) 1363 Loss=85.21774 t1=0.71752 t2=84.50023\n",
      "Epoch (critic) 1364 Loss=166.71288 t1=1.38648 t2=165.32642\n",
      "Epoch (critic) 1365 Loss=159.86880 t1=-0.05689 t2=159.92570\n",
      "Epoch (critic) 1366 Loss=19.96867 t1=2.17541 t2=17.79326\n",
      "Epoch (critic) 1367 Loss=18.37003 t1=1.55951 t2=16.81051\n",
      "Epoch (critic) 1368 Loss=18.47960 t1=1.72309 t2=16.75651\n",
      "Epoch (critic) 1369 Loss=16.92978 t1=1.87550 t2=15.05428\n",
      "Epoch (score) 1370 Loss=18.82154 t1=1.68828 t2=17.13326\n",
      "Epoch (score) 1371 Loss=17.90245 t1=1.70366 t2=16.19879\n",
      "Epoch (score) 1372 Loss=17.01365 t1=1.56600 t2=15.44765\n",
      "Epoch (score) 1373 Loss=18.12651 t1=1.58349 t2=16.54302\n",
      "Epoch (score) 1374 Loss=16.47083 t1=1.47011 t2=15.00072\n",
      "Epoch (score) 1375 Loss=17.39967 t1=1.35908 t2=16.04059\n",
      "Epoch (score) 1376 Loss=17.56035 t1=1.26666 t2=16.29369\n",
      "Epoch (score) 1377 Loss=17.10654 t1=1.08496 t2=16.02158\n",
      "Epoch (score) 1378 Loss=16.85242 t1=0.95532 t2=15.89710\n",
      "Epoch (score) 1379 Loss=17.98588 t1=0.78406 t2=17.20181\n",
      "Epoch (critic) 1380 Loss=18.05135 t1=0.64412 t2=17.40722\n",
      "Epoch (critic) 1381 Loss=20.88282 t1=0.67244 t2=20.21038\n",
      "Epoch (critic) 1382 Loss=24.13568 t1=0.81233 t2=23.32334\n",
      "Epoch (critic) 1383 Loss=23.05947 t1=0.79026 t2=22.26921\n",
      "Epoch (critic) 1384 Loss=32.03518 t1=0.72982 t2=31.30536\n",
      "Epoch (critic) 1385 Loss=39.17731 t1=0.70020 t2=38.47711\n",
      "Epoch (critic) 1386 Loss=45.03574 t1=1.24920 t2=43.78654\n",
      "Epoch (critic) 1387 Loss=25.25375 t1=0.86644 t2=24.38731\n",
      "Epoch (critic) 1388 Loss=27.84688 t1=0.46244 t2=27.38444\n",
      "Epoch (critic) 1389 Loss=27.81486 t1=0.41150 t2=27.40335\n",
      "Epoch (score) 1390 Loss=28.28049 t1=0.42685 t2=27.85365\n",
      "Epoch (score) 1391 Loss=30.53352 t1=0.44836 t2=30.08516\n",
      "Epoch (score) 1392 Loss=28.62533 t1=0.42881 t2=28.19652\n",
      "Epoch (score) 1393 Loss=29.12091 t1=0.42180 t2=28.69910\n",
      "Epoch (score) 1394 Loss=30.97945 t1=0.40839 t2=30.57106\n",
      "Epoch (score) 1395 Loss=27.73634 t1=0.39173 t2=27.34460\n",
      "Epoch (score) 1396 Loss=29.49419 t1=0.33737 t2=29.15682\n",
      "Epoch (score) 1397 Loss=28.18554 t1=0.27645 t2=27.90908\n",
      "Epoch (score) 1398 Loss=29.38304 t1=0.23824 t2=29.14480\n",
      "Epoch (score) 1399 Loss=27.45521 t1=0.18840 t2=27.26681\n",
      "Epoch (critic) 1400 Loss=29.56170 t1=0.13634 t2=29.42536\n",
      "Epoch (critic) 1401 Loss=24.06656 t1=0.08243 t2=23.98414\n",
      "Epoch (critic) 1402 Loss=15.57699 t1=-0.06938 t2=15.64637\n",
      "Epoch (critic) 1403 Loss=15.41418 t1=-0.18356 t2=15.59774\n",
      "Epoch (critic) 1404 Loss=24.78579 t1=-0.15656 t2=24.94234\n",
      "Epoch (critic) 1405 Loss=24.26960 t1=-0.06318 t2=24.33278\n",
      "Epoch (critic) 1406 Loss=42.20755 t1=0.05434 t2=42.15321\n",
      "Epoch (critic) 1407 Loss=47.73888 t1=0.18158 t2=47.55730\n",
      "Epoch (critic) 1408 Loss=26.43527 t1=0.23763 t2=26.19764\n",
      "Epoch (critic) 1409 Loss=18.78909 t1=0.08726 t2=18.70183\n",
      "Epoch (score) 1410 Loss=16.59180 t1=-0.04571 t2=16.63751\n",
      "Epoch (score) 1411 Loss=11.27819 t1=-0.03372 t2=11.31191\n",
      "Epoch (score) 1412 Loss=18.07521 t1=-0.06597 t2=18.14118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 1413 Loss=14.71752 t1=-0.08507 t2=14.80259\n",
      "Epoch (score) 1414 Loss=16.17643 t1=-0.09777 t2=16.27420\n",
      "Epoch (score) 1415 Loss=20.11597 t1=-0.11914 t2=20.23512\n",
      "Epoch (score) 1416 Loss=16.51806 t1=-0.11352 t2=16.63157\n",
      "Epoch (score) 1417 Loss=18.36753 t1=-0.14068 t2=18.50821\n",
      "Epoch (score) 1418 Loss=13.70660 t1=-0.15540 t2=13.86201\n",
      "Epoch (score) 1419 Loss=8.95134 t1=-0.16825 t2=9.11958\n",
      "Epoch (critic) 1420 Loss=15.45474 t1=-0.22380 t2=15.67853\n",
      "Epoch (critic) 1421 Loss=16.85809 t1=-0.19153 t2=17.04962\n",
      "Epoch (critic) 1422 Loss=34.76453 t1=-0.16978 t2=34.93431\n",
      "Epoch (critic) 1423 Loss=43.29589 t1=-0.10346 t2=43.39935\n",
      "Epoch (critic) 1424 Loss=65.17311 t1=-0.05677 t2=65.22988\n",
      "Epoch (critic) 1425 Loss=71.08200 t1=0.02393 t2=71.05807\n",
      "Epoch (critic) 1426 Loss=40.49037 t1=0.02386 t2=40.46651\n",
      "Epoch (critic) 1427 Loss=56.72800 t1=-0.04037 t2=56.76837\n",
      "Epoch (critic) 1428 Loss=29.53892 t1=-0.02593 t2=29.56485\n",
      "Epoch (critic) 1429 Loss=19.76507 t1=-0.10539 t2=19.87046\n",
      "Epoch (score) 1430 Loss=15.00664 t1=-0.21766 t2=15.22430\n",
      "Epoch (score) 1431 Loss=16.60618 t1=-0.20965 t2=16.81583\n",
      "Epoch (score) 1432 Loss=15.79796 t1=-0.22890 t2=16.02687\n",
      "Epoch (score) 1433 Loss=15.87811 t1=-0.25599 t2=16.13410\n",
      "Epoch (score) 1434 Loss=14.51724 t1=-0.28346 t2=14.80070\n",
      "Epoch (score) 1435 Loss=20.98750 t1=-0.31275 t2=21.30025\n",
      "Epoch (score) 1436 Loss=17.77176 t1=-0.36887 t2=18.14063\n",
      "Epoch (score) 1437 Loss=21.13180 t1=-0.37200 t2=21.50380\n",
      "Epoch (score) 1438 Loss=13.56767 t1=-0.41851 t2=13.98618\n",
      "Epoch (score) 1439 Loss=17.55009 t1=-0.44602 t2=17.99612\n",
      "Epoch (critic) 1440 Loss=18.92830 t1=-0.47864 t2=19.40694\n",
      "Epoch (critic) 1441 Loss=27.94666 t1=-0.61860 t2=28.56526\n",
      "Epoch (critic) 1442 Loss=42.43349 t1=-0.48253 t2=42.91602\n",
      "Epoch (critic) 1443 Loss=73.39064 t1=-0.32422 t2=73.71485\n",
      "Epoch (critic) 1444 Loss=56.90088 t1=-0.15400 t2=57.05489\n",
      "Epoch (critic) 1445 Loss=84.16155 t1=0.14677 t2=84.01477\n",
      "Epoch (critic) 1446 Loss=61.87313 t1=0.40476 t2=61.46837\n",
      "Epoch (critic) 1447 Loss=51.91541 t1=0.30507 t2=51.61034\n",
      "Epoch (critic) 1448 Loss=64.66447 t1=0.04396 t2=64.62051\n",
      "Epoch (critic) 1449 Loss=96.45834 t1=-0.31291 t2=96.77125\n",
      "Epoch (score) 1450 Loss=118.15192 t1=-0.41416 t2=118.56607\n",
      "Epoch (score) 1451 Loss=100.68250 t1=-0.40786 t2=101.09036\n",
      "Epoch (score) 1452 Loss=100.45242 t1=-0.47788 t2=100.93030\n",
      "Epoch (score) 1453 Loss=102.70531 t1=-0.55320 t2=103.25851\n",
      "Epoch (score) 1454 Loss=105.77143 t1=-0.76144 t2=106.53288\n",
      "Epoch (score) 1455 Loss=115.35669 t1=-0.87405 t2=116.23073\n",
      "Epoch (score) 1456 Loss=100.32774 t1=-0.98987 t2=101.31761\n",
      "Epoch (score) 1457 Loss=108.50688 t1=-0.91332 t2=109.42020\n",
      "Epoch (score) 1458 Loss=93.59068 t1=-1.02485 t2=94.61552\n",
      "Epoch (score) 1459 Loss=104.96840 t1=-1.30962 t2=106.27802\n",
      "Epoch (critic) 1460 Loss=101.64001 t1=-1.46715 t2=103.10715\n",
      "Epoch (critic) 1461 Loss=79.96579 t1=-1.17821 t2=81.14400\n",
      "Epoch (critic) 1462 Loss=96.76512 t1=-0.99692 t2=97.76204\n",
      "Epoch (critic) 1463 Loss=121.82236 t1=-0.82034 t2=122.64270\n",
      "Epoch (critic) 1464 Loss=115.15976 t1=0.57380 t2=114.58595\n",
      "Epoch (critic) 1465 Loss=166.05003 t1=0.67764 t2=165.37238\n",
      "Epoch (critic) 1466 Loss=55.44293 t1=0.40462 t2=55.03830\n",
      "Epoch (critic) 1467 Loss=92.57536 t1=1.77697 t2=90.79838\n",
      "Epoch (critic) 1468 Loss=79.51312 t1=1.78507 t2=77.72805\n",
      "Epoch (critic) 1469 Loss=81.49290 t1=1.56322 t2=79.92969\n",
      "Epoch (score) 1470 Loss=77.41042 t1=1.54474 t2=75.86568\n",
      "Epoch (score) 1471 Loss=67.52065 t1=1.75008 t2=65.77055\n",
      "Epoch (score) 1472 Loss=76.95219 t1=1.52764 t2=75.42455\n",
      "Epoch (score) 1473 Loss=79.18229 t1=1.46988 t2=77.71240\n",
      "Epoch (score) 1474 Loss=75.58175 t1=1.54302 t2=74.03873\n",
      "Epoch (score) 1475 Loss=62.57434 t1=1.35907 t2=61.21528\n",
      "Epoch (score) 1476 Loss=78.47116 t1=0.99935 t2=77.47180\n",
      "Epoch (score) 1477 Loss=74.49541 t1=0.92076 t2=73.57465\n",
      "Epoch (score) 1478 Loss=74.88448 t1=0.59997 t2=74.28452\n",
      "Epoch (score) 1479 Loss=76.11428 t1=0.27410 t2=75.84019\n",
      "Epoch (critic) 1480 Loss=74.40944 t1=0.08660 t2=74.32284\n",
      "Epoch (critic) 1481 Loss=78.03899 t1=0.57266 t2=77.46632\n",
      "Epoch (critic) 1482 Loss=84.96785 t1=0.99514 t2=83.97270\n",
      "Epoch (critic) 1483 Loss=99.24869 t1=0.92860 t2=98.32008\n",
      "Epoch (critic) 1484 Loss=113.24538 t1=0.25011 t2=112.99527\n",
      "Epoch (critic) 1485 Loss=137.77007 t1=-0.25163 t2=138.02170\n",
      "Epoch (critic) 1486 Loss=162.31041 t1=-0.81061 t2=163.12100\n",
      "Epoch (critic) 1487 Loss=204.52873 t1=0.21525 t2=204.31349\n",
      "Epoch (critic) 1488 Loss=145.31775 t1=0.85754 t2=144.46022\n",
      "Epoch (critic) 1489 Loss=79.47864 t1=0.83962 t2=78.63902\n",
      "Epoch (score) 1490 Loss=79.63303 t1=0.99603 t2=78.63699\n",
      "Epoch (score) 1491 Loss=101.52349 t1=0.95437 t2=100.56911\n",
      "Epoch (score) 1492 Loss=90.11620 t1=0.94224 t2=89.17395\n",
      "Epoch (score) 1493 Loss=96.69154 t1=0.90323 t2=95.78832\n",
      "Epoch (score) 1494 Loss=90.96182 t1=0.92818 t2=90.03366\n",
      "Epoch (score) 1495 Loss=105.45110 t1=0.82959 t2=104.62151\n",
      "Epoch (score) 1496 Loss=115.05958 t1=0.79927 t2=114.26031\n",
      "Epoch (score) 1497 Loss=96.95494 t1=0.72243 t2=96.23251\n",
      "Epoch (score) 1498 Loss=90.54716 t1=0.67727 t2=89.86989\n",
      "Epoch (score) 1499 Loss=85.10194 t1=0.57910 t2=84.52286\n",
      "Epoch (critic) 1500 Loss=115.67204 t1=0.33497 t2=115.33705\n",
      "Epoch (critic) 1501 Loss=81.69481 t1=-0.20709 t2=81.90189\n",
      "Epoch (critic) 1502 Loss=97.26810 t1=-0.19765 t2=97.46575\n",
      "Epoch (critic) 1503 Loss=86.78693 t1=-0.27533 t2=87.06226\n",
      "Epoch (critic) 1504 Loss=109.13831 t1=-0.41403 t2=109.55235\n",
      "Epoch (critic) 1505 Loss=166.21011 t1=-0.50746 t2=166.71759\n",
      "Epoch (critic) 1506 Loss=129.01944 t1=-0.36555 t2=129.38498\n",
      "Epoch (critic) 1507 Loss=190.21454 t1=-0.34092 t2=190.55545\n",
      "Epoch (critic) 1508 Loss=141.01816 t1=0.85259 t2=140.16557\n",
      "Epoch (critic) 1509 Loss=87.08571 t1=0.56760 t2=86.51810\n",
      "Epoch (score) 1510 Loss=140.02873 t1=0.45882 t2=139.56992\n",
      "Epoch (score) 1511 Loss=150.22855 t1=0.56729 t2=149.66124\n",
      "Epoch (score) 1512 Loss=126.68469 t1=0.43992 t2=126.24476\n",
      "Epoch (score) 1513 Loss=165.75912 t1=0.52655 t2=165.23256\n",
      "Epoch (score) 1514 Loss=116.20898 t1=0.58514 t2=115.62381\n",
      "Epoch (score) 1515 Loss=144.61414 t1=0.16112 t2=144.45300\n",
      "Epoch (score) 1516 Loss=135.82994 t1=0.10535 t2=135.72461\n",
      "Epoch (score) 1517 Loss=131.92853 t1=0.22424 t2=131.70428\n",
      "Epoch (score) 1518 Loss=119.08659 t1=-0.04382 t2=119.13040\n",
      "Epoch (score) 1519 Loss=130.92569 t1=0.04520 t2=130.88049\n",
      "Epoch (critic) 1520 Loss=163.90329 t1=-0.47308 t2=164.37637\n",
      "Epoch (critic) 1521 Loss=129.80070 t1=0.04842 t2=129.75227\n",
      "Epoch (critic) 1522 Loss=130.43094 t1=0.16064 t2=130.27029\n",
      "Epoch (critic) 1523 Loss=120.45229 t1=-0.49110 t2=120.94339\n",
      "Epoch (critic) 1524 Loss=107.51511 t1=-0.11099 t2=107.62610\n",
      "Epoch (critic) 1525 Loss=106.69054 t1=0.21616 t2=106.47437\n",
      "Epoch (critic) 1526 Loss=165.87383 t1=-0.13613 t2=166.00995\n",
      "Epoch (critic) 1527 Loss=148.48769 t1=-0.50149 t2=148.98918\n",
      "Epoch (critic) 1528 Loss=183.12679 t1=-0.57578 t2=183.70258\n",
      "Epoch (critic) 1529 Loss=229.75101 t1=0.01665 t2=229.73436\n",
      "Epoch (score) 1530 Loss=238.17126 t1=-0.78382 t2=238.95509\n",
      "Epoch (score) 1531 Loss=312.82547 t1=-0.67334 t2=313.49878\n",
      "Epoch (score) 1532 Loss=236.89024 t1=-0.94254 t2=237.83278\n",
      "Epoch (score) 1533 Loss=223.13104 t1=-1.48608 t2=224.61713\n",
      "Epoch (score) 1534 Loss=215.95300 t1=-1.40136 t2=217.35437\n",
      "Epoch (score) 1535 Loss=269.65292 t1=-1.56178 t2=271.21472\n",
      "Epoch (score) 1536 Loss=205.09207 t1=-2.60484 t2=207.69690\n",
      "Epoch (score) 1537 Loss=175.05705 t1=-2.65844 t2=177.71550\n",
      "Epoch (score) 1538 Loss=147.00958 t1=-3.70680 t2=150.71637\n",
      "Epoch (score) 1539 Loss=243.56775 t1=-3.14814 t2=246.71590\n",
      "Epoch (critic) 1540 Loss=64.61351 t1=-5.73959 t2=70.35311\n",
      "Epoch (critic) 1541 Loss=246.19385 t1=0.24712 t2=245.94673\n",
      "Epoch (critic) 1542 Loss=303.97626 t1=-1.31890 t2=305.29514\n",
      "Epoch (critic) 1543 Loss=233.61929 t1=-4.70062 t2=238.31992\n",
      "Epoch (critic) 1544 Loss=158.59100 t1=2.71478 t2=155.87622\n",
      "Epoch (critic) 1545 Loss=197.59123 t1=-0.23443 t2=197.82565\n",
      "Epoch (critic) 1546 Loss=167.95598 t1=-1.38148 t2=169.33748\n",
      "Epoch (critic) 1547 Loss=141.04747 t1=-1.68462 t2=142.73209\n",
      "Epoch (critic) 1548 Loss=172.13695 t1=-1.74160 t2=173.87854\n",
      "Epoch (critic) 1549 Loss=139.44870 t1=0.19498 t2=139.25372\n",
      "Epoch (score) 1550 Loss=72.30968 t1=0.31581 t2=71.99386\n",
      "Epoch (score) 1551 Loss=109.90273 t1=-0.18935 t2=110.09207\n",
      "Epoch (score) 1552 Loss=89.90295 t1=-0.43685 t2=90.33980\n",
      "Epoch (score) 1553 Loss=176.38579 t1=-1.55885 t2=177.94464\n",
      "Epoch (score) 1554 Loss=153.65680 t1=-1.25614 t2=154.91295\n",
      "Epoch (score) 1555 Loss=138.53606 t1=-1.24671 t2=139.78278\n",
      "Epoch (score) 1556 Loss=157.52182 t1=-1.86165 t2=159.38345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 1557 Loss=165.17880 t1=-2.57421 t2=167.75302\n",
      "Epoch (score) 1558 Loss=205.95230 t1=-3.54809 t2=209.50040\n",
      "Epoch (score) 1559 Loss=121.12344 t1=-1.91115 t2=123.03460\n",
      "Epoch (critic) 1560 Loss=96.18275 t1=-1.04755 t2=97.23028\n",
      "Epoch (critic) 1561 Loss=132.73827 t1=-4.04684 t2=136.78510\n",
      "Epoch (critic) 1562 Loss=128.77968 t1=-4.84303 t2=133.62271\n",
      "Epoch (critic) 1563 Loss=128.61076 t1=-1.90468 t2=130.51544\n",
      "Epoch (critic) 1564 Loss=159.62968 t1=-1.94510 t2=161.57478\n",
      "Epoch (critic) 1565 Loss=179.69327 t1=-1.13822 t2=180.83150\n",
      "Epoch (critic) 1566 Loss=179.11792 t1=-0.37126 t2=179.48920\n",
      "Epoch (critic) 1567 Loss=207.10498 t1=-4.43299 t2=211.53796\n",
      "Epoch (critic) 1568 Loss=287.36755 t1=3.27529 t2=284.09229\n",
      "Epoch (critic) 1569 Loss=294.99374 t1=-2.87085 t2=297.86456\n",
      "Epoch (score) 1570 Loss=470.64569 t1=4.49639 t2=466.14935\n",
      "Epoch (score) 1571 Loss=385.32861 t1=6.00304 t2=379.32550\n",
      "Epoch (score) 1572 Loss=379.28604 t1=4.60784 t2=374.67816\n",
      "Epoch (score) 1573 Loss=225.83743 t1=9.73740 t2=216.10004\n",
      "Epoch (score) 1574 Loss=408.05856 t1=2.82072 t2=405.23785\n",
      "Epoch (score) 1575 Loss=439.84036 t1=3.61405 t2=436.22632\n",
      "Epoch (score) 1576 Loss=450.39157 t1=0.43853 t2=449.95309\n",
      "Epoch (score) 1577 Loss=462.59808 t1=0.25855 t2=462.33954\n",
      "Epoch (score) 1578 Loss=343.21899 t1=4.20772 t2=339.01126\n",
      "Epoch (score) 1579 Loss=409.11334 t1=0.57299 t2=408.54037\n",
      "Epoch (critic) 1580 Loss=398.85364 t1=2.07683 t2=396.77676\n",
      "Epoch (critic) 1581 Loss=257.97775 t1=-7.41425 t2=265.39200\n",
      "Epoch (critic) 1582 Loss=65.63988 t1=-4.53791 t2=70.17780\n",
      "Epoch (critic) 1583 Loss=48.25718 t1=10.28550 t2=37.97168\n",
      "Epoch (critic) 1584 Loss=71.64444 t1=7.90207 t2=63.74237\n",
      "Epoch (critic) 1585 Loss=53.40231 t1=5.36420 t2=48.03811\n",
      "Epoch (critic) 1586 Loss=54.77905 t1=3.91215 t2=50.86690\n",
      "Epoch (critic) 1587 Loss=86.71552 t1=2.31818 t2=84.39734\n",
      "Epoch (critic) 1588 Loss=144.10400 t1=0.42864 t2=143.67535\n",
      "Epoch (critic) 1589 Loss=-7.45380 t1=-7.05666 t2=-0.39714\n",
      "Epoch (score) 1590 Loss=-3.74498 t1=-3.82602 t2=0.08104\n",
      "Epoch (score) 1591 Loss=-5.30926 t1=-3.98092 t2=-1.32834\n",
      "Epoch (score) 1592 Loss=-5.27117 t1=-4.06100 t2=-1.21017\n",
      "Epoch (score) 1593 Loss=-5.87811 t1=-4.33724 t2=-1.54088\n",
      "Epoch (score) 1594 Loss=-4.84261 t1=-4.67152 t2=-0.17109\n",
      "Epoch (score) 1595 Loss=-5.83316 t1=-5.16270 t2=-0.67046\n",
      "Epoch (score) 1596 Loss=-5.93342 t1=-5.85323 t2=-0.08019\n",
      "Epoch (score) 1597 Loss=-6.33920 t1=-6.57656 t2=0.23735\n",
      "Epoch (score) 1598 Loss=-6.10625 t1=-7.13362 t2=1.02737\n",
      "Epoch (score) 1599 Loss=-10.29981 t1=-8.23967 t2=-2.06015\n",
      "Epoch (critic) 1600 Loss=-10.67714 t1=-9.22466 t2=-1.45248\n",
      "Epoch (critic) 1601 Loss=-1.49383 t1=-2.05468 t2=0.56085\n",
      "Epoch (critic) 1602 Loss=5.62720 t1=2.90634 t2=2.72085\n",
      "Epoch (critic) 1603 Loss=9.07449 t1=5.87657 t2=3.19793\n",
      "Epoch (critic) 1604 Loss=11.84581 t1=7.34920 t2=4.49661\n",
      "Epoch (critic) 1605 Loss=12.41962 t1=7.74368 t2=4.67594\n",
      "Epoch (critic) 1606 Loss=12.10512 t1=7.22075 t2=4.88437\n",
      "Epoch (critic) 1607 Loss=12.55549 t1=6.62693 t2=5.92856\n",
      "Epoch (critic) 1608 Loss=10.05819 t1=5.65637 t2=4.40182\n",
      "Epoch (critic) 1609 Loss=10.15148 t1=5.02438 t2=5.12710\n",
      "Epoch (score) 1610 Loss=8.00309 t1=4.24150 t2=3.76159\n",
      "Epoch (score) 1611 Loss=9.42217 t1=4.30642 t2=5.11575\n",
      "Epoch (score) 1612 Loss=9.29176 t1=4.54360 t2=4.74816\n",
      "Epoch (score) 1613 Loss=9.30637 t1=4.54237 t2=4.76400\n",
      "Epoch (score) 1614 Loss=9.86311 t1=4.40126 t2=5.46185\n",
      "Epoch (score) 1615 Loss=8.07543 t1=4.36149 t2=3.71394\n",
      "Epoch (score) 1616 Loss=9.39293 t1=4.57627 t2=4.81666\n",
      "Epoch (score) 1617 Loss=9.74594 t1=4.34342 t2=5.40252\n",
      "Epoch (score) 1618 Loss=9.57467 t1=4.11005 t2=5.46462\n",
      "Epoch (score) 1619 Loss=9.60415 t1=4.00860 t2=5.59555\n",
      "Epoch (critic) 1620 Loss=8.88937 t1=3.61891 t2=5.27046\n",
      "Epoch (critic) 1621 Loss=8.97612 t1=2.89061 t2=6.08550\n",
      "Epoch (critic) 1622 Loss=8.76535 t1=1.99403 t2=6.77131\n",
      "Epoch (critic) 1623 Loss=8.71358 t1=1.59611 t2=7.11747\n",
      "Epoch (critic) 1624 Loss=9.00669 t1=1.11703 t2=7.88965\n",
      "Epoch (critic) 1625 Loss=11.97812 t1=1.00098 t2=10.97714\n",
      "Epoch (critic) 1626 Loss=11.54880 t1=0.64045 t2=10.90835\n",
      "Epoch (critic) 1627 Loss=14.34939 t1=1.53801 t2=12.81138\n",
      "Epoch (critic) 1628 Loss=17.59318 t1=1.90145 t2=15.69173\n",
      "Epoch (critic) 1629 Loss=20.18591 t1=3.08033 t2=17.10557\n",
      "Epoch (score) 1630 Loss=21.84628 t1=3.64628 t2=18.20000\n",
      "Epoch (score) 1631 Loss=22.10389 t1=3.39248 t2=18.71141\n",
      "Epoch (score) 1632 Loss=23.09321 t1=3.38604 t2=19.70717\n",
      "Epoch (score) 1633 Loss=20.78666 t1=2.84126 t2=17.94540\n",
      "Epoch (score) 1634 Loss=19.06837 t1=2.66611 t2=16.40226\n",
      "Epoch (score) 1635 Loss=22.39482 t1=2.77781 t2=19.61701\n",
      "Epoch (score) 1636 Loss=21.23564 t1=2.34625 t2=18.88939\n",
      "Epoch (score) 1637 Loss=19.93379 t1=2.25017 t2=17.68362\n",
      "Epoch (score) 1638 Loss=21.00248 t1=2.12037 t2=18.88211\n",
      "Epoch (score) 1639 Loss=19.67083 t1=1.73330 t2=17.93753\n",
      "Epoch (critic) 1640 Loss=21.77745 t1=1.97960 t2=19.79785\n",
      "Epoch (critic) 1641 Loss=23.77435 t1=1.69948 t2=22.07486\n",
      "Epoch (critic) 1642 Loss=22.54462 t1=1.18192 t2=21.36271\n",
      "Epoch (critic) 1643 Loss=25.30537 t1=0.91742 t2=24.38795\n",
      "Epoch (critic) 1644 Loss=31.36258 t1=1.12884 t2=30.23374\n",
      "Epoch (critic) 1645 Loss=32.15279 t1=1.21761 t2=30.93518\n",
      "Epoch (critic) 1646 Loss=45.87698 t1=1.76364 t2=44.11334\n",
      "Epoch (critic) 1647 Loss=54.07469 t1=2.60581 t2=51.46888\n",
      "Epoch (critic) 1648 Loss=45.33522 t1=1.94423 t2=43.39098\n",
      "Epoch (critic) 1649 Loss=55.00018 t1=2.09976 t2=52.90042\n",
      "Epoch (score) 1650 Loss=85.66617 t1=4.29504 t2=81.37112\n",
      "Epoch (score) 1651 Loss=88.72177 t1=4.32349 t2=84.39828\n",
      "Epoch (score) 1652 Loss=85.57925 t1=3.94564 t2=81.63361\n",
      "Epoch (score) 1653 Loss=71.89369 t1=3.66113 t2=68.23257\n",
      "Epoch (score) 1654 Loss=74.98531 t1=3.35650 t2=71.62881\n",
      "Epoch (score) 1655 Loss=71.48510 t1=3.16432 t2=68.32079\n",
      "Epoch (score) 1656 Loss=74.10942 t1=2.78249 t2=71.32693\n",
      "Epoch (score) 1657 Loss=85.04585 t1=2.48433 t2=82.56152\n",
      "Epoch (score) 1658 Loss=62.94285 t1=2.24657 t2=60.69629\n",
      "Epoch (score) 1659 Loss=69.97850 t1=1.92374 t2=68.05477\n",
      "Epoch (critic) 1660 Loss=72.86389 t1=1.75024 t2=71.11365\n",
      "Epoch (critic) 1661 Loss=54.26139 t1=0.81378 t2=53.44762\n",
      "Epoch (critic) 1662 Loss=75.04723 t1=0.84360 t2=74.20363\n",
      "Epoch (critic) 1663 Loss=53.35502 t1=2.07376 t2=51.28127\n",
      "Epoch (critic) 1664 Loss=56.02893 t1=0.14393 t2=55.88500\n",
      "Epoch (critic) 1665 Loss=43.56836 t1=-0.18434 t2=43.75270\n",
      "Epoch (critic) 1666 Loss=43.37310 t1=-0.23120 t2=43.60430\n",
      "Epoch (critic) 1667 Loss=42.26252 t1=-0.02609 t2=42.28860\n",
      "Epoch (critic) 1668 Loss=38.93771 t1=0.05073 t2=38.88698\n",
      "Epoch (critic) 1669 Loss=33.13228 t1=0.18768 t2=32.94460\n",
      "Epoch (score) 1670 Loss=48.75430 t1=0.09556 t2=48.65874\n",
      "Epoch (score) 1671 Loss=47.75690 t1=0.00717 t2=47.74973\n",
      "Epoch (score) 1672 Loss=51.53205 t1=0.05954 t2=51.47252\n",
      "Epoch (score) 1673 Loss=49.33294 t1=0.09794 t2=49.23501\n",
      "Epoch (score) 1674 Loss=47.47717 t1=0.05620 t2=47.42096\n",
      "Epoch (score) 1675 Loss=48.96611 t1=0.00865 t2=48.95746\n",
      "Epoch (score) 1676 Loss=48.06396 t1=-0.03189 t2=48.09585\n",
      "Epoch (score) 1677 Loss=48.36721 t1=-0.08606 t2=48.45327\n",
      "Epoch (score) 1678 Loss=47.48248 t1=-0.13893 t2=47.62141\n",
      "Epoch (score) 1679 Loss=46.62139 t1=-0.16008 t2=46.78147\n",
      "Epoch (critic) 1680 Loss=48.85869 t1=-0.26718 t2=49.12587\n",
      "Epoch (critic) 1681 Loss=75.07961 t1=-0.45571 t2=75.53532\n",
      "Epoch (critic) 1682 Loss=62.18544 t1=-0.44539 t2=62.63082\n",
      "Epoch (critic) 1683 Loss=64.40340 t1=-0.03323 t2=64.43663\n",
      "Epoch (critic) 1684 Loss=72.27181 t1=-0.47270 t2=72.74451\n",
      "Epoch (critic) 1685 Loss=47.73905 t1=0.20245 t2=47.53661\n",
      "Epoch (critic) 1686 Loss=46.80501 t1=-0.07132 t2=46.87633\n",
      "Epoch (critic) 1687 Loss=56.11690 t1=-0.16853 t2=56.28542\n",
      "Epoch (critic) 1688 Loss=65.32950 t1=0.17355 t2=65.15595\n",
      "Epoch (critic) 1689 Loss=79.95232 t1=0.21780 t2=79.73453\n",
      "Epoch (score) 1690 Loss=92.65416 t1=-0.44238 t2=93.09653\n",
      "Epoch (score) 1691 Loss=87.22227 t1=-0.44030 t2=87.66257\n",
      "Epoch (score) 1692 Loss=85.76199 t1=-0.49000 t2=86.25199\n",
      "Epoch (score) 1693 Loss=103.27800 t1=-0.47126 t2=103.74925\n",
      "Epoch (score) 1694 Loss=91.21754 t1=-0.61913 t2=91.83666\n",
      "Epoch (score) 1695 Loss=78.67178 t1=-0.65229 t2=79.32407\n",
      "Epoch (score) 1696 Loss=94.77647 t1=-0.73524 t2=95.51170\n",
      "Epoch (score) 1697 Loss=87.63239 t1=-0.94577 t2=88.57816\n",
      "Epoch (score) 1698 Loss=86.04535 t1=-1.00416 t2=87.04950\n",
      "Epoch (score) 1699 Loss=87.77976 t1=-1.17198 t2=88.95174\n",
      "Epoch (critic) 1700 Loss=92.42017 t1=-1.32016 t2=93.74033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 1701 Loss=39.14417 t1=1.76148 t2=37.38269\n",
      "Epoch (critic) 1702 Loss=29.70290 t1=1.96824 t2=27.73466\n",
      "Epoch (critic) 1703 Loss=35.30958 t1=1.77755 t2=33.53203\n",
      "Epoch (critic) 1704 Loss=37.15858 t1=1.52357 t2=35.63502\n",
      "Epoch (critic) 1705 Loss=38.54643 t1=1.30741 t2=37.23902\n",
      "Epoch (critic) 1706 Loss=37.92504 t1=1.15505 t2=36.76999\n",
      "Epoch (critic) 1707 Loss=43.03540 t1=0.94561 t2=42.08979\n",
      "Epoch (critic) 1708 Loss=40.85735 t1=0.62298 t2=40.23437\n",
      "Epoch (critic) 1709 Loss=48.00927 t1=0.17464 t2=47.83463\n",
      "Epoch (score) 1710 Loss=58.50204 t1=-0.28315 t2=58.78519\n",
      "Epoch (score) 1711 Loss=53.09857 t1=-0.26298 t2=53.36155\n",
      "Epoch (score) 1712 Loss=56.70308 t1=-0.29070 t2=56.99378\n",
      "Epoch (score) 1713 Loss=58.21258 t1=-0.35852 t2=58.57111\n",
      "Epoch (score) 1714 Loss=57.01678 t1=-0.40212 t2=57.41891\n",
      "Epoch (score) 1715 Loss=48.00467 t1=-0.42269 t2=48.42735\n",
      "Epoch (score) 1716 Loss=59.11109 t1=-0.54871 t2=59.65980\n",
      "Epoch (score) 1717 Loss=56.30276 t1=-0.63237 t2=56.93513\n",
      "Epoch (score) 1718 Loss=46.42770 t1=-0.56573 t2=46.99343\n",
      "Epoch (score) 1719 Loss=52.03690 t1=-0.80019 t2=52.83710\n",
      "Epoch (critic) 1720 Loss=58.36061 t1=-0.98655 t2=59.34716\n",
      "Epoch (critic) 1721 Loss=79.68102 t1=-2.18930 t2=81.87032\n",
      "Epoch (critic) 1722 Loss=101.01588 t1=-3.32776 t2=104.34364\n",
      "Epoch (critic) 1723 Loss=42.22800 t1=-1.12852 t2=43.35652\n",
      "Epoch (critic) 1724 Loss=64.47333 t1=-1.63544 t2=66.10876\n",
      "Epoch (critic) 1725 Loss=60.85473 t1=-3.23486 t2=64.08958\n",
      "Epoch (critic) 1726 Loss=140.83115 t1=-2.30988 t2=143.14102\n",
      "Epoch (critic) 1727 Loss=152.36835 t1=-2.32328 t2=154.69162\n",
      "Epoch (critic) 1728 Loss=103.32357 t1=1.38634 t2=101.93723\n",
      "Epoch (critic) 1729 Loss=63.02136 t1=-0.66609 t2=63.68745\n",
      "Epoch (score) 1730 Loss=-10.90470 t1=2.83840 t2=-13.74309\n",
      "Epoch (score) 1731 Loss=-6.00108 t1=2.91705 t2=-8.91813\n",
      "Epoch (score) 1732 Loss=-8.52706 t1=2.95225 t2=-11.47931\n",
      "Epoch (score) 1733 Loss=-10.48366 t1=2.93802 t2=-13.42168\n",
      "Epoch (score) 1734 Loss=-6.32873 t1=2.92440 t2=-9.25314\n",
      "Epoch (score) 1735 Loss=-9.23182 t1=2.73657 t2=-11.96839\n",
      "Epoch (score) 1736 Loss=-10.12140 t1=2.68642 t2=-12.80782\n",
      "Epoch (score) 1737 Loss=-7.47927 t1=2.47982 t2=-9.95909\n",
      "Epoch (score) 1738 Loss=-9.93849 t1=2.30673 t2=-12.24521\n",
      "Epoch (score) 1739 Loss=-10.64009 t1=2.12071 t2=-12.76080\n",
      "Epoch (critic) 1740 Loss=-11.63893 t1=1.98531 t2=-13.62424\n",
      "Epoch (critic) 1741 Loss=-1.58629 t1=1.97858 t2=-3.56487\n",
      "Epoch (critic) 1742 Loss=0.66776 t1=1.75315 t2=-1.08540\n",
      "Epoch (critic) 1743 Loss=1.54462 t1=1.50361 t2=0.04101\n",
      "Epoch (critic) 1744 Loss=3.76721 t1=1.27648 t2=2.49072\n",
      "Epoch (critic) 1745 Loss=-0.23713 t1=1.01118 t2=-1.24831\n",
      "Epoch (critic) 1746 Loss=5.32877 t1=0.86481 t2=4.46397\n",
      "Epoch (critic) 1747 Loss=4.28628 t1=0.85402 t2=3.43226\n",
      "Epoch (critic) 1748 Loss=5.70380 t1=0.82096 t2=4.88283\n",
      "Epoch (critic) 1749 Loss=8.40430 t1=0.96810 t2=7.43620\n",
      "Epoch (score) 1750 Loss=15.52231 t1=1.05791 t2=14.46440\n",
      "Epoch (score) 1751 Loss=10.06543 t1=0.99853 t2=9.06690\n",
      "Epoch (score) 1752 Loss=13.68238 t1=0.93400 t2=12.74837\n",
      "Epoch (score) 1753 Loss=13.67975 t1=0.79960 t2=12.88015\n",
      "Epoch (score) 1754 Loss=10.91864 t1=0.66220 t2=10.25644\n",
      "Epoch (score) 1755 Loss=16.81121 t1=0.57560 t2=16.23560\n",
      "Epoch (score) 1756 Loss=13.60091 t1=0.43861 t2=13.16230\n",
      "Epoch (score) 1757 Loss=11.97378 t1=0.25564 t2=11.71814\n",
      "Epoch (score) 1758 Loss=13.28421 t1=0.12907 t2=13.15514\n",
      "Epoch (score) 1759 Loss=11.54742 t1=-0.03068 t2=11.57810\n",
      "Epoch (critic) 1760 Loss=10.36241 t1=-0.17293 t2=10.53534\n",
      "Epoch (critic) 1761 Loss=13.65462 t1=-0.10300 t2=13.75763\n",
      "Epoch (critic) 1762 Loss=22.15480 t1=0.01054 t2=22.14426\n",
      "Epoch (critic) 1763 Loss=18.45700 t1=0.07355 t2=18.38346\n",
      "Epoch (critic) 1764 Loss=28.83478 t1=0.12777 t2=28.70701\n",
      "Epoch (critic) 1765 Loss=41.47262 t1=0.12459 t2=41.34803\n",
      "Epoch (critic) 1766 Loss=57.05296 t1=0.07998 t2=56.97298\n",
      "Epoch (critic) 1767 Loss=68.57927 t1=0.06099 t2=68.51828\n",
      "Epoch (critic) 1768 Loss=104.32346 t1=0.03111 t2=104.29234\n",
      "Epoch (critic) 1769 Loss=131.23322 t1=0.11935 t2=131.11388\n",
      "Epoch (score) 1770 Loss=172.76277 t1=0.10213 t2=172.66064\n",
      "Epoch (score) 1771 Loss=216.43579 t1=-0.01023 t2=216.44601\n",
      "Epoch (score) 1772 Loss=173.12228 t1=0.15248 t2=172.96979\n",
      "Epoch (score) 1773 Loss=174.39670 t1=0.17735 t2=174.21933\n",
      "Epoch (score) 1774 Loss=204.14090 t1=0.11588 t2=204.02504\n",
      "Epoch (score) 1775 Loss=164.72787 t1=0.19579 t2=164.53210\n",
      "Epoch (score) 1776 Loss=141.29836 t1=0.23833 t2=141.06003\n",
      "Epoch (score) 1777 Loss=191.51572 t1=0.17611 t2=191.33960\n",
      "Epoch (score) 1778 Loss=141.41809 t1=0.25970 t2=141.15839\n",
      "Epoch (score) 1779 Loss=155.14821 t1=0.23690 t2=154.91129\n",
      "Epoch (critic) 1780 Loss=182.11378 t1=0.20383 t2=181.90996\n",
      "Epoch (critic) 1781 Loss=228.10027 t1=0.07094 t2=228.02934\n",
      "Epoch (critic) 1782 Loss=318.48386 t1=-0.32202 t2=318.80585\n",
      "Epoch (critic) 1783 Loss=227.04317 t1=0.19021 t2=226.85297\n",
      "Epoch (critic) 1784 Loss=236.33743 t1=0.19180 t2=236.14565\n",
      "Epoch (critic) 1785 Loss=300.39087 t1=0.01674 t2=300.37415\n",
      "Epoch (critic) 1786 Loss=314.77621 t1=-0.25988 t2=315.03607\n",
      "Epoch (critic) 1787 Loss=373.32104 t1=0.12918 t2=373.19183\n",
      "Epoch (critic) 1788 Loss=208.09207 t1=0.87164 t2=207.22043\n",
      "Epoch (critic) 1789 Loss=428.54504 t1=0.81613 t2=427.72891\n",
      "Epoch (score) 1790 Loss=277.88052 t1=0.57332 t2=277.30719\n",
      "Epoch (score) 1791 Loss=63.04167 t1=0.45080 t2=62.59087\n",
      "Epoch (score) 1792 Loss=39.59811 t1=0.13292 t2=39.46518\n",
      "Epoch (score) 1793 Loss=117.02734 t1=-0.25233 t2=117.27968\n",
      "Epoch (score) 1794 Loss=120.74514 t1=-0.70938 t2=121.45451\n",
      "Epoch (score) 1795 Loss=230.57716 t1=-1.07694 t2=231.65410\n",
      "Epoch (score) 1796 Loss=35.50421 t1=-1.82723 t2=37.33144\n",
      "Epoch (score) 1797 Loss=39.72237 t1=-2.46228 t2=42.18465\n",
      "Epoch (score) 1798 Loss=67.51189 t1=-3.24349 t2=70.75539\n",
      "Epoch (score) 1799 Loss=120.55156 t1=-4.18282 t2=124.73439\n",
      "Epoch (critic) 1800 Loss=47.53332 t1=-5.67021 t2=53.20354\n",
      "Epoch (critic) 1801 Loss=33.18897 t1=-1.22550 t2=34.41447\n",
      "Epoch (critic) 1802 Loss=33.01382 t1=0.59533 t2=32.41849\n",
      "Epoch (critic) 1803 Loss=30.17397 t1=1.41582 t2=28.75815\n",
      "Epoch (critic) 1804 Loss=31.11753 t1=1.58867 t2=29.52886\n",
      "Epoch (critic) 1805 Loss=33.53593 t1=1.42325 t2=32.11268\n",
      "Epoch (critic) 1806 Loss=33.78211 t1=1.08372 t2=32.69839\n",
      "Epoch (critic) 1807 Loss=38.04296 t1=0.71108 t2=37.33189\n",
      "Epoch (critic) 1808 Loss=39.06661 t1=0.34112 t2=38.72549\n",
      "Epoch (critic) 1809 Loss=45.84707 t1=0.04450 t2=45.80256\n",
      "Epoch (score) 1810 Loss=52.75230 t1=-0.06302 t2=52.81532\n",
      "Epoch (score) 1811 Loss=55.24425 t1=-0.09226 t2=55.33650\n",
      "Epoch (score) 1812 Loss=59.36408 t1=-0.15180 t2=59.51587\n",
      "Epoch (score) 1813 Loss=54.84418 t1=-0.22240 t2=55.06658\n",
      "Epoch (score) 1814 Loss=57.02841 t1=-0.36242 t2=57.39083\n",
      "Epoch (score) 1815 Loss=55.78697 t1=-0.31159 t2=56.09856\n",
      "Epoch (score) 1816 Loss=55.27962 t1=-0.52208 t2=55.80170\n",
      "Epoch (score) 1817 Loss=55.23846 t1=-0.74438 t2=55.98284\n",
      "Epoch (score) 1818 Loss=51.68807 t1=-0.74920 t2=52.43726\n",
      "Epoch (score) 1819 Loss=52.35955 t1=-1.03232 t2=53.39188\n",
      "Epoch (critic) 1820 Loss=53.53558 t1=-1.14298 t2=54.67856\n",
      "Epoch (critic) 1821 Loss=67.57930 t1=-0.76722 t2=68.34652\n",
      "Epoch (critic) 1822 Loss=92.19402 t1=0.40493 t2=91.78909\n",
      "Epoch (critic) 1823 Loss=93.37300 t1=1.78938 t2=91.58362\n",
      "Epoch (critic) 1824 Loss=133.11148 t1=1.11711 t2=131.99437\n",
      "Epoch (critic) 1825 Loss=171.90442 t1=1.48087 t2=170.42355\n",
      "Epoch (critic) 1826 Loss=124.93502 t1=3.85571 t2=121.07930\n",
      "Epoch (critic) 1827 Loss=157.44589 t1=-2.20454 t2=159.65042\n",
      "Epoch (critic) 1828 Loss=216.98759 t1=2.22070 t2=214.76691\n",
      "Epoch (critic) 1829 Loss=76.70397 t1=5.52086 t2=71.18311\n",
      "Epoch (score) 1830 Loss=144.79051 t1=-2.48869 t2=147.27921\n",
      "Epoch (score) 1831 Loss=157.94229 t1=-3.79770 t2=161.73999\n",
      "Epoch (score) 1832 Loss=146.17906 t1=-4.49298 t2=150.67204\n",
      "Epoch (score) 1833 Loss=124.24001 t1=-5.12117 t2=129.36118\n",
      "Epoch (score) 1834 Loss=138.16309 t1=-7.65922 t2=145.82231\n",
      "Epoch (score) 1835 Loss=145.35512 t1=-8.80372 t2=154.15884\n",
      "Epoch (score) 1836 Loss=143.16377 t1=-10.14905 t2=153.31281\n",
      "Epoch (score) 1837 Loss=116.11741 t1=-12.14368 t2=128.26109\n",
      "Epoch (score) 1838 Loss=138.29507 t1=-14.26690 t2=152.56197\n",
      "Epoch (score) 1839 Loss=149.55901 t1=-16.89477 t2=166.45377\n",
      "Epoch (critic) 1840 Loss=135.23871 t1=-19.91861 t2=155.15732\n",
      "Epoch (critic) 1841 Loss=128.57906 t1=-19.64447 t2=148.22353\n",
      "Epoch (critic) 1842 Loss=161.97057 t1=-3.55582 t2=165.52637\n",
      "Epoch (critic) 1843 Loss=197.24313 t1=1.57491 t2=195.66823\n",
      "Epoch (critic) 1844 Loss=276.77032 t1=4.54167 t2=272.22864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (critic) 1845 Loss=138.63573 t1=33.95533 t2=104.68039\n",
      "Epoch (critic) 1846 Loss=175.56635 t1=2.56051 t2=173.00583\n",
      "Epoch (critic) 1847 Loss=260.09039 t1=9.30434 t2=250.78604\n",
      "Epoch (critic) 1848 Loss=374.25177 t1=20.70671 t2=353.54510\n",
      "Epoch (critic) 1849 Loss=64.05381 t1=-0.25047 t2=64.30428\n",
      "Epoch (score) 1850 Loss=255.77942 t1=-0.02269 t2=255.80211\n",
      "Epoch (score) 1851 Loss=223.22241 t1=-3.22796 t2=226.45038\n",
      "Epoch (score) 1852 Loss=240.73865 t1=-8.84654 t2=249.58521\n",
      "Epoch (score) 1853 Loss=272.49460 t1=-13.19289 t2=285.68750\n",
      "Epoch (score) 1854 Loss=214.80464 t1=-18.19025 t2=232.99487\n",
      "Epoch (score) 1855 Loss=187.90256 t1=-22.30330 t2=210.20584\n",
      "Epoch (score) 1856 Loss=194.48819 t1=-25.57534 t2=220.06354\n",
      "Epoch (score) 1857 Loss=220.26801 t1=-27.93587 t2=248.20386\n",
      "Epoch (score) 1858 Loss=242.93845 t1=-31.31683 t2=274.25528\n",
      "Epoch (score) 1859 Loss=206.43835 t1=-35.85236 t2=242.29070\n",
      "Epoch (critic) 1860 Loss=221.53857 t1=-38.99258 t2=260.53116\n",
      "Epoch (critic) 1861 Loss=27.02805 t1=20.09760 t2=6.93045\n",
      "Epoch (critic) 1862 Loss=45.64146 t1=40.01353 t2=5.62793\n",
      "Epoch (critic) 1863 Loss=54.02293 t1=49.31670 t2=4.70623\n",
      "Epoch (critic) 1864 Loss=59.27790 t1=53.77164 t2=5.50626\n",
      "Epoch (critic) 1865 Loss=61.59354 t1=56.24663 t2=5.34691\n",
      "Epoch (critic) 1866 Loss=62.73514 t1=57.41766 t2=5.31748\n",
      "Epoch (critic) 1867 Loss=63.18820 t1=57.70677 t2=5.48143\n",
      "Epoch (critic) 1868 Loss=62.67052 t1=57.34387 t2=5.32665\n",
      "Epoch (critic) 1869 Loss=61.45489 t1=56.49447 t2=4.96042\n",
      "Epoch (score) 1870 Loss=60.48040 t1=55.39669 t2=5.08371\n",
      "Epoch (score) 1871 Loss=59.07207 t1=53.80083 t2=5.27124\n",
      "Epoch (score) 1872 Loss=52.77816 t1=47.82011 t2=4.95806\n",
      "Epoch (score) 1873 Loss=43.86466 t1=39.17104 t2=4.69363\n",
      "Epoch (score) 1874 Loss=32.78720 t1=27.71974 t2=5.06747\n",
      "Epoch (score) 1875 Loss=19.19628 t1=14.05613 t2=5.14015\n",
      "Epoch (score) 1876 Loss=13.77385 t1=8.97514 t2=4.79871\n",
      "Epoch (score) 1877 Loss=10.53005 t1=5.34610 t2=5.18395\n",
      "Epoch (score) 1878 Loss=7.83467 t1=2.88963 t2=4.94504\n",
      "Epoch (score) 1879 Loss=5.87673 t1=0.83868 t2=5.03805\n",
      "Epoch (critic) 1880 Loss=4.17942 t1=-1.07444 t2=5.25386\n",
      "Epoch (critic) 1881 Loss=3.90666 t1=-1.04582 t2=4.95248\n",
      "Epoch (critic) 1882 Loss=4.39352 t1=-0.55123 t2=4.94475\n",
      "Epoch (critic) 1883 Loss=5.60277 t1=0.89338 t2=4.70939\n",
      "Epoch (critic) 1884 Loss=7.32350 t1=2.69788 t2=4.62562\n",
      "Epoch (critic) 1885 Loss=9.51977 t1=4.74905 t2=4.77071\n",
      "Epoch (critic) 1886 Loss=10.95801 t1=6.53000 t2=4.42801\n",
      "Epoch (critic) 1887 Loss=12.77810 t1=8.39762 t2=4.38048\n",
      "Epoch (critic) 1888 Loss=14.24206 t1=9.94326 t2=4.29880\n",
      "Epoch (critic) 1889 Loss=15.53909 t1=11.32746 t2=4.21163\n",
      "Epoch (score) 1890 Loss=16.62955 t1=12.37559 t2=4.25396\n",
      "Epoch (score) 1891 Loss=15.80156 t1=11.56917 t2=4.23239\n",
      "Epoch (score) 1892 Loss=14.57633 t1=10.43076 t2=4.14556\n",
      "Epoch (score) 1893 Loss=14.00005 t1=9.62600 t2=4.37406\n",
      "Epoch (score) 1894 Loss=13.27944 t1=9.02601 t2=4.25343\n",
      "Epoch (score) 1895 Loss=12.89510 t1=8.59219 t2=4.30291\n",
      "Epoch (score) 1896 Loss=12.53955 t1=8.33116 t2=4.20840\n",
      "Epoch (score) 1897 Loss=12.20430 t1=7.93260 t2=4.27170\n",
      "Epoch (score) 1898 Loss=11.84220 t1=7.49174 t2=4.35046\n",
      "Epoch (score) 1899 Loss=11.47301 t1=7.14209 t2=4.33092\n",
      "Epoch (critic) 1900 Loss=10.82236 t1=6.73080 t2=4.09156\n",
      "Epoch (critic) 1901 Loss=11.67603 t1=7.68683 t2=3.98920\n",
      "Epoch (critic) 1902 Loss=12.68052 t1=8.51274 t2=4.16778\n",
      "Epoch (critic) 1903 Loss=13.27654 t1=9.07378 t2=4.20276\n",
      "Epoch (critic) 1904 Loss=13.71889 t1=9.52775 t2=4.19113\n",
      "Epoch (critic) 1905 Loss=13.90470 t1=9.83253 t2=4.07217\n",
      "Epoch (critic) 1906 Loss=14.20443 t1=10.00329 t2=4.20114\n",
      "Epoch (critic) 1907 Loss=13.94484 t1=9.89616 t2=4.04868\n",
      "Epoch (critic) 1908 Loss=14.01507 t1=9.77504 t2=4.24004\n",
      "Epoch (critic) 1909 Loss=14.07396 t1=9.55744 t2=4.51652\n",
      "Epoch (score) 1910 Loss=13.57779 t1=9.19693 t2=4.38086\n",
      "Epoch (score) 1911 Loss=13.18002 t1=8.91097 t2=4.26905\n",
      "Epoch (score) 1912 Loss=12.65075 t1=8.54152 t2=4.10923\n",
      "Epoch (score) 1913 Loss=12.61397 t1=8.14730 t2=4.46667\n",
      "Epoch (score) 1914 Loss=12.19911 t1=7.72517 t2=4.47394\n",
      "Epoch (score) 1915 Loss=11.73921 t1=7.20646 t2=4.53275\n",
      "Epoch (score) 1916 Loss=10.67463 t1=6.47233 t2=4.20230\n",
      "Epoch (score) 1917 Loss=9.58292 t1=5.61389 t2=3.96903\n",
      "Epoch (score) 1918 Loss=8.83152 t1=4.60484 t2=4.22668\n",
      "Epoch (score) 1919 Loss=8.15007 t1=3.62049 t2=4.52958\n",
      "Epoch (critic) 1920 Loss=7.07944 t1=2.76677 t2=4.31267\n",
      "Epoch (critic) 1921 Loss=6.86888 t1=2.52671 t2=4.34218\n",
      "Epoch (critic) 1922 Loss=7.05248 t1=2.47392 t2=4.57856\n",
      "Epoch (critic) 1923 Loss=6.72395 t1=2.29352 t2=4.43043\n",
      "Epoch (critic) 1924 Loss=6.74138 t1=2.11953 t2=4.62185\n",
      "Epoch (critic) 1925 Loss=6.40722 t1=1.89069 t2=4.51653\n",
      "Epoch (critic) 1926 Loss=6.61117 t1=1.71249 t2=4.89868\n",
      "Epoch (critic) 1927 Loss=6.32968 t1=1.52966 t2=4.80002\n",
      "Epoch (critic) 1928 Loss=6.54333 t1=1.34420 t2=5.19913\n",
      "Epoch (critic) 1929 Loss=5.95259 t1=1.14715 t2=4.80544\n",
      "Epoch (score) 1930 Loss=5.74580 t1=0.98977 t2=4.75603\n",
      "Epoch (score) 1931 Loss=5.67976 t1=0.77187 t2=4.90789\n",
      "Epoch (score) 1932 Loss=5.39483 t1=0.63860 t2=4.75623\n",
      "Epoch (score) 1933 Loss=5.40052 t1=0.55025 t2=4.85027\n",
      "Epoch (score) 1934 Loss=5.45439 t1=0.44965 t2=5.00473\n",
      "Epoch (score) 1935 Loss=5.27426 t1=0.34135 t2=4.93291\n",
      "Epoch (score) 1936 Loss=5.24239 t1=0.19694 t2=5.04545\n",
      "Epoch (score) 1937 Loss=4.80749 t1=0.03207 t2=4.77542\n",
      "Epoch (score) 1938 Loss=4.61278 t1=-0.25586 t2=4.86864\n",
      "Epoch (score) 1939 Loss=4.40015 t1=-0.39136 t2=4.79151\n",
      "Epoch (critic) 1940 Loss=4.70467 t1=-0.42988 t2=5.13455\n",
      "Epoch (critic) 1941 Loss=4.58353 t1=-0.30692 t2=4.89045\n",
      "Epoch (critic) 1942 Loss=4.97189 t1=-0.22391 t2=5.19580\n",
      "Epoch (critic) 1943 Loss=5.25970 t1=-0.02334 t2=5.28304\n",
      "Epoch (critic) 1944 Loss=5.18300 t1=0.13031 t2=5.05269\n",
      "Epoch (critic) 1945 Loss=5.53307 t1=0.34645 t2=5.18662\n",
      "Epoch (critic) 1946 Loss=5.89193 t1=0.52862 t2=5.36331\n",
      "Epoch (critic) 1947 Loss=6.07673 t1=0.73620 t2=5.34053\n",
      "Epoch (critic) 1948 Loss=6.07927 t1=0.92971 t2=5.14957\n",
      "Epoch (critic) 1949 Loss=6.36513 t1=1.07189 t2=5.29324\n",
      "Epoch (score) 1950 Loss=7.01661 t1=1.32660 t2=5.69001\n",
      "Epoch (score) 1951 Loss=6.93471 t1=1.25658 t2=5.67813\n",
      "Epoch (score) 1952 Loss=7.08355 t1=1.24415 t2=5.83940\n",
      "Epoch (score) 1953 Loss=6.39623 t1=1.14570 t2=5.25054\n",
      "Epoch (score) 1954 Loss=6.93785 t1=1.08893 t2=5.84892\n",
      "Epoch (score) 1955 Loss=6.49513 t1=1.01606 t2=5.47906\n",
      "Epoch (score) 1956 Loss=6.58219 t1=0.89687 t2=5.68532\n",
      "Epoch (score) 1957 Loss=6.52015 t1=0.79738 t2=5.72277\n",
      "Epoch (score) 1958 Loss=6.58154 t1=0.74191 t2=5.83963\n",
      "Epoch (score) 1959 Loss=6.25961 t1=0.64991 t2=5.60970\n",
      "Epoch (critic) 1960 Loss=6.41747 t1=0.57858 t2=5.83888\n",
      "Epoch (critic) 1961 Loss=6.28704 t1=0.63145 t2=5.65558\n",
      "Epoch (critic) 1962 Loss=6.94796 t1=0.70612 t2=6.24184\n",
      "Epoch (critic) 1963 Loss=6.37848 t1=0.73458 t2=5.64390\n",
      "Epoch (critic) 1964 Loss=7.09884 t1=0.76190 t2=6.33694\n",
      "Epoch (critic) 1965 Loss=7.23646 t1=0.79703 t2=6.43943\n",
      "Epoch (critic) 1966 Loss=6.68649 t1=0.79814 t2=5.88835\n",
      "Epoch (critic) 1967 Loss=6.97495 t1=0.78133 t2=6.19362\n",
      "Epoch (critic) 1968 Loss=6.76421 t1=0.77463 t2=5.98959\n",
      "Epoch (critic) 1969 Loss=7.47142 t1=0.76866 t2=6.70275\n",
      "Epoch (score) 1970 Loss=8.07958 t1=0.75501 t2=7.32457\n",
      "Epoch (score) 1971 Loss=7.24300 t1=0.65265 t2=6.59035\n",
      "Epoch (score) 1972 Loss=6.99484 t1=0.59471 t2=6.40013\n",
      "Epoch (score) 1973 Loss=6.60174 t1=0.51914 t2=6.08260\n",
      "Epoch (score) 1974 Loss=6.74599 t1=0.45136 t2=6.29464\n",
      "Epoch (score) 1975 Loss=7.15533 t1=0.41808 t2=6.73725\n",
      "Epoch (score) 1976 Loss=6.89259 t1=0.35187 t2=6.54072\n",
      "Epoch (score) 1977 Loss=6.68650 t1=0.31373 t2=6.37277\n",
      "Epoch (score) 1978 Loss=7.55803 t1=0.29914 t2=7.25889\n",
      "Epoch (score) 1979 Loss=7.36775 t1=0.26941 t2=7.09833\n",
      "Epoch (critic) 1980 Loss=7.28485 t1=0.24497 t2=7.03989\n",
      "Epoch (critic) 1981 Loss=6.80379 t1=0.24477 t2=6.55902\n",
      "Epoch (critic) 1982 Loss=6.78200 t1=0.23366 t2=6.54834\n",
      "Epoch (critic) 1983 Loss=6.34029 t1=0.20715 t2=6.13314\n",
      "Epoch (critic) 1984 Loss=6.94499 t1=0.21534 t2=6.72965\n",
      "Epoch (critic) 1985 Loss=5.85843 t1=0.18754 t2=5.67089\n",
      "Epoch (critic) 1986 Loss=5.91987 t1=0.19313 t2=5.72674\n",
      "Epoch (critic) 1987 Loss=6.25132 t1=0.15279 t2=6.09854\n",
      "Epoch (critic) 1988 Loss=5.44455 t1=0.16966 t2=5.27489\n",
      "Epoch (critic) 1989 Loss=5.42134 t1=0.14862 t2=5.27272\n",
      "Epoch (score) 1990 Loss=5.12471 t1=0.14795 t2=4.97676\n",
      "Epoch (score) 1991 Loss=4.25063 t1=0.12643 t2=4.12420\n",
      "Epoch (score) 1992 Loss=4.82358 t1=0.13145 t2=4.69213\n",
      "Epoch (score) 1993 Loss=4.54809 t1=0.10626 t2=4.44183\n",
      "Epoch (score) 1994 Loss=3.70967 t1=0.09668 t2=3.61299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (score) 1995 Loss=5.41733 t1=0.08863 t2=5.32869\n",
      "Epoch (score) 1996 Loss=4.60465 t1=0.08995 t2=4.51470\n",
      "Epoch (score) 1997 Loss=5.92712 t1=0.08464 t2=5.84248\n",
      "Epoch (score) 1998 Loss=4.68067 t1=0.06642 t2=4.61425\n",
      "Epoch (score) 1999 Loss=4.72752 t1=0.05606 t2=4.67146\n"
     ]
    }
   ],
   "source": [
    "# score net\n",
    "score_net = Scorenet()\n",
    "critic_net = Criticnet()\n",
    "critic_net.to(device)\n",
    "score_net.to(device)\n",
    "\n",
    "print(score_net)\n",
    "print(critic_net)\n",
    "\n",
    "# optimizer\n",
    "opt_scorenet, scheduler_scorenet = get_opt(score_net.parameters(), cfg.trainer.opt_scorenet)\n",
    "opt_criticnet, scheduler_criticnet = get_opt(critic_net.parameters(), cfg.trainer.opt_scorenet)\n",
    "\n",
    "# training\n",
    "start_epoch = 0\n",
    "print(\"Start epoch: %d End epoch: %d\" % (start_epoch, cfg.trainer.epochs))\n",
    "k_iters = 10\n",
    "e_iters = 10\n",
    "for epoch in range(start_epoch, cfg.trainer.epochs):\n",
    "    score_net.train()\n",
    "    critic_net.train()\n",
    "    opt_scorenet.zero_grad()\n",
    "    opt_criticnet.zero_grad()\n",
    "    \n",
    "    labels = torch.randint(0, len(sigmas), (1,), device=tr_pts.device)\n",
    "    used_sigmas = torch.tensor(np.array(sigmas))[-1].float().view(1, 1).cuda()\n",
    "    \n",
    "    perturbed_points = tr_pts + torch.randn_like(tr_pts) * used_sigmas\n",
    "    \n",
    "    score_pred = score_net(perturbed_points, used_sigmas)\n",
    "    critic_output = critic_net(perturbed_points, used_sigmas)\n",
    "    \n",
    "    t1 = (score_pred * critic_output).sum(-1)\n",
    "#     t2 = approx_jacobian_trace(critic_output, perturbed_points)\n",
    "    t2 = exact_jacobian_trace(critic_output, perturbed_points)\n",
    "    stein = t1 + t2\n",
    "    l2_penalty = (critic_output * critic_output).sum(-1).mean()\n",
    "    loss = stein.mean()\n",
    "    \n",
    "    cycle_iter = epoch % (k_iters + e_iters)\n",
    "    cpu_loss = loss.detach().cpu().item()\n",
    "    cpu_t1 = t1.mean().detach().cpu().item()\n",
    "    cpu_t2 = t2.mean().detach().cpu().item()\n",
    "    if cycle_iter < k_iters:\n",
    "        (-loss + l2_penalty).backward()\n",
    "        opt_criticnet.step()\n",
    "        print(\"Epoch (critic) %d Loss=%2.5f t1=%2.5f t2=%2.5f\" % (epoch, cpu_loss, cpu_t1, cpu_t2))\n",
    "    else:\n",
    "        loss.backward()\n",
    "        opt_scorenet.step()\n",
    "        print(\"Epoch (score) %d Loss=%2.5f t1=%2.5f t2=%2.5f\" % (epoch, cpu_loss, cpu_t1, cpu_t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
